{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## This Notebook will create the database for the temporary project washing machine manuals\n",
    "\n",
    "The intention of this notebook is to create a clean and best practice database structure, along with utilizing snowflakes AI functions.\n",
    "\n",
    "The intended database structure is as follows: \n",
    "\n",
    "- **documents** (Stores metadata about each manual)  \n",
    "  - `document_id` (Unique ID for each manual)  \n",
    "  - `doc_name` (Document name)\n",
    "  - `version` (Version or revision number)  \n",
    "  - `relative_path` (Original PDF file path or S3 URL) \n",
    "  - `stage_name`  (snowflake stage name (source))\n",
    "  - `size`  (size in bytes of the PDF document) \n",
    "\n",
    "- **sections** (Defines logical sections and subsections within each manual)  \n",
    "  - `section_id` (Unique ID for the section)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `title` (Title or heading of the section)  \n",
    "  - `order_num` (Numerical order of the section in the manual)  \n",
    "  - `parent_section_id` (Optional FK for nested subsections)  \n",
    "\n",
    "- **chunks small** (1024 characters, 64 overlap)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **chunks large** (4096 characters, overlap 256)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **images** (Stores references to images extracted from the manual)  \n",
    "  - `image_id` (Unique ID for the image)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `page_number` \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `order_num` (Display order within the section)  \n",
    "  - `image_path` (S3 or web-accessible path to the image)  \n",
    "  - `description`   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6fc673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keyring\n",
    "import os \n",
    "import snowflake.connector as sf_connector # ( https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect)\n",
    "from snowflake.connector.pandas_tools import write_pandas # (https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#write_pandas)\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.evaluation import load_evaluator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from io import BytesIO\n",
    "import fitz \n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Set max rows to display in pandas DataFrame 200\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90ca260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Identifier:  EPTJRCA-HWB83214\n",
      "User Name:  EMHALDEMO1\n",
      "Database:  WASHING_MACHINE_MANUALS\n",
      "Schema:  PUBLIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x1f54c90c740>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    open_ai_api_key = keyring.get_password(\"openai api\", \"api_key\")\n",
    "    if open_ai_api_key is None:\n",
    "        raise ValueError(\"API key not found in keyring.\")\n",
    "except:\n",
    "    print(\"Please set your OpenAI API key in microsoft generic credential manager with the hostname 'openai api' and username 'api_key'\")\n",
    "\n",
    "try:\n",
    "    account_identifier = keyring.get_password('NC_Snowflake_Trial_Account_Name', 'account_identifier')\n",
    "    user_name = \"EMHALDEMO1\" # Change this to your Snowflake user name\n",
    "    password = keyring.get_password('NC_Snowflake_Trial_User_Password', user_name)\n",
    "    database = \"WASHING_MACHINE_MANUALS\"\n",
    "    schema = \"PUBLIC\"\n",
    "except:\n",
    "    print(\"Please set your Snowflake account identifier and user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_Account_Name' and username 'account_identifier'\")\n",
    "    print(\"Please set your Snowflake user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_User_Password' and username \")\n",
    "\n",
    "client = OpenAI(api_key = open_ai_api_key)\n",
    "\n",
    "print(\"Account Identifier: \", account_identifier)\n",
    "print(\"User Name: \", user_name)\n",
    "print(\"Database: \", database)\n",
    "print(\"Schema: \", schema)\n",
    "\n",
    "try:\n",
    "    connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": database,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "except:\n",
    "        connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"schema\": \"CORTEX\"\n",
    "    }\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = sf_connector.connect(\n",
    "    user=connection_parameters['user'],\n",
    "    password=connection_parameters['password'],\n",
    "    account=connection_parameters['account_identifier'],\n",
    "    warehouse=connection_parameters['warehouse'],\n",
    "    database=connection_parameters['database'],\n",
    "    schema=connection_parameters['schema'],\n",
    "    role=connection_parameters['role']\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\" CREATE DATABASE IF NOT EXISTS {database}; \")\n",
    "cursor.execute(f\" CREATE SCHEMA IF NOT EXISTS {database}.{schema}; \")\n",
    "cursor.execute(f\" USE DATABASE {database}; \")\n",
    "cursor.execute(f\" USE SCHEMA {schema}; \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055a0e",
   "metadata": {},
   "source": [
    "## Create a stage for the PDF files with the code below\n",
    " Try Except wrapping the code should catch if you already have the database and data created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530a54",
   "metadata": {},
   "source": [
    "## Creating documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dcced24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_NAME</th>\n",
       "      <th>DOC_VERSION</th>\n",
       "      <th>FILE_PATH</th>\n",
       "      <th>FILE_SIZE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WGA1420SIN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGA1420SIN.pdf</td>\n",
       "      <td>3247850</td>\n",
       "      <td>2025-04-25 07:01:58.434000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WGG254Z0GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGG254Z0GB.pdf</td>\n",
       "      <td>3291555</td>\n",
       "      <td>2025-04-25 07:01:58.434000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID   DOCUMENT_NAME DOC_VERSION                        FILE_PATH  \\\n",
       "0            1  WGA1420SIN.pdf         N/A  .\\Washer_Manuals\\WGA1420SIN.pdf   \n",
       "1            2  WGG254Z0GB.pdf         N/A  .\\Washer_Manuals\\WGG254Z0GB.pdf   \n",
       "\n",
       "   FILE_SIZE                       CREATED_AT  \n",
       "0    3247850 2025-04-25 07:01:58.434000-07:00  \n",
       "1    3291555 2025-04-25 07:01:58.434000-07:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "    \"\"\")\n",
    "\n",
    "    documents_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table DOCUMENTS\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE DOCUMENTS (\n",
    "        DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_NAME STRING,\n",
    "        DOC_VERSION STRING,\n",
    "        FILE_PATH STRING NOT NULL,\n",
    "        FILE_SIZE NUMBER,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table DOCUMENTS created. Fetching data from PDF files.\")\n",
    "    pdf_files_path = \".\\\\Washer_Manuals\"\n",
    "    document_rows = []\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(pdf_files_path)):\n",
    "        # Temporary filter to only process a set of PDF files\n",
    "        if filename not in [\"WGG254Z0GB.pdf\", \"WGA1420SIN.pdf\"]: #,\"WAV28KH3GB.pdf\"]:\n",
    "            continue\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_files_path, filename)\n",
    "            print(f\"Document number: {idx}  : {file_path}\")\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            document_rows.append({\n",
    "                \"DOCUMENT_NAME\": filename,\n",
    "                \"FILE_PATH\": file_path,\n",
    "                \"DOC_VERSION\": \"N/A\",  # Placeholder, you can modify this logic as needed\n",
    "                \"FILE_SIZE\": file_size\n",
    "            })\n",
    "\n",
    "    documents_df = pd.DataFrame(document_rows)\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=documents_df,\n",
    "        database =database,\n",
    "        table_name=\"DOCUMENTS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    create_dataframe = False\n",
    "    print(f\"Success: {success}, Number of chunks: {nchunks}, Number of rows: {nrows}\")\n",
    "\n",
    "\n",
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524735a",
   "metadata": {},
   "source": [
    "# The section below focuses on creating chunks_large and chunks_small.\n",
    "\n",
    "Different size chunks are good at different things - it could be a good idea to store both size, especially during testing\n",
    "\n",
    "To include page numbers, i decided to create the tables using pandas, and then uploading them to snowflake\n",
    "\n",
    "Followed by that will be a query to crete a vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ab02b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_LARGE\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "\n",
    "    large_chunks_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table CHUNKS_LARGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e88f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_LARGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.65s/it]\n"
     ]
    }
   ],
   "source": [
    "## Extracting section headers from the PDF files\n",
    "\n",
    "def extract_text_chunks(file_path, manual_id, chunk_size=512, chunk_overlap=128):\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Step 1: Combine all text across pages with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = []  # (char_index, page_number)\n",
    "\n",
    "    for doc_page in docs:\n",
    "        text = doc_page.page_content.strip().replace('\\n', ' ')\n",
    "        start_idx = len(all_text)\n",
    "        all_text += text + \" \"  # Add space to separate pages\n",
    "        end_idx = len(all_text)\n",
    "        page_map.append((start_idx, end_idx, doc_page.metadata['page']))\n",
    "\n",
    "    # Step 2: Create chunks with overlap, spanning across pages\n",
    "    chunks = []\n",
    "    chunk_order = []\n",
    "    page_start_list = []\n",
    "    page_end_list = []\n",
    "\n",
    "    idx = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while idx < len(all_text):\n",
    "        chunk = all_text[idx:idx + chunk_size]\n",
    "\n",
    "        # Determine pages involved in this chunk\n",
    "        chunk_start = idx\n",
    "        chunk_end = idx + len(chunk)\n",
    "\n",
    "        pages_in_chunk = [\n",
    "            page_num\n",
    "            for start, end, page_num in page_map\n",
    "            if not (end <= chunk_start or start >= chunk_end)  # overlap condition\n",
    "        ]\n",
    "\n",
    "        page_start = min(pages_in_chunk) if pages_in_chunk else None\n",
    "        page_end = max(pages_in_chunk) if pages_in_chunk else None\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        page_start_list.append(page_start)\n",
    "        page_end_list.append(page_end)\n",
    "        chunk_order.append(chunk_idx)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        idx += chunk_size - chunk_overlap\n",
    "\n",
    "    # Step 3: Create DataFrame\n",
    "    rows = [{\n",
    "        'DOCUMENT_ID': manual_id,\n",
    "        'PAGE_START_NUMBER': start,\n",
    "        'PAGE_END_NUMBER': end,\n",
    "        'CHUNK_TEXT': chunk,\n",
    "        'CHUNK_ORDER': order\n",
    "    } for chunk, start, end, order in zip(chunks, page_start_list, page_end_list, chunk_order)]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"DOCUMENT_ID\", \"PAGE_START_NUMBER\", \"PAGE_END_NUMBER\", \"CHUNK_TEXT\", \"CHUNK_ORDER\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "if create_dataframe:\n",
    "    print(\"Creating table CHUNKS_LARGE\")\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "\n",
    "    large_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 6000,#1024,\n",
    "                            chunk_overlap = 128)  # Show first 5 chunks\n",
    "        large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "else: \n",
    "    print(\"Loading the existing data from Snowflake into a DataFrame\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "    large_chunks_df = cursor.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc67cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the embedding update as the data already exists in Snowflake\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>s damaged, this is dangerous. ▶ Never let the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>the fuse in the fuse box. ▶ Do not use steam- ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>nts of package When using the appliance on a b...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>8 Before using for the first time en 5. Add wa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  \\\n",
       "0            1                  0                4   \n",
       "1            1                  4                8   \n",
       "2            1                  8               11   \n",
       "3            1                 11               18   \n",
       "4            1                 17               24   \n",
       "\n",
       "                                          CHUNK_TEXT  CHUNK_ORDER  \n",
       "0  Register your b M o ge s y n c t B e h f o r w...            0  \n",
       "1  s damaged, this is dangerous. ▶ Never let the ...            1  \n",
       "2  the fuse in the fuse box. ▶ Do not use steam- ...            2  \n",
       "3  nts of package When using the appliance on a b...            3  \n",
       "4  8 Before using for the first time en 5. Add wa...            4  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if create_dataframe:\n",
    "    print(\"Writing the large chunks DataFrame to Snowflake\")\n",
    "    # Write the DataFrame to Snowflake\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=large_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_LARGE\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    # Update the embeddings for the chunks in the CHUNKS_LARGE table\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_LARGE\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "else: \n",
    "    print(\"Skipping the embedding update as the data already exists in Snowflake\")\n",
    "\n",
    "large_chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3b343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_SMALL\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table CHUNKS_SMALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d11219e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the embedding update as the data already exists in Snowflake\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>EMBEDDING</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>[0.032073975, 0.05307007, -0.021911621, -0.006...</td>\n",
       "      <td>2025-04-25 07:02:52.548000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>..... 29 2Preventing material damage.... 10 13...</td>\n",
       "      <td>[0.018035889, 0.035339355, -0.0013151169, 0.00...</td>\n",
       "      <td>2025-04-25 07:02:52.548000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>moving the transit bolts...... 13 13.10 Cancel...</td>\n",
       "      <td>[0.086364746, 0.014724731, 0.004234314, -0.003...</td>\n",
       "      <td>2025-04-25 07:02:52.548000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>......... 20 2 en 15 Basic settings..............</td>\n",
       "      <td>[0.029403687, -0.017074585, -0.033416748, 0.00...</td>\n",
       "      <td>2025-04-25 07:02:52.548000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e the following safety instructions. 1.1 Gener...</td>\n",
       "      <td>[0.050964355, 0.06311035, -0.032409668, 0.0217...</td>\n",
       "      <td>2025-04-25 07:02:52.548000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHUNK_ID  DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  CHUNK_ORDER  \\\n",
       "0         1            1                  0                1            0   \n",
       "1         2            1                  1                1            1   \n",
       "2         3            1                  1                2            2   \n",
       "3         4            1                  1                3            3   \n",
       "4         5            1                  3                3            4   \n",
       "\n",
       "                                          CHUNK_TEXT  \\\n",
       "0  Register your b M o ge s y n c t B e h f o r w...   \n",
       "1  ..... 29 2Preventing material damage.... 10 13...   \n",
       "2  moving the transit bolts...... 13 13.10 Cancel...   \n",
       "3  ......... 20 2 en 15 Basic settings..............   \n",
       "4  e the following safety instructions. 1.1 Gener...   \n",
       "\n",
       "                                           EMBEDDING  \\\n",
       "0  [0.032073975, 0.05307007, -0.021911621, -0.006...   \n",
       "1  [0.018035889, 0.035339355, -0.0013151169, 0.00...   \n",
       "2  [0.086364746, 0.014724731, 0.004234314, -0.003...   \n",
       "3  [0.029403687, -0.017074585, -0.033416748, 0.00...   \n",
       "4  [0.050964355, 0.06311035, -0.032409668, 0.0217...   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-25 07:02:52.548000-07:00  \n",
       "1 2025-04-25 07:02:52.548000-07:00  \n",
       "2 2025-04-25 07:02:52.548000-07:00  \n",
       "3 2025-04-25 07:02:52.548000-07:00  \n",
       "4 2025-04-25 07:02:52.548000-07:00  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if create_dataframe:\n",
    "\n",
    "    small_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 1024,\n",
    "                            chunk_overlap = 64)  # Show first 5 chunks\n",
    "        small_chunks_df = pd.concat([small_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=small_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_SMALL\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    # Update the embeddings for the small chunks\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_SMALL\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "else: \n",
    "    print(\"Skipping the embedding update as the data already exists in Snowflake\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "\n",
    "small_chunks_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b8c0f",
   "metadata": {},
   "source": [
    "## Creating sections table using LLM for TOC extraction\n",
    "\n",
    "The function `extract_TOC` takes quite a while due to the chunk size and the model. This can be tampered with, but i found most consistent results with said model. I also think that larger chunks are better for this task, as the model can see context of the first few pages, and it also ensures that the table of contents is included in the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ea5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOC(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = cursor.execute(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\")\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "\n",
    "    return cursor.fetch_pandas_all().iloc[0,0]\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "\n",
    "# llm_output = extract_TOC(large_chunks_df.loc[0,\"CHUNK_TEXT\"], model = 'llama3.1-70b')\n",
    "# llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fec25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_llm_output(llm_output: str) -> dict:\n",
    "    try:\n",
    "        # Confirming that a JSON block is returned\n",
    "        match = re.search(r\"```\\s*(\\{.*?\\})\\s*```\", llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        # Extracting sub string (json string)\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Clean common JSON errors (e.g., trailing commas)\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)  # remove trailing commas before ] or }\n",
    "        \n",
    "        # Parse string to json\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "        \n",
    "# parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "# print(json.dumps(parsed_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef7f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_sections(node, parent_section=None):\n",
    "    rows = []\n",
    "\n",
    "    # Get info from the current node\n",
    "    section = node.get(\"Section\")\n",
    "    section_number = node.get(\"Section Number\")\n",
    "    page = node.get(\"Page\")\n",
    "\n",
    "    # Add current node to the list\n",
    "    evaluator = load_evaluator(\"string_distance\")\n",
    "    levenshtein_score_toc = evaluator.evaluate_strings(\n",
    "    prediction=section,\n",
    "    reference=\"Table of Contents\",\n",
    "    metric=\"levenshtein\"\n",
    "    )[\"score\"]  # This will be a float between 0 and 1, where 0 means identical\n",
    "\n",
    "    if levenshtein_score_toc > 0.1:  # if the levenshtein distance is very small its likely to match \"Table of Contents\"\n",
    "        rows.append({\n",
    "            \"SECTION\": section,\n",
    "            \"SECTION_NUMBER\": section_number,\n",
    "            \"PAGE\": page,\n",
    "            \"PARENT_SECTION_NUMBER\": parent_section\n",
    "        })\n",
    "\n",
    "    # Recurse into each sub-section, if any\n",
    "    for subsection in node.get(\"Sub Sections\", []):\n",
    "        rows.extend(traverse_sections(subsection, parent_section=section_number))\n",
    "\n",
    "    return rows\n",
    "\n",
    "# flat_rows = traverse_sections(parsed_dict)\n",
    "# toc_df = pd.DataFrame(flat_rows)\n",
    "# toc_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b9c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\"):\n",
    "    df_list = []\n",
    "\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        first_chunk_of_doc = large_chunks_df.loc[large_chunks_df[\"DOCUMENT_ID\"] == manual_id, \"CHUNK_TEXT\"].iloc[0]\n",
    "        # print(\"First chunk:\", first_chunk_of_doc)\n",
    "\n",
    "        llm_output = extract_TOC(first_chunk_of_doc, model = model)\n",
    "        parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "        flat_rows = traverse_sections(parsed_dict)\n",
    "        local_toc_df = pd.DataFrame(flat_rows)\n",
    "        local_toc_df[\"DOCUMENT_ID\"] = manual_id\n",
    "        df_list.append(local_toc_df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73257e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Safety</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe use</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe cleaning and maintenance</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Preventing material damage</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Environmental protection and saving energy</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Disposing of packaging</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-04-25 07:04:39.476000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SECTION_ID  DOCUMENT_ID                                     SECTION  \\\n",
       "0           1            1                                      Safety   \n",
       "1           2            1                         General information   \n",
       "2           3            1                                Intended use   \n",
       "3           4            1                   Restriction on user group   \n",
       "4           5            1                           Safe installation   \n",
       "5           6            1                                    Safe use   \n",
       "6           7            1               Safe cleaning and maintenance   \n",
       "7           8            1                  Preventing material damage   \n",
       "8           9            1  Environmental protection and saving energy   \n",
       "9          10            1                      Disposing of packaging   \n",
       "\n",
       "  SECTION_NUMBER PARENT_SECTION_NUMBER  PAGE                       CREATED_AT  \n",
       "0              1                           4 2025-04-25 07:04:39.476000-07:00  \n",
       "1            1.1                     1     4 2025-04-25 07:04:39.476000-07:00  \n",
       "2            1.2                     1     4 2025-04-25 07:04:39.476000-07:00  \n",
       "3            1.3                     1     4 2025-04-25 07:04:39.476000-07:00  \n",
       "4            1.4                     1     5 2025-04-25 07:04:39.476000-07:00  \n",
       "5            1.5                     1     7 2025-04-25 07:04:39.476000-07:00  \n",
       "6            1.6                     1     9 2025-04-25 07:04:39.476000-07:00  \n",
       "7              2                          10 2025-04-25 07:04:39.476000-07:00  \n",
       "8              3                          11 2025-04-25 07:04:39.476000-07:00  \n",
       "9            3.1                     3    11 2025-04-25 07:04:39.476000-07:00  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table SECTIONS\")\n",
    "    sections_df = create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SECTIONS (\n",
    "        SECTION_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        SECTION STRING NOT NULL,\n",
    "        SECTION_NUMBER STRING NOT NULL,\n",
    "        PARENT_SECTION_NUMBER STRING,\n",
    "        PAGE INT,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=sections_df,\n",
    "        database =database,\n",
    "        table_name=\"SECTIONS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "\n",
    "sections_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8cb1",
   "metadata": {},
   "source": [
    "# Extracting images from the manual\n",
    "\n",
    "This chosen method which appears to be more diverse across the manuals treats each page as an image. This is a good way to ensure that all images are extracted. \n",
    "The downside is that tables and other image like content will be extracted as images. Currently this is a feature not a bug. Adjusting the image extraction method is a task for the future when we have the real PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f45c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pdf_to_images(pdf_path, zoom=2.0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i, page in enumerate(doc):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img_data = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append({\n",
    "            \"page_number\": i + 1,\n",
    "            \"image\": img_data\n",
    "        })\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_pdf_page_pixel_size(pdf_image):\n",
    "    width, height = pdf_image.size\n",
    "    return width * height\n",
    "\n",
    "\n",
    "def detect_image_regions(page_image, buffer=0, min_size=70, max_size = 1000, threshold=240):\n",
    "    image = np.array(page_image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Applying blur to reduce fine lines from tables\n",
    "    _, thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > min_size and h > min_size:  # Skip tiny blocks (Maybe reconsider)\n",
    "            regions.append([x - buffer, \n",
    "                            y - buffer, \n",
    "                            x + w + buffer, \n",
    "                            y + h + buffer])\n",
    "            if w * h > max_size:\n",
    "                regions.pop(-1)  \n",
    "    return regions\n",
    "\n",
    "\n",
    "def crop_regions_from_image(page_image, regions, output_dir, page_num, manual_id):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_images = []\n",
    "\n",
    "    for i, coords in enumerate(regions):\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        cropped = page_image.crop((x1, y1, x2, y2))\n",
    "        save_path = os.path.join(output_dir, f\"doc_{manual_id}_page_{page_num}_img_{i+1}.png\")\n",
    "        cropped.save(save_path)\n",
    "        saved_images.append({\n",
    "            \"page\": page_num,\n",
    "            \"image_path\": save_path,\n",
    "            \"coords\": (x1, y1, x2, y2)\n",
    "        })\n",
    "    return saved_images\n",
    "\n",
    "\n",
    "\n",
    "def add_region_to_page(page_image, regions, output_dir, page_num, pdf_path ,color=(0, 255, 0), alpha=50, save=True, verbose=0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to RGBA to allow transparency\n",
    "    annotated = page_image.convert(\"RGBA\")\n",
    "    overlay = Image.new(\"RGBA\", annotated.size, (0, 0, 0, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    for coords in regions:\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color + (alpha,), fill=color + (alpha,))\n",
    "\n",
    "    # Combine original image with overlay\n",
    "    combined = Image.alpha_composite(annotated, overlay)\n",
    "\n",
    "    if save:\n",
    "        save_path = os.path.join(output_dir, f\"page_{page_num:03d}_with_regions_{color}.png\")\n",
    "        combined.convert(\"RGB\").save(save_path)\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved page {page_num} with highlighted regions to {save_path}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def merge_overlapping_regions(regions, buffer=0):\n",
    "    \"\"\"\n",
    "    Merges overlapping or intersecting regions.\n",
    "\n",
    "    Args:\n",
    "        regions (List[List[int]]): List of regions as [x1, y1, x2, y2].\n",
    "        buffer (int): Optional buffer added to each region before checking overlaps.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Merged list of non-overlapping regions.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    # Convert to shapely boxes with optional buffer\n",
    "    boxes = [box(x1 - buffer, y1 - buffer, x2 + buffer, y2 + buffer) for x1, y1, x2, y2 in regions]\n",
    "\n",
    "    # Merge all overlapping boxes (A fix to a previous issues of diagrams being cropped into multiple images)\n",
    "    merged = unary_union(boxes)\n",
    "\n",
    "    # Ensure output is a list of boxes\n",
    "    if merged.geom_type == 'Polygon':\n",
    "        merged_boxes = [merged]\n",
    "    else:\n",
    "        merged_boxes = list(merged.geoms)\n",
    "\n",
    "    # Convert back to [x1, y1, x2, y2] format (round to int)\n",
    "    merged_regions = []\n",
    "    for b in merged_boxes:\n",
    "        x1, y1, x2, y2 = b.bounds\n",
    "        merged_regions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    return merged_regions\n",
    "\n",
    "\n",
    "\n",
    "# This is the main function to extract images from the PDF\n",
    "def extract_images_from_pdf(pdf_path:str, manual_id:int, output_dir: str, verbose:int =0):\n",
    "    rendered_pages = render_pdf_to_images(pdf_path)\n",
    "    all_extracted = []\n",
    "\n",
    "    for page_idx,page in enumerate(rendered_pages):\n",
    "        page_num = page[\"page_number\"] \n",
    "        image = page[\"image\"]\n",
    "        if verbose > 0:\n",
    "            print(f\"Processing page {page_num}...\")\n",
    "\n",
    "        # Detecting regions\n",
    "        regions = detect_image_regions(image , buffer=2, min_size=70, \n",
    "                                        max_size=get_pdf_page_pixel_size(image) * 0.99)\n",
    "        # Creates new regions by merging overlapping regions (this is a fix for cropped images  )\n",
    "        new_regions = merge_overlapping_regions(regions, buffer=0)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Found {len(new_regions)} image regions on page {page_num}\")\n",
    "\n",
    "        if not new_regions:\n",
    "            if verbose > 0:\n",
    "                print(f\"No image regions found on page {page_num}\")\n",
    "            continue\n",
    "        \n",
    "        # Creates an image directory for each PDF file\n",
    "        image_output_dir = pdf_path.split(\"/\")[-1].replace(\".pdf\", \"\").replace(\"Washer_Manuals\", output_dir)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "        # Showing the pages with the masked regions \n",
    "        modified_image = add_region_to_page(image, new_regions, image_output_dir, page_num, pdf_path, color=(0, 0, 255), alpha=50, save = False)\n",
    "\n",
    "        # OLD code \n",
    "        extracted = crop_regions_from_image(\n",
    "            image, new_regions, output_dir=image_output_dir, page_num=page_num, manual_id=manual_id\n",
    "        )\n",
    "        all_extracted.extend(extracted)\n",
    "    return all_extracted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b32ef3",
   "metadata": {},
   "source": [
    "# Creating table for image references and metadata\n",
    "\n",
    "Currently the images are matched to the sections using the page number, which is problematic if the end of section 4.3 is one the same page as the start of section 4.4. On the top of my head i'm not quite sure how to match the images to the sections accurately, but this method yields mostly correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7cf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_number_from_filename(filename):\n",
    "    return filename.split(\"_\")[3] if \"_\" in filename else None\n",
    "\n",
    "def generate_image_table(documents_df, sections_df, image_dir):\n",
    "    image_records = []\n",
    "\n",
    "    # Loop over all subdirectories in image_dir\n",
    "    for subfolder in os.listdir(image_dir):\n",
    "        subfolder_path = os.path.join(image_dir, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue  # skip files\n",
    "        \n",
    "        # Match to document by DOCUMENT_NAME (strip extension if needed)\n",
    "        matching_docs = documents_df[documents_df['DOCUMENT_NAME'].str.contains(subfolder, case=False)]\n",
    "        if matching_docs.empty:\n",
    "            print(f\"No matching document for subfolder: {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        document_id = matching_docs.iloc[0]['DOCUMENT_ID']\n",
    "        document_name = matching_docs.iloc[0]['DOCUMENT_NAME']\n",
    "        \n",
    "        # List all image files in subdirectory\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            if not image_file.lower().endswith((\".png\")):\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            page_number = extract_page_number_from_filename(image_file)\n",
    "            order_number = image_file.split(\"img_\")[-1].strip(\".png\")\n",
    "\n",
    "            image_size = os.path.getsize(image_path)\n",
    "            image_width, image_height = Image.open(image_path).size\n",
    "            \n",
    "            # Try to match to a section (same document, closest PAGE <= image page)\n",
    "            section_match = None\n",
    "            if page_number is not None:\n",
    "                matching_sections = sections_df[\n",
    "                    (sections_df['DOCUMENT_ID'] == document_id) & \n",
    "                    (sections_df['PAGE'].astype(str) <= str(page_number))\n",
    "                ]\n",
    "                if not matching_sections.empty:\n",
    "                    section_match = matching_sections.sort_values(\"PAGE\", ascending=False).iloc[0]\n",
    "            \n",
    "            image_records.append({\n",
    "                \"DOCUMENT_ID\": document_id,\n",
    "                \"SECTION_ID\": section_match[\"SECTION_ID\"] if section_match is not None else None,\n",
    "                \"SECTION_NUMBER\": section_match[\"SECTION_NUMBER\"] if section_match is not None else None,\n",
    "                \"PAGE\": page_number,\n",
    "                \"IMG_ORDER\": order_number,\n",
    "                \"IMAGE_FILE\": image_file,\n",
    "                \"IMAGE_PATH\": image_path,\n",
    "                \"IMAGE_SIZE\": image_size,\n",
    "                \"IMAGE_WIDTH\": image_width,\n",
    "                \"IMAGE_HEIGHT\": image_height\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(image_records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4f76112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Images from PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table IMAGES\n",
      "Success: True, Chunks: 1, Rows: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE_ID</th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>IMG_ORDER</th>\n",
       "      <th>IMAGE_FILE</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>IMAGE_SIZE</th>\n",
       "      <th>IMAGE_WIDTH</th>\n",
       "      <th>IMAGE_HEIGHT</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_12_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_12_img_1...</td>\n",
       "      <td>4997</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_13_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_1...</td>\n",
       "      <td>24386</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_1_page_13_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_2...</td>\n",
       "      <td>5048</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>doc_1_page_13_img_3.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_3...</td>\n",
       "      <td>5044</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>doc_1_page_13_img_4.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_4...</td>\n",
       "      <td>27660</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMAGE_ID  SECTION_ID  DOCUMENT_ID SECTION_NUMBER  PAGE  IMG_ORDER  \\\n",
       "0         1          15            1            4.2    12          1   \n",
       "1         2          17            1            4.4    13          1   \n",
       "2         3          17            1            4.4    13          2   \n",
       "3         4          17            1            4.4    13          3   \n",
       "4         5          17            1            4.4    13          4   \n",
       "\n",
       "                IMAGE_FILE                                         IMAGE_PATH  \\\n",
       "0  doc_1_page_12_img_1.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_12_img_1...   \n",
       "1  doc_1_page_13_img_1.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_1...   \n",
       "2  doc_1_page_13_img_2.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_2...   \n",
       "3  doc_1_page_13_img_3.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_3...   \n",
       "4  doc_1_page_13_img_4.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_4...   \n",
       "\n",
       "   IMAGE_SIZE  IMAGE_WIDTH  IMAGE_HEIGHT                       CREATED_AT  \n",
       "0        4997          166           120 2025-04-25 07:11:33.078000-07:00  \n",
       "1       24386          328           237 2025-04-25 07:11:33.078000-07:00  \n",
       "2        5048          166           121 2025-04-25 07:11:33.078000-07:00  \n",
       "3        5044          166           121 2025-04-25 07:11:33.078000-07:00  \n",
       "4       27660          328           237 2025-04-25 07:11:33.078000-07:00  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM IMAGES;\n",
    "    \"\"\")\n",
    "\n",
    "    images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    print(\"Extracting Images from PDF files\")\n",
    "    \n",
    "    for idx,row in tqdm(enumerate(documents_df.iterrows()), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        extract_images_from_pdf(file_path, manual_id, output_dir=\"Washer_Images\", verbose = 0)\n",
    "\n",
    "    print(\"Creating table IMAGES\")\n",
    "    images_df = generate_image_table(documents_df, sections_df, \".\\\\Washer_Images\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE IMAGES (\n",
    "        IMAGE_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        SECTION_ID INT NOT NULL,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        SECTION_NUMBER STRING NOT NULL,\n",
    "        PAGE INT,\n",
    "        IMG_ORDER INT,\n",
    "        IMAGE_FILE STRING,\n",
    "        IMAGE_PATH STRING,\n",
    "        IMAGE_SIZE NUMBER,\n",
    "        IMAGE_WIDTH NUMBER,\n",
    "        IMAGE_HEIGHT NUMBER,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID),\n",
    "            \n",
    "        CONSTRAINT fk_section\n",
    "                FOREIGN KEY (SECTION_ID)\n",
    "                REFERENCES SECTIONS(SECTION_ID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=images_df,\n",
    "        database =database,\n",
    "        table_name=\"IMAGES\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM IMAGES;\n",
    "    \"\"\")\n",
    "    images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "images_df.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VestasVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "authorEmail": "emhaldemo1@gmail.com",
   "authorId": "3960725274243",
   "authorName": "EMHALDEMO1",
   "lastEditTime": 1744786105031,
   "notebookId": "yjjjwqle6a6h6njufd4n",
   "sessionId": "0bd143f8-a219-40e9-853f-0ddbdab7a3c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

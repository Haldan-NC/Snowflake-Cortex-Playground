{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## This Notebook will create the database for the temporary project washing machine manuals\n",
    "\n",
    "The intention of this notebook is to create a clean and best practice database structure, along with utilizing snowflakes AI functions.\n",
    "\n",
    "The intended database structure is as follows: \n",
    "\n",
    "- **documents** (Stores metadata about each manual)  \n",
    "  - `document_id` (Unique ID for each manual)  \n",
    "  - `doc_name` (Document name)\n",
    "  - `version` (Version or revision number)  \n",
    "  - `relative_path` (Original PDF file path or S3 URL) \n",
    "  - `stage_name`  (snowflake stage name (source))\n",
    "  - `size`  (size in bytes of the PDF document) \n",
    "\n",
    "- **sections** (Defines logical sections and subsections within each manual)  \n",
    "  - `section_id` (Unique ID for the section)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `title` (Title or heading of the section)  \n",
    "  - `order_num` (Numerical order of the section in the manual)  \n",
    "  - `parent_section_id` (Optional FK for nested subsections)  \n",
    "\n",
    "- **chunks small** (1024 characters, 64 overlap)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **chunks large** (4096 characters, overlap 256)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **images** (Stores references to images extracted from the manual)  \n",
    "  - `image_id` (Unique ID for the image)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `page_number` \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `order_num` (Display order within the section)  \n",
    "  - `image_path` (S3 or web-accessible path to the image)  \n",
    "  - `description`   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fc673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keyring\n",
    "import os \n",
    "import snowflake.connector as sf_connector # ( https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect)\n",
    "from snowflake.connector.pandas_tools import write_pandas # (https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#write_pandas)\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.evaluation import load_evaluator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from io import BytesIO\n",
    "import fitz \n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Set max rows to display in pandas DataFrame 200\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90ca260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Identifier:  EPTJRCA-HWB83214\n",
      "User Name:  EMHALDEMO1\n",
      "Database:  WASHING_MACHINE_MANUALS\n",
      "Schema:  PUBLIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x18d2efa0ce0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    open_ai_api_key = keyring.get_password(\"openai api\", \"api_key\")\n",
    "    if open_ai_api_key is None:\n",
    "        raise ValueError(\"API key not found in keyring.\")\n",
    "except:\n",
    "    print(\"Please set your OpenAI API key in microsoft generic credential manager with the hostname 'openai api' and username 'api_key'\")\n",
    "    open_ai_api_key = None\n",
    "    \n",
    "try:\n",
    "    account_identifier = keyring.get_password('NC_Snowflake_Trial_Account_Name', 'account_identifier')\n",
    "    user_name = \"EMHALDEMO1\" # Change this to your Snowflake user name\n",
    "    password = keyring.get_password('NC_Snowflake_Trial_User_Password', user_name)\n",
    "    database = \"WASHING_MACHINE_MANUALS\"\n",
    "    schema = \"PUBLIC\"\n",
    "except:\n",
    "    print(\"Please set your Snowflake account identifier and user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_Account_Name' and username 'account_identifier'\")\n",
    "    print(\"Please set your Snowflake user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_User_Password' and username \")\n",
    "\n",
    "client = OpenAI(api_key = open_ai_api_key)\n",
    "\n",
    "print(\"Account Identifier: \", account_identifier)\n",
    "print(\"User Name: \", user_name)\n",
    "print(\"Database: \", database)\n",
    "print(\"Schema: \", schema)\n",
    "\n",
    "try:\n",
    "    connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": database,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "except:\n",
    "        connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"schema\": \"CORTEX\"\n",
    "    }\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = sf_connector.connect(\n",
    "    user=connection_parameters['user'],\n",
    "    password=connection_parameters['password'],\n",
    "    account=connection_parameters['account_identifier'],\n",
    "    warehouse=connection_parameters['warehouse'],\n",
    "    database=connection_parameters['database'],\n",
    "    schema=connection_parameters['schema'],\n",
    "    role=connection_parameters['role']\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\" CREATE DATABASE IF NOT EXISTS {database}; \")\n",
    "cursor.execute(f\" CREATE SCHEMA IF NOT EXISTS {database}.{schema}; \")\n",
    "cursor.execute(f\" USE DATABASE {database}; \")\n",
    "cursor.execute(f\" USE SCHEMA {schema}; \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055a0e",
   "metadata": {},
   "source": [
    "## Create a stage for the PDF files with the code below\n",
    " Try Except wrapping the code should catch if you already have the database and data created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530a54",
   "metadata": {},
   "source": [
    "## Creating documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcced24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table DOCUMENTS\n",
      "Table DOCUMENTS created. Fetching data from PDF files.\n",
      "Document number: 0  : .\\Washer_Manuals\\WAK20160IN.pdf\n",
      "Document number: 1  : .\\Washer_Manuals\\WAN28258GB.pdf\n",
      "Document number: 2  : .\\Washer_Manuals\\WAN28282GC.pdf\n",
      "Document number: 3  : .\\Washer_Manuals\\WAT24168IN.pdf\n",
      "Document number: 4  : .\\Washer_Manuals\\WAV28KH3GB.pdf\n",
      "Document number: 5  : .\\Washer_Manuals\\WFL2050.pdf\n",
      "Document number: 6  : .\\Washer_Manuals\\WGA1340SIN.pdf\n",
      "Document number: 7  : .\\Washer_Manuals\\WGA1420SIN.pdf\n",
      "Document number: 8  : .\\Washer_Manuals\\WGE03408GB.pdf\n",
      "Document number: 9  : .\\Washer_Manuals\\WGG254Z0GB.pdf\n",
      "Success: True, Number of chunks: 1, Number of rows: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_NAME</th>\n",
       "      <th>DOC_VERSION</th>\n",
       "      <th>FILE_PATH</th>\n",
       "      <th>FILE_SIZE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WAK20160IN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAK20160IN.pdf</td>\n",
       "      <td>5052759</td>\n",
       "      <td>2025-04-28 06:16:29.763000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WAN28258GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAN28258GB.pdf</td>\n",
       "      <td>3374759</td>\n",
       "      <td>2025-04-28 06:16:29.763000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>WAN28282GC.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAN28282GC.pdf</td>\n",
       "      <td>3004904</td>\n",
       "      <td>2025-04-28 06:16:29.763000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>WAT24168IN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAT24168IN.pdf</td>\n",
       "      <td>4721819</td>\n",
       "      <td>2025-04-28 06:16:29.763000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>WAV28KH3GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAV28KH3GB.pdf</td>\n",
       "      <td>5686613</td>\n",
       "      <td>2025-04-28 06:16:29.763000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID   DOCUMENT_NAME DOC_VERSION                        FILE_PATH  \\\n",
       "0            1  WAK20160IN.pdf         N/A  .\\Washer_Manuals\\WAK20160IN.pdf   \n",
       "1            2  WAN28258GB.pdf         N/A  .\\Washer_Manuals\\WAN28258GB.pdf   \n",
       "2            3  WAN28282GC.pdf         N/A  .\\Washer_Manuals\\WAN28282GC.pdf   \n",
       "3            4  WAT24168IN.pdf         N/A  .\\Washer_Manuals\\WAT24168IN.pdf   \n",
       "4            5  WAV28KH3GB.pdf         N/A  .\\Washer_Manuals\\WAV28KH3GB.pdf   \n",
       "\n",
       "   FILE_SIZE                       CREATED_AT  \n",
       "0    5052759 2025-04-28 06:16:29.763000-07:00  \n",
       "1    3374759 2025-04-28 06:16:29.763000-07:00  \n",
       "2    3004904 2025-04-28 06:16:29.763000-07:00  \n",
       "3    4721819 2025-04-28 06:16:29.763000-07:00  \n",
       "4    5686613 2025-04-28 06:16:29.763000-07:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "    \"\"\")\n",
    "\n",
    "    documents_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table DOCUMENTS\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE DOCUMENTS (\n",
    "        DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_NAME STRING,\n",
    "        DOC_VERSION STRING,\n",
    "        FILE_PATH STRING NOT NULL,\n",
    "        FILE_SIZE NUMBER,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table DOCUMENTS created. Fetching data from PDF files.\")\n",
    "    pdf_files_path = \".\\\\Washer_Manuals\"\n",
    "    document_rows = []\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(pdf_files_path)):\n",
    "        # Temporary filter to only process a set of PDF files\n",
    "        # if filename not in [\"WGG254Z0GB.pdf\", \"WGA1420SIN.pdf\"]: #,\"WAV28KH3GB.pdf\"]:\n",
    "        #     continue\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_files_path, filename)\n",
    "            print(f\"Document number: {idx}  : {file_path}\")\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            document_rows.append({\n",
    "                \"DOCUMENT_NAME\": filename,\n",
    "                \"FILE_PATH\": file_path,\n",
    "                \"DOC_VERSION\": \"N/A\",  # Placeholder, you can modify this logic as needed\n",
    "                \"FILE_SIZE\": file_size\n",
    "            })\n",
    "\n",
    "    documents_df = pd.DataFrame(document_rows)\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=documents_df,\n",
    "        database =database,\n",
    "        table_name=\"DOCUMENTS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    create_dataframe = False\n",
    "    print(f\"Success: {success}, Number of chunks: {nchunks}, Number of rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "    \"\"\")\n",
    "\n",
    "    documents_df = cursor.fetch_pandas_all()\n",
    "\n",
    "\n",
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524735a",
   "metadata": {},
   "source": [
    "# The section below focuses on creating chunks_large and chunks_small.\n",
    "\n",
    "Different size chunks are good at different things - it could be a good idea to store both size, especially during testing\n",
    "\n",
    "To include page numbers, i decided to create the tables using pandas, and then uploading them to snowflake\n",
    "\n",
    "Followed by that will be a query to crete a vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ab02b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_LARGE\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "\n",
    "    large_chunks_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table CHUNKS_LARGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e88f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_LARGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:15<01:10, 14.09s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "100%|██████████| 10/10 [01:47<00:00, 10.75s/it]\n"
     ]
    }
   ],
   "source": [
    "## Extracting section headers from the PDF files\n",
    "\n",
    "def extract_text_chunks(file_path, manual_id, chunk_size=512, chunk_overlap=128):\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Step 1: Combine all text across pages with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = []  # (char_index, page_number)\n",
    "\n",
    "    for doc_page in docs:\n",
    "        text = doc_page.page_content.strip().replace('\\n', ' ')\n",
    "        start_idx = len(all_text)\n",
    "        all_text += text + \" \"  # Add space to separate pages\n",
    "        end_idx = len(all_text)\n",
    "        page_map.append((start_idx, end_idx, doc_page.metadata['page']))\n",
    "\n",
    "    # Step 2: Create chunks with overlap, spanning across pages\n",
    "    chunks = []\n",
    "    chunk_order = []\n",
    "    page_start_list = []\n",
    "    page_end_list = []\n",
    "\n",
    "    idx = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while idx < len(all_text):\n",
    "        chunk = all_text[idx:idx + chunk_size]\n",
    "\n",
    "        # Determine pages involved in this chunk\n",
    "        chunk_start = idx\n",
    "        chunk_end = idx + len(chunk)\n",
    "\n",
    "        pages_in_chunk = [\n",
    "            page_num\n",
    "            for start, end, page_num in page_map\n",
    "            if not (end <= chunk_start or start >= chunk_end)  # overlap condition\n",
    "        ]\n",
    "\n",
    "        page_start = min(pages_in_chunk) if pages_in_chunk else None\n",
    "        page_end = max(pages_in_chunk) if pages_in_chunk else None\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        page_start_list.append(page_start)\n",
    "        page_end_list.append(page_end)\n",
    "        chunk_order.append(chunk_idx)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        idx += chunk_size - chunk_overlap\n",
    "\n",
    "    # Step 3: Create DataFrame\n",
    "    rows = [{\n",
    "        'DOCUMENT_ID': manual_id,\n",
    "        'PAGE_START_NUMBER': start,\n",
    "        'PAGE_END_NUMBER': end,\n",
    "        'CHUNK_TEXT': chunk,\n",
    "        'CHUNK_ORDER': order\n",
    "    } for chunk, start, end, order in zip(chunks, page_start_list, page_end_list, chunk_order)]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"DOCUMENT_ID\", \"PAGE_START_NUMBER\", \"PAGE_END_NUMBER\", \"CHUNK_TEXT\", \"CHUNK_ORDER\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "if create_dataframe:\n",
    "    print(\"Creating table CHUNKS_LARGE\")\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "\n",
    "    large_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 6000,#1024,\n",
    "                            chunk_overlap = 128)  # Show first 5 chunks\n",
    "        large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "else: \n",
    "    print(\"Loading the existing data from Snowflake into a DataFrame\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "    large_chunks_df = cursor.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc67cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the large chunks DataFrame to Snowflake\n",
      "Success: True, Chunks: 1, Rows: 133\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Washing machine WAK20160IN WAK24168IN en Instr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>ith reduced Levelling . . . . . . . . . . . . ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>dditional Start the ~ Page 18 programme defaul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>fabric for which it is suita- * Reduced load f...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>the programme as You can select \"Rinse hold\" t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  \\\n",
       "0            1                  0                3   \n",
       "1            1                  3                7   \n",
       "2            1                  7               12   \n",
       "3            1                 12               15   \n",
       "4            1                 15               18   \n",
       "\n",
       "                                          CHUNK_TEXT  CHUNK_ORDER  \n",
       "0  Washing machine WAK20160IN WAK24168IN en Instr...            0  \n",
       "1  ith reduced Levelling . . . . . . . . . . . . ...            1  \n",
       "2  dditional Start the ~ Page 18 programme defaul...            2  \n",
       "3  fabric for which it is suita- * Reduced load f...            3  \n",
       "4  the programme as You can select \"Rinse hold\" t...            4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if create_dataframe:\n",
    "    print(\"Writing the large chunks DataFrame to Snowflake\")\n",
    "    # Write the DataFrame to Snowflake\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=large_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_LARGE\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    # Update the embeddings for the chunks in the CHUNKS_LARGE table\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_LARGE\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "else: \n",
    "    print(\"Skipping the embedding update as the data already exists in Snowflake\")\n",
    "\n",
    "large_chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3b343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table CHUNKS_SMALL\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table CHUNKS_SMALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11219e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:11<01:08, 13.71s/it]CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Washing machine WAK20160IN WAK24168IN en Instr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-sales information. service centres. 1. 2. 3. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ect additional programme 7 settings . . . . . ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hing off the appliance . . . . . . . .21 Prepa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>. . . . .15 Detergent drawer and housing . . ....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  \\\n",
       "0            1                  0                1   \n",
       "1            1                  1                2   \n",
       "2            1                  2                2   \n",
       "3            1                  2                2   \n",
       "4            1                  2                3   \n",
       "\n",
       "                                          CHUNK_TEXT  CHUNK_ORDER  \n",
       "0  Washing machine WAK20160IN WAK24168IN en Instr...            0  \n",
       "1  -sales information. service centres. 1. 2. 3. ...            1  \n",
       "2  ect additional programme 7 settings . . . . . ...            2  \n",
       "3  hing off the appliance . . . . . . . .21 Prepa...            3  \n",
       "4  . . . . .15 Detergent drawer and housing . . ....            4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if create_dataframe:\n",
    "\n",
    "    small_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 1024,\n",
    "                            chunk_overlap = 64)  # Show first 5 chunks\n",
    "        small_chunks_df = pd.concat([small_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=small_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_SMALL\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    # Update the embeddings for the small chunks\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_SMALL\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "else: \n",
    "    print(\"Skipping the embedding update as the data already exists in Snowflake\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "\n",
    "small_chunks_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b8c0f",
   "metadata": {},
   "source": [
    "## Creating sections table using LLM for TOC extraction\n",
    "\n",
    "The function `extract_TOC` takes quite a while due to the chunk size and the model. This can be tampered with, but i found most consistent results with said model. I also think that larger chunks are better for this task, as the model can see context of the first few pages, and it also ensures that the table of contents is included in the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ea5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOC(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = cursor.execute(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\")\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "\n",
    "    return cursor.fetch_pandas_all().iloc[0,0]\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "\n",
    "# llm_output = extract_TOC(large_chunks_df.loc[0,\"CHUNK_TEXT\"], model = 'llama3.1-70b')\n",
    "# llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fec25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_llm_output(llm_output: str) -> dict:\n",
    "    try:\n",
    "        # Confirming that a JSON block is returned\n",
    "        match = re.search(r\"```\\s*(\\{.*?\\})\\s*```\", llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        # Extracting sub string (json string)\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Clean common JSON errors (e.g., trailing commas)\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)  # remove trailing commas before ] or }\n",
    "        \n",
    "        # Parse string to json\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "        \n",
    "# parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "# print(json.dumps(parsed_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_sections(node, parent_section=None):\n",
    "    rows = []\n",
    "\n",
    "    # Get info from the current node\n",
    "    section = node.get(\"Section\")\n",
    "    section_number = node.get(\"Section Number\")\n",
    "    page = node.get(\"Page\")\n",
    "\n",
    "    # The levenshtein distance is used to ensure the the section called \"Table of Contents\" is not added to the dataframe\n",
    "    evaluator = load_evaluator(\"string_distance\")\n",
    "    levenshtein_score_toc = evaluator.evaluate_strings(\n",
    "    prediction=section,\n",
    "    reference=\"Table of Contents\",\n",
    "    metric=\"levenshtein\"\n",
    "    )[\"score\"]  # This will be a float between 0 and 1, where 0 means identical\n",
    "\n",
    "    if levenshtein_score_toc > 0.1:  # if the levenshtein distance is very small its likely to match \"Table of Contents\"\n",
    "        rows.append({\n",
    "            \"SECTION\": section,\n",
    "            \"SECTION_NUMBER\": section_number,\n",
    "            \"PAGE\": page,\n",
    "            \"PARENT_SECTION_NUMBER\": parent_section\n",
    "        })\n",
    "\n",
    "    # Recurse into each sub-section, if any\n",
    "    for subsection in node.get(\"Sub Sections\", []):\n",
    "        rows.extend(traverse_sections(subsection, parent_section=section_number))\n",
    "\n",
    "    return rows\n",
    "\n",
    "# flat_rows = traverse_sections(parsed_dict)\n",
    "# toc_df = pd.DataFrame(flat_rows)\n",
    "# toc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b9c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\"):\n",
    "    df_list = []\n",
    "\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        first_chunk_of_doc = large_chunks_df.loc[large_chunks_df[\"DOCUMENT_ID\"] == manual_id, \"CHUNK_TEXT\"].iloc[0]\n",
    "        # print(\"First chunk:\", first_chunk_of_doc)\n",
    "\n",
    "        llm_output = extract_TOC(first_chunk_of_doc, model = model)\n",
    "        parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "        flat_rows = traverse_sections(parsed_dict)\n",
    "        local_toc_df = pd.DataFrame(flat_rows)\n",
    "        local_toc_df[\"DOCUMENT_ID\"] = manual_id\n",
    "        df_list.append(local_toc_df)\n",
    "\n",
    "        break\n",
    "\n",
    "    toc_df = pd.concat(df_list, ignore_index=True)\n",
    "    toc_df[\"SECTION_NUMBER\"] = toc_df[\"SECTION_NUMBER\"].astype(str)\n",
    "    return toc_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73257e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table SECTIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:43<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime in seconds: 43.7676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "100071 (22000): Failed to cast variant value \"\" to FIXED",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      2\u001b[39m create_dataframe = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43mSELECT * \u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43mFROM SECTIONS;\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m sections_df = cursor.fetch_pandas_all()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\cursor.py:1104\u001b[39m, in \u001b[36mSnowflakeCursor.execute\u001b[39m\u001b[34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[39m\n\u001b[32m   1103\u001b[39m     error_class = IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:283\u001b[39m, in \u001b[36mError.errorhandler_wrapper\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m \u001b[33;03m    exception to the first handler in that order.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m handed_over = \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:338\u001b[39m, in \u001b[36mError.hand_to_other_handler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    337\u001b[39m cursor.messages.append((error_class, error_value))\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:214\u001b[39m, in \u001b[36mError.default_errorhandler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    213\u001b[39m done_format_msg = error_value.get(\u001b[33m\"\u001b[39m\u001b[33mdone_format_msg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[32m    215\u001b[39m     msg=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    216\u001b[39m     errno=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[32m    217\u001b[39m     sqlstate=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msqlstate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    218\u001b[39m     sfqid=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    219\u001b[39m     query=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    220\u001b[39m     done_format_msg=(\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[32m    222\u001b[39m     ),\n\u001b[32m    223\u001b[39m     connection=connection,\n\u001b[32m    224\u001b[39m     cursor=cursor,\n\u001b[32m    225\u001b[39m )\n",
      "\u001b[31mProgrammingError\u001b[39m: 002003 (42S02): SQL compilation error:\nObject 'SECTIONS' does not exist or not authorized.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     12\u001b[39m cursor.execute(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m    CREATE OR REPLACE TABLE SECTIONS (\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m    SECTION_ID INT AUTOINCREMENT PRIMARY KEY,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33m);\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     27\u001b[39m sections_df = create_TOC_table(documents_df, large_chunks_df, model =\u001b[33m\"\u001b[39m\u001b[33mllama3.1-70b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m success, nchunks, nrows, output = \u001b[43mwrite_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43msections_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSECTIONS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_create_table\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccess\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnchunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrows\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m time.sleep(\u001b[32m3\u001b[39m) \u001b[38;5;66;03m# Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\pandas_tools.py:591\u001b[39m, in \u001b[36mwrite_pandas\u001b[39m\u001b[34m(conn, df, table_name, database, schema, chunk_size, compression, on_error, parallel, quote_identifiers, auto_create_table, create_temp_table, overwrite, table_type, use_logical_type, iceberg_config, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m params = (target_table_location, on_error)\n\u001b[32m    590\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcopying into with \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcopy_into_sql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. params: %s\u001b[39m\u001b[33m\"\u001b[39m, params)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m copy_results = \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy_into_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_internal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_force_qmark_paramstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_statements\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.fetchall()\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m auto_create_table:\n\u001b[32m    600\u001b[39m     original_table_location = build_location_helper(\n\u001b[32m    601\u001b[39m         database=database,\n\u001b[32m    602\u001b[39m         schema=schema,\n\u001b[32m    603\u001b[39m         name=table_name,\n\u001b[32m    604\u001b[39m         quote_identifiers=quote_identifiers,\n\u001b[32m    605\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\cursor.py:1104\u001b[39m, in \u001b[36mSnowflakeCursor.execute\u001b[39m\u001b[34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[39m\n\u001b[32m   1100\u001b[39m     is_integrity_error = (\n\u001b[32m   1101\u001b[39m         code == \u001b[33m\"\u001b[39m\u001b[33m100072\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[32m   1103\u001b[39m     error_class = IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:283\u001b[39m, in \u001b[36mError.errorhandler_wrapper\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merrorhandler_wrapper\u001b[39m(\n\u001b[32m    262\u001b[39m     connection: SnowflakeConnection | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    266\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    267\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m \u001b[33;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     handed_over = \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[32m    290\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Error.errorhandler_make_exception(\n\u001b[32m    291\u001b[39m             error_class,\n\u001b[32m    292\u001b[39m             error_value,\n\u001b[32m    293\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:338\u001b[39m, in \u001b[36mError.hand_to_other_handler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    337\u001b[39m     cursor.messages.append((error_class, error_value))\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\snowflake\\connector\\errors.py:214\u001b[39m, in \u001b[36mError.default_errorhandler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    212\u001b[39m errno = error_value.get(\u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    213\u001b[39m done_format_msg = error_value.get(\u001b[33m\"\u001b[39m\u001b[33mdone_format_msg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[32m    215\u001b[39m     msg=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    216\u001b[39m     errno=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[32m    217\u001b[39m     sqlstate=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msqlstate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    218\u001b[39m     sfqid=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    219\u001b[39m     query=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    220\u001b[39m     done_format_msg=(\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[32m    222\u001b[39m     ),\n\u001b[32m    223\u001b[39m     connection=connection,\n\u001b[32m    224\u001b[39m     cursor=cursor,\n\u001b[32m    225\u001b[39m )\n",
      "\u001b[31mProgrammingError\u001b[39m: 100071 (22000): Failed to cast variant value \"\" to FIXED"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table SECTIONS\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SECTIONS (\n",
    "        SECTION_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        SECTION STRING NOT NULL,\n",
    "        SECTION_NUMBER STRING NOT NULL,\n",
    "        PARENT_SECTION_NUMBER STRING,\n",
    "        PAGE INT,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\")\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=sections_df,\n",
    "        database =database,\n",
    "        table_name=\"SECTIONS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "\n",
    "sections_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8cb1",
   "metadata": {},
   "source": [
    "# Extracting images from the manual\n",
    "\n",
    "This chosen method which appears to be more diverse across the manuals treats each page as an image. This is a good way to ensure that all images are extracted. \n",
    "The downside is that tables and other image like content will be extracted as images. Currently this is a feature not a bug. Adjusting the image extraction method is a task for the future when we have the real PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pdf_to_images(pdf_path, zoom=2.0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i, page in enumerate(doc):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img_data = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append({\n",
    "            \"page_number\": i + 1,\n",
    "            \"image\": img_data\n",
    "        })\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_pdf_page_pixel_size(pdf_image):\n",
    "    width, height = pdf_image.size\n",
    "    return width * height\n",
    "\n",
    "\n",
    "def detect_image_regions(page_image, buffer=0, min_size=70, max_size = 1000, threshold=240):\n",
    "    image = np.array(page_image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Applying blur to reduce fine lines from tables\n",
    "    _, thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > min_size and h > min_size:  # Skip tiny blocks (Maybe reconsider)\n",
    "            regions.append([x - buffer, \n",
    "                            y - buffer, \n",
    "                            x + w + buffer, \n",
    "                            y + h + buffer])\n",
    "            if w * h > max_size:\n",
    "                regions.pop(-1)  \n",
    "    return regions\n",
    "\n",
    "\n",
    "def crop_regions_from_image(page_image, regions, output_dir, page_num, manual_id):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_images = []\n",
    "\n",
    "    for i, coords in enumerate(regions):\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        cropped = page_image.crop((x1, y1, x2, y2))\n",
    "        save_path = os.path.join(output_dir, f\"doc_{manual_id}_page_{page_num}_img_{i+1}.png\")\n",
    "        cropped.save(save_path)\n",
    "        saved_images.append({\n",
    "            \"page\": page_num,\n",
    "            \"image_path\": save_path,\n",
    "            \"coords\": (x1, y1, x2, y2)\n",
    "        })\n",
    "    return saved_images\n",
    "\n",
    "\n",
    "\n",
    "def add_region_to_page(page_image, regions, output_dir, page_num, pdf_path ,color=(0, 255, 0), alpha=50, save=True, verbose=0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to RGBA to allow transparency\n",
    "    annotated = page_image.convert(\"RGBA\")\n",
    "    overlay = Image.new(\"RGBA\", annotated.size, (0, 0, 0, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    for coords in regions:\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color + (alpha,), fill=color + (alpha,))\n",
    "\n",
    "    # Combine original image with overlay\n",
    "    combined = Image.alpha_composite(annotated, overlay)\n",
    "\n",
    "    if save:\n",
    "        save_path = os.path.join(output_dir, f\"page_{page_num:03d}_with_regions_{color}.png\")\n",
    "        combined.convert(\"RGB\").save(save_path)\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved page {page_num} with highlighted regions to {save_path}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def merge_overlapping_regions(regions, buffer=0):\n",
    "    \"\"\"\n",
    "    Merges overlapping or intersecting regions.\n",
    "\n",
    "    Args:\n",
    "        regions (List[List[int]]): List of regions as [x1, y1, x2, y2].\n",
    "        buffer (int): Optional buffer added to each region before checking overlaps.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Merged list of non-overlapping regions.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    # Convert to shapely boxes with optional buffer\n",
    "    boxes = [box(x1 - buffer, y1 - buffer, x2 + buffer, y2 + buffer) for x1, y1, x2, y2 in regions]\n",
    "\n",
    "    # Merge all overlapping boxes (A fix to a previous issues of diagrams being cropped into multiple images)\n",
    "    merged = unary_union(boxes)\n",
    "\n",
    "    # Ensure output is a list of boxes\n",
    "    if merged.geom_type == 'Polygon':\n",
    "        merged_boxes = [merged]\n",
    "    else:\n",
    "        merged_boxes = list(merged.geoms)\n",
    "\n",
    "    # Convert back to [x1, y1, x2, y2] format (round to int)\n",
    "    merged_regions = []\n",
    "    for b in merged_boxes:\n",
    "        x1, y1, x2, y2 = b.bounds\n",
    "        merged_regions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    return merged_regions\n",
    "\n",
    "\n",
    "\n",
    "# This is the main function to extract images from the PDF\n",
    "def extract_images_from_pdf(pdf_path:str, manual_id:int, output_dir: str, verbose:int =0):\n",
    "    rendered_pages = render_pdf_to_images(pdf_path)\n",
    "    all_extracted = []\n",
    "\n",
    "    for page_idx,page in enumerate(rendered_pages):\n",
    "        page_num = page[\"page_number\"] \n",
    "        image = page[\"image\"]\n",
    "        if verbose > 0:\n",
    "            print(f\"Processing page {page_num}...\")\n",
    "\n",
    "        # Detecting regions\n",
    "        regions = detect_image_regions(image , buffer=2, min_size=70, \n",
    "                                        max_size=get_pdf_page_pixel_size(image) * 0.99)\n",
    "        # Creates new regions by merging overlapping regions (this is a fix for cropped images  )\n",
    "        new_regions = merge_overlapping_regions(regions, buffer=0)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Found {len(new_regions)} image regions on page {page_num}\")\n",
    "\n",
    "        if not new_regions:\n",
    "            if verbose > 0:\n",
    "                print(f\"No image regions found on page {page_num}\")\n",
    "            continue\n",
    "        \n",
    "        # Creates an image directory for each PDF file\n",
    "        image_output_dir = pdf_path.split(\"/\")[-1].replace(\".pdf\", \"\").replace(\"Washer_Manuals\", output_dir)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "        # Showing the pages with the masked regions \n",
    "        modified_image = add_region_to_page(image, new_regions, image_output_dir, page_num, pdf_path, color=(0, 0, 255), alpha=50, save = False)\n",
    "\n",
    "        # OLD code \n",
    "        extracted = crop_regions_from_image(\n",
    "            image, new_regions, output_dir=image_output_dir, page_num=page_num, manual_id=manual_id\n",
    "        )\n",
    "        all_extracted.extend(extracted)\n",
    "    return all_extracted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b32ef3",
   "metadata": {},
   "source": [
    "# Creating table for image references and metadata\n",
    "\n",
    "Currently the images are matched to the sections using the page number, which is problematic if the end of section 4.3 is one the same page as the start of section 4.4. On the top of my head i'm not quite sure how to match the images to the sections accurately, but this method yields mostly correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_number_from_filename(filename):\n",
    "    return filename.split(\"_\")[3] if \"_\" in filename else None\n",
    "\n",
    "def generate_image_table(documents_df, sections_df, image_dir):\n",
    "    image_records = []\n",
    "\n",
    "    # Loop over all subdirectories in image_dir\n",
    "    for subfolder in os.listdir(image_dir):\n",
    "        subfolder_path = os.path.join(image_dir, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue  # skip files\n",
    "        \n",
    "        # Match to document by DOCUMENT_NAME (strip extension if needed)\n",
    "        matching_docs = documents_df[documents_df['DOCUMENT_NAME'].str.contains(subfolder, case=False)]\n",
    "        if matching_docs.empty:\n",
    "            print(f\"No matching document for subfolder: {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        document_id = matching_docs.iloc[0]['DOCUMENT_ID']\n",
    "        document_name = matching_docs.iloc[0]['DOCUMENT_NAME']\n",
    "        \n",
    "        # List all image files in subdirectory\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            if not image_file.lower().endswith((\".png\")):\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            page_number = extract_page_number_from_filename(image_file)\n",
    "            order_number = image_file.split(\"img_\")[-1].strip(\".png\")\n",
    "\n",
    "            image_size = os.path.getsize(image_path)\n",
    "            image_width, image_height = Image.open(image_path).size\n",
    "            \n",
    "            # Try to match to a section (same document, closest PAGE <= image page)\n",
    "            section_match = None\n",
    "            if page_number is not None:\n",
    "                matching_sections = sections_df[\n",
    "                    (sections_df['DOCUMENT_ID'] == document_id) & \n",
    "                    (sections_df['PAGE'].astype(str) <= str(page_number))\n",
    "                ]\n",
    "                if not matching_sections.empty:\n",
    "                    section_match = matching_sections.sort_values(\"PAGE\", ascending=False).iloc[0]\n",
    "            \n",
    "            image_records.append({\n",
    "                \"DOCUMENT_ID\": document_id,\n",
    "                \"SECTION_ID\": section_match[\"SECTION_ID\"] if section_match is not None else None,\n",
    "                \"SECTION_NUMBER\": section_match[\"SECTION_NUMBER\"] if section_match is not None else None,\n",
    "                \"PAGE\": page_number,\n",
    "                \"IMG_ORDER\": order_number,\n",
    "                \"IMAGE_FILE\": image_file,\n",
    "                \"IMAGE_PATH\": image_path,\n",
    "                \"IMAGE_SIZE\": image_size,\n",
    "                \"IMAGE_WIDTH\": image_width,\n",
    "                \"IMAGE_HEIGHT\": image_height\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(image_records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f76112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Images from PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table IMAGES\n",
      "Success: True, Chunks: 1, Rows: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE_ID</th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>IMG_ORDER</th>\n",
       "      <th>IMAGE_FILE</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>IMAGE_SIZE</th>\n",
       "      <th>IMAGE_WIDTH</th>\n",
       "      <th>IMAGE_HEIGHT</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_12_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_12_img_1...</td>\n",
       "      <td>4997</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_13_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_1...</td>\n",
       "      <td>24386</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_1_page_13_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_2...</td>\n",
       "      <td>5048</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>doc_1_page_13_img_3.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_3...</td>\n",
       "      <td>5044</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>doc_1_page_13_img_4.png</td>\n",
       "      <td>.\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_4...</td>\n",
       "      <td>27660</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-25 07:11:33.078000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMAGE_ID  SECTION_ID  DOCUMENT_ID SECTION_NUMBER  PAGE  IMG_ORDER  \\\n",
       "0         1          15            1            4.2    12          1   \n",
       "1         2          17            1            4.4    13          1   \n",
       "2         3          17            1            4.4    13          2   \n",
       "3         4          17            1            4.4    13          3   \n",
       "4         5          17            1            4.4    13          4   \n",
       "\n",
       "                IMAGE_FILE                                         IMAGE_PATH  \\\n",
       "0  doc_1_page_12_img_1.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_12_img_1...   \n",
       "1  doc_1_page_13_img_1.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_1...   \n",
       "2  doc_1_page_13_img_2.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_2...   \n",
       "3  doc_1_page_13_img_3.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_3...   \n",
       "4  doc_1_page_13_img_4.png  .\\Washer_Images\\WGA1420SIN\\doc_1_page_13_img_4...   \n",
       "\n",
       "   IMAGE_SIZE  IMAGE_WIDTH  IMAGE_HEIGHT                       CREATED_AT  \n",
       "0        4997          166           120 2025-04-25 07:11:33.078000-07:00  \n",
       "1       24386          328           237 2025-04-25 07:11:33.078000-07:00  \n",
       "2        5048          166           121 2025-04-25 07:11:33.078000-07:00  \n",
       "3        5044          166           121 2025-04-25 07:11:33.078000-07:00  \n",
       "4       27660          328           237 2025-04-25 07:11:33.078000-07:00  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM IMAGES;\n",
    "    \"\"\")\n",
    "\n",
    "    images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    print(\"Extracting Images from PDF files\")\n",
    "    \n",
    "    for idx,row in tqdm(enumerate(documents_df.iterrows()), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        extract_images_from_pdf(file_path, manual_id, output_dir=\"Washer_Images\", verbose = 0)\n",
    "\n",
    "    print(\"Creating table IMAGES\")\n",
    "    images_df = generate_image_table(documents_df, sections_df, \".\\\\Washer_Images\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE IMAGES (\n",
    "        IMAGE_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        SECTION_ID INT NOT NULL,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        SECTION_NUMBER STRING NOT NULL,\n",
    "        PAGE INT,\n",
    "        IMG_ORDER INT,\n",
    "        IMAGE_FILE STRING,\n",
    "        IMAGE_PATH STRING,\n",
    "        IMAGE_SIZE NUMBER,\n",
    "        IMAGE_WIDTH NUMBER,\n",
    "        IMAGE_HEIGHT NUMBER,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID),\n",
    "            \n",
    "        CONSTRAINT fk_section\n",
    "                FOREIGN KEY (SECTION_ID)\n",
    "                REFERENCES SECTIONS(SECTION_ID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=images_df,\n",
    "        database =database,\n",
    "        table_name=\"IMAGES\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM IMAGES;\n",
    "    \"\"\")\n",
    "    images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "images_df.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VestasVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "authorEmail": "emhaldemo1@gmail.com",
   "authorId": "3960725274243",
   "authorName": "EMHALDEMO1",
   "lastEditTime": 1744786105031,
   "notebookId": "yjjjwqle6a6h6njufd4n",
   "sessionId": "0bd143f8-a219-40e9-853f-0ddbdab7a3c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

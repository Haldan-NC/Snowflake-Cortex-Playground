{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## This Notebook will create the database for the temporary project washing machine manuals\n",
    "\n",
    "The intention of this notebook is to create a clean and best practice database structure, along with utilizing snowflakes AI functions.\n",
    "\n",
    "The intended database structure is as follows: \n",
    "\n",
    "- **documents** (Stores metadata about each manual)  \n",
    "  - `document_id` (Unique ID for each manual)  \n",
    "  - `doc_name` (Document name)\n",
    "  - `version` (Version or revision number)  \n",
    "  - `relative_path` (Original PDF file path or S3 URL) \n",
    "  - `stage_name`  (snowflake stage name (source))\n",
    "  - `size`  (size in bytes of the PDF document) \n",
    "\n",
    "- **sections** (Defines logical sections and subsections within each manual)  \n",
    "  - `section_id` (Unique ID for the section)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `title` (Title or heading of the section)  \n",
    "  - `order_num` (Numerical order of the section in the manual)  \n",
    "  - `parent_section_id` (Optional FK for nested subsections)  \n",
    "\n",
    "- **chunks small** (1024 characters, 64 overlap)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **chunks large** (4096 characters, overlap 256)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **images** (Stores references to images extracted from the manual)  \n",
    "  - `image_id` (Unique ID for the image)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `page_number` \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `order_num` (Display order within the section)  \n",
    "  - `image_path` (S3 or web-accessible path to the image)  \n",
    "  - `description`   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fc673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keyring\n",
    "import os \n",
    "import snowflake.connector as sf_connector # ( https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect)\n",
    "from snowflake.connector.pandas_tools import write_pandas # (https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#write_pandas)\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.evaluation import load_evaluator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from io import BytesIO\n",
    "import fitz \n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "# Set max rows to display in pandas DataFrame 200\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90ca260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Identifier:  EPTJRCA-HWB83214\n",
      "User Name:  EMHALDEMO1\n",
      "Database:  WASHING_MACHINE_MANUALS\n",
      "Schema:  PUBLIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x1f2b5275190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_identifier = keyring.get_password('NC_Snowflake_Trial_Account_Name', 'account_identifier')\n",
    "user_name = \"EMHALDEMO1\"\n",
    "password = keyring.get_password('NC_Snowflake_Trial_User_Password', user_name)\n",
    "database = \"WASHING_MACHINE_MANUALS\"\n",
    "schema = \"PUBLIC\"\n",
    "\n",
    "print(\"Account Identifier: \", account_identifier)\n",
    "print(\"User Name: \", user_name)\n",
    "print(\"Database: \", database)\n",
    "print(\"Schema: \", schema)\n",
    "\n",
    "try:\n",
    "    connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": database,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "except:\n",
    "        connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"schema\": \"CORTEX\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = sf_connector.connect(\n",
    "    user=connection_parameters['user'],\n",
    "    password=connection_parameters['password'],\n",
    "    account=connection_parameters['account_identifier'],\n",
    "    warehouse=connection_parameters['warehouse'],\n",
    "    database=connection_parameters['database'],\n",
    "    schema=connection_parameters['schema'],\n",
    "    role=connection_parameters['role']\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\" CREATE DATABASE IF NOT EXISTS {database}; \")\n",
    "cursor.execute(f\" CREATE SCHEMA IF NOT EXISTS {database}.{schema}; \")\n",
    "cursor.execute(f\" USE DATABASE {database}; \")\n",
    "cursor.execute(f\" USE SCHEMA {schema}; \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055a0e",
   "metadata": {},
   "source": [
    "## Create a stage for the PDF files with the code below\n",
    "#### DO NOT RUN - unless you don't have the documents in the stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bb5ea-b91f-4578-aa76-6c1389acd1aa",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x1943df39760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating stage to dump PDF documents into\n",
    "# cursor.execute(\" create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true ); \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530a54",
   "metadata": {},
   "source": [
    "## Creating documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c452e4c-6ecc-418e-a988-0ee231cf5ae6",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x269b7eb5910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE DOCUMENTS (\n",
    "    DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_NAME STRING,\n",
    "    DOC_VERSION STRING,\n",
    "    FILE_PATH STRING NOT NULL,\n",
    "    FILE_SIZE NUMBER,\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1f2c9-1d83-48c5-8be6-40c1760576c8",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "# The section below focuses on creating chunks_large and chunks_small.\n",
    "\n",
    "Different size chunks are good at different things - it could be a good idea to store both size, especially during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcced24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number: 8  : .\\Washer_Manuals\\WAV28KH3GB.pdf\n",
      "Document number: 11  : .\\Washer_Manuals\\WGA1420SIN.pdf\n",
      "Document number: 13  : .\\Washer_Manuals\\WGG254Z0GB.pdf\n",
      "    DOCUMENT_NAME                        FILE_PATH DOC_VERSION  FILE_SIZE\n",
      "0  WAV28KH3GB.pdf  .\\Washer_Manuals\\WAV28KH3GB.pdf         N/A    5686613\n",
      "1  WGA1420SIN.pdf  .\\Washer_Manuals\\WGA1420SIN.pdf         N/A    3247850\n",
      "2  WGG254Z0GB.pdf  .\\Washer_Manuals\\WGG254Z0GB.pdf         N/A    3291555\n"
     ]
    }
   ],
   "source": [
    "pdf_files_path = \".\\\\Washer_Manuals\"\n",
    "document_rows = []\n",
    "\n",
    "for idx, filename in enumerate(os.listdir(pdf_files_path)):\n",
    "    # Temporary filter to only process a set of PDF files\n",
    "    if filename not in [\"WGG254Z0GB.pdf\", \"WGA1420SIN.pdf\",\"WAV28KH3GB.pdf\"]:\n",
    "        continue\n",
    "\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_files_path, filename)\n",
    "        print(f\"Document number: {idx}  : {file_path}\")\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        document_rows.append({\n",
    "            \"DOCUMENT_NAME\": filename,\n",
    "            \"FILE_PATH\": file_path,\n",
    "            \"DOC_VERSION\": \"N/A\",  # Placeholder, you can modify this logic as needed\n",
    "            \"FILE_SIZE\": file_size\n",
    "        })\n",
    "\n",
    "documents_df = pd.DataFrame(document_rows)\n",
    "print(documents_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67141627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_NAME</th>\n",
       "      <th>DOC_VERSION</th>\n",
       "      <th>FILE_PATH</th>\n",
       "      <th>FILE_SIZE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WAV28KH3GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAV28KH3GB.pdf</td>\n",
       "      <td>5686613</td>\n",
       "      <td>2025-04-22 01:52:03.390000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WGA1420SIN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGA1420SIN.pdf</td>\n",
       "      <td>3247850</td>\n",
       "      <td>2025-04-22 01:52:03.390000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>WGG254Z0GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGG254Z0GB.pdf</td>\n",
       "      <td>3291555</td>\n",
       "      <td>2025-04-22 01:52:03.390000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>WAV28KH3GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAV28KH3GB.pdf</td>\n",
       "      <td>5686613</td>\n",
       "      <td>2025-04-25 04:44:31.675000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>WGA1420SIN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGA1420SIN.pdf</td>\n",
       "      <td>3247850</td>\n",
       "      <td>2025-04-25 04:44:31.675000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID   DOCUMENT_NAME DOC_VERSION                        FILE_PATH  \\\n",
       "0            1  WAV28KH3GB.pdf         N/A  .\\Washer_Manuals\\WAV28KH3GB.pdf   \n",
       "1            2  WGA1420SIN.pdf         N/A  .\\Washer_Manuals\\WGA1420SIN.pdf   \n",
       "2            3  WGG254Z0GB.pdf         N/A  .\\Washer_Manuals\\WGG254Z0GB.pdf   \n",
       "3          101  WAV28KH3GB.pdf         N/A  .\\Washer_Manuals\\WAV28KH3GB.pdf   \n",
       "4          102  WGA1420SIN.pdf         N/A  .\\Washer_Manuals\\WGA1420SIN.pdf   \n",
       "\n",
       "   FILE_SIZE                       CREATED_AT  \n",
       "0    5686613 2025-04-22 01:52:03.390000-07:00  \n",
       "1    3247850 2025-04-22 01:52:03.390000-07:00  \n",
       "2    3291555 2025-04-22 01:52:03.390000-07:00  \n",
       "3    5686613 2025-04-25 04:44:31.675000-07:00  \n",
       "4    3247850 2025-04-25 04:44:31.675000-07:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=documents_df,\n",
    "    database =database,\n",
    "    table_name=\"DOCUMENTS\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "\"\"\")\n",
    "\n",
    "documents_df = cursor.fetch_pandas_all()\n",
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524735a",
   "metadata": {},
   "source": [
    "## Creating chunks tables with vector embeddings\n",
    "\n",
    "To include page numbers, i decided to create the tables using pandas, and then uploading them to snowflake\n",
    "\n",
    "Followed by that will be a query to crete a vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e88f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m     manual_id = row[\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mDOCUMENT_ID\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     67\u001b[39m     file_path = os.path.join(pdf_files_path, row[\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mDOCUMENT_NAME\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     tmp_chunked_df = \u001b[43mextract_text_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmanual_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#1024,\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Show first 5 chunks\u001b[39;00m\n\u001b[32m     73\u001b[39m     large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     75\u001b[39m large_chunks_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mextract_text_chunks\u001b[39m\u001b[34m(file_path, manual_id, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_text_chunks\u001b[39m(file_path, manual_id, chunk_size=\u001b[32m512\u001b[39m, chunk_overlap=\u001b[32m128\u001b[39m):\n\u001b[32m      4\u001b[39m     loader = PDFPlumberLoader(file_path)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Step 1: Combine all text across pages with page tracking\u001b[39;00m\n\u001b[32m      8\u001b[39m     all_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:1052\u001b[39m, in \u001b[36mPDFPlumberLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1051\u001b[39m     blob = Blob.from_path(\u001b[38;5;28mself\u001b[39m.file_path)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:127\u001b[39m, in \u001b[36mBaseBlobParser.parse\u001b[39m\u001b[34m(self, blob)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Blob) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Eagerly parse the blob into a document or documents.\u001b[39;00m\n\u001b[32m    114\u001b[39m \n\u001b[32m    115\u001b[39m \u001b[33;03m    This is a convenience method for interactive development environment.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03m        List of documents\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:1413\u001b[39m, in \u001b[36mPDFPlumberParser.lazy_parse\u001b[39m\u001b[34m(self, blob)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m blob.as_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m file_path:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   1409\u001b[39m     doc = pdfplumber.open(file_path)  \u001b[38;5;66;03m# open document\u001b[39;00m\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[32m   1412\u001b[39m         Document(\n\u001b[32m-> \u001b[39m\u001b[32m1413\u001b[39m             page_content=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_page_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1414\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1415\u001b[39m             + \u001b[38;5;28mself\u001b[39m._extract_images_from_page(page),\n\u001b[32m   1416\u001b[39m             metadata=\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   1417\u001b[39m                 {\n\u001b[32m   1418\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: blob.source,  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   1419\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mfile_path\u001b[39m\u001b[33m\"\u001b[39m: blob.source,  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   1420\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m: page.page_number - \u001b[32m1\u001b[39m,\n\u001b[32m   1421\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mtotal_pages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(doc.pages),\n\u001b[32m   1422\u001b[39m                 },\n\u001b[32m   1423\u001b[39m                 **{\n\u001b[32m   1424\u001b[39m                     k: doc.metadata[k]\n\u001b[32m   1425\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m doc.metadata\n\u001b[32m   1426\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(doc.metadata[k]) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]\n\u001b[32m   1427\u001b[39m                 },\n\u001b[32m   1428\u001b[39m             ),\n\u001b[32m   1429\u001b[39m         )\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc.pages\n\u001b[32m   1431\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:1437\u001b[39m, in \u001b[36mPDFPlumberParser._process_page_content\u001b[39m\u001b[34m(self, page)\u001b[39m\n\u001b[32m   1435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dedupe:\n\u001b[32m   1436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m page.dedupe_chars().extract_text(**\u001b[38;5;28mself\u001b[39m.text_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\pdfplumber\\page.py:557\u001b[39m, in \u001b[36mPage.extract_text\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_textmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtuplify_list_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.as_string\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\pdfplumber\\page.py:534\u001b[39m, in \u001b[36mPage._get_textmap\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    532\u001b[39m     defaults.update({\u001b[33m\"\u001b[39m\u001b[33mlayout_height\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.height})\n\u001b[32m    533\u001b[39m full_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {**defaults, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m utils.chars_to_textmap(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchars\u001b[49m, **full_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\pdfplumber\\container.py:52\u001b[39m, in \u001b[36mContainer.chars\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchars\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T_obj_list:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjects\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mchar\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\pdfplumber\\page.py:366\u001b[39m, in \u001b[36mPage.objects\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_objects\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._objects\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[38;5;28mself\u001b[39m._objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._objects\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emhal\\Venv_directory\\VestasVenv\\Lib\\site-packages\\pdfplumber\\page.py:476\u001b[39m, in \u001b[36mPage.parse_objects\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m objects.get(kind) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    475\u001b[39m         objects[kind] = []\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     objects[kind].append(obj)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## Extracting section headers from the PDF files\n",
    "\n",
    "def extract_text_chunks(file_path, manual_id, chunk_size=512, chunk_overlap=128):\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Step 1: Combine all text across pages with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = []  # (char_index, page_number)\n",
    "\n",
    "    for doc_page in docs:\n",
    "        text = doc_page.page_content.strip().replace('\\n', ' ')\n",
    "        start_idx = len(all_text)\n",
    "        all_text += text + \" \"  # Add space to separate pages\n",
    "        end_idx = len(all_text)\n",
    "        page_map.append((start_idx, end_idx, doc_page.metadata['page']))\n",
    "\n",
    "    # Step 2: Create chunks with overlap, spanning across pages\n",
    "    chunks = []\n",
    "    chunk_order = []\n",
    "    page_start_list = []\n",
    "    page_end_list = []\n",
    "\n",
    "    idx = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while idx < len(all_text):\n",
    "        chunk = all_text[idx:idx + chunk_size]\n",
    "\n",
    "        # Determine pages involved in this chunk\n",
    "        chunk_start = idx\n",
    "        chunk_end = idx + len(chunk)\n",
    "\n",
    "        pages_in_chunk = [\n",
    "            page_num\n",
    "            for start, end, page_num in page_map\n",
    "            if not (end <= chunk_start or start >= chunk_end)  # overlap condition\n",
    "        ]\n",
    "\n",
    "        page_start = min(pages_in_chunk) if pages_in_chunk else None\n",
    "        page_end = max(pages_in_chunk) if pages_in_chunk else None\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        page_start_list.append(page_start)\n",
    "        page_end_list.append(page_end)\n",
    "        chunk_order.append(chunk_idx)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        idx += chunk_size - chunk_overlap\n",
    "\n",
    "    # Step 3: Create DataFrame\n",
    "    rows = [{\n",
    "        'DOCUMENT_ID': manual_id,\n",
    "        'PAGE_START_NUMBER': start,\n",
    "        'PAGE_END_NUMBER': end,\n",
    "        'CHUNK_TEXT': chunk,\n",
    "        'CHUNK_ORDER': order\n",
    "    } for chunk, start, end, order in zip(chunks, page_start_list, page_end_list, chunk_order)]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"DOCUMENT_ID\", \"PAGE_START_NUMBER\", \"PAGE_END_NUMBER\", \"CHUNK_TEXT\", \"CHUNK_ORDER\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "large_chunks_df = pd.DataFrame()\n",
    "for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "\n",
    "    tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                        manual_id = manual_id,\n",
    "                        chunk_size = 6000,#1024,\n",
    "                        chunk_overlap = 128)  # Show first 5 chunks\n",
    "    large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "large_chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1fb5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x269b7eb5910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    PAGE_START_NUMBER INT,\n",
    "    PAGE_END_NUMBER INT,\n",
    "    CHUNK_ORDER INT,\n",
    "    CHUNK_TEXT STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc67cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x269b7eb5910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=large_chunks_df,\n",
    "    database =database,\n",
    "    table_name=\"CHUNKS_LARGE\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "# Update the embeddings for the chunks in the CHUNKS_LARGE table\n",
    "cursor.execute(\"\"\"\n",
    "    UPDATE CHUNKS_LARGE\n",
    "    SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0',\n",
    "        CHUNK_TEXT\n",
    "    )\n",
    "    WHERE EMBEDDING IS NULL;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d11219e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:23<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x269b7eb5910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_chunks_df = pd.DataFrame()\n",
    "for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                        manual_id = manual_id,\n",
    "                        chunk_size = 1024,\n",
    "                        chunk_overlap = 64)  # Show first 5 chunks\n",
    "    small_chunks_df = pd.concat([small_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    PAGE_START_NUMBER INT,\n",
    "    PAGE_END_NUMBER INT,\n",
    "    CHUNK_ORDER INT,\n",
    "    CHUNK_TEXT STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=small_chunks_df,\n",
    "    database =database,\n",
    "    table_name=\"CHUNKS_SMALL\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "# Update the embeddings for the small chunks\n",
    "cursor.execute(\"\"\"\n",
    "    UPDATE CHUNKS_SMALL\n",
    "    SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0',\n",
    "        CHUNK_TEXT\n",
    "    )\n",
    "    WHERE EMBEDDING IS NULL;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b8c0f",
   "metadata": {},
   "source": [
    "## Creating sections table using LLM for TOC extraction\n",
    "\n",
    "The function `extract_TOC` takes quite a while due to the chunk size and the model. This can be tampered with, but i found most consistent results with said model. I also think that larger chunks are better for this task, as the model can see context of the first few pages, and it also ensures that the table of contents is included in the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime in seconds: 84.2520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is the extracted table of contents in the requested JSON format:\\n\\n```\\n{\\n  \"Section\": \"Table of contents\",\\n  \"Section Number\": \"\",\\n  \"Page\": \"\",\\n  \"Sub Sections\": [\\n    {\\n      \"Section\": \"Safety\",\\n      \"Section Number\": \"1\",\\n      \"Page\": \"4\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"General information\",\\n          \"Section Number\": \"1.1\",\\n          \"Page\": \"4\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Intended use\",\\n          \"Section Number\": \"1.2\",\\n          \"Page\": \"4\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Restriction on user group\",\\n          \"Section Number\": \"1.3\",\\n          \"Page\": \"4\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe installation\",\\n          \"Section Number\": \"1.4\",\\n          \"Page\": \"5\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe use\",\\n          \"Section Number\": \"1.5\",\\n          \"Page\": \"7\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe cleaning and maintenance\",\\n          \"Section Number\": \"1.6\",\\n          \"Page\": \"9\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Preventing material damage\",\\n      \"Section Number\": \"2\",\\n      \"Page\": \"10\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Environmental protection and saving energy\",\\n      \"Section Number\": \"3\",\\n      \"Page\": \"11\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Disposing of packaging\",\\n          \"Section Number\": \"3.1\",\\n          \"Page\": \"11\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Save energy and conserve resources\",\\n          \"Section Number\": \"3.2\",\\n          \"Page\": \"11\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Energy saving mode\",\\n          \"Section Number\": \"3.3\",\\n          \"Page\": \"12\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Installation and connection\",\\n      \"Section Number\": \"4\",\\n      \"Page\": \"12\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Unpacking the appliance\",\\n          \"Section Number\": \"4.1\",\\n          \"Page\": \"12\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Contents of package\",\\n          \"Section Number\": \"4.2\",\\n          \"Page\": \"13\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Requirements for the installation location\",\\n          \"Section Number\": \"4.3\",\\n          \"Page\": \"13\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Removing the transit bolts\",\\n          \"Section Number\": \"4.4\",\\n          \"Page\": \"14\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance\",\\n          \"Section Number\": \"4.5\",\\n          \"Page\": \"15\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Aligning the appliance\",\\n          \"Section Number\": \"4.6\",\\n          \"Page\": \"16\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance to the electricity supply\",\\n          \"Section Number\": \"4.7\",\\n          \"Page\": \"17\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Familiarising yourself with your appliance\",\\n      \"Section Number\": \"5\",\\n      \"Page\": \"18\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Appliance\",\\n          \"Section Number\": \"5.1\",\\n          \"Page\": \"18\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Detergent drawer\",\\n          \"Section Number\": \"5.2\",\\n          \"Page\": \"19\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Controls\",\\n          \"Section Number\": \"5.3\",\\n          \"Page\": \"19\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Display\",\\n      \"Section Number\": \"6\",\\n      \"Page\": \"20\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Buttons\",\\n      \"Section Number\": \"7\",\\n      \"Page\": \"23\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Programmes\",\\n      \"Section Number\": \"8\",\\n      \"Page\": \"26\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Accessories\",\\n      \"Section Number\": \"9\",\\n      \"Page\": \"33\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Laundry\",\\n      \"Section Number\": \"11\",\\n      \"Page\": \"34\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Preparing the laundry\",\\n          \"Section Number\": \"11.1\",\\n          \"Page\": \"34\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Sorting laundry\",\\n          \"Section Number\": \"11.2\",\\n          \"Page\": \"35\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Degrees of soiling\",\\n          \"Section Number\": \"11.3\",\\n          \"Page\": \"35\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Care symbols on the care labels\",\\n          \"Section Number\": \"11.4\",\\n          \"Page\": \"35\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Detergents and care products\",\\n      \"Section Number\": \"12\",\\n      \"Page\": \"36\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Detergent recommendation\",\\n          \"Section Number\": \"12.1\",\\n          \"Page\": \"36\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Detergent dosage\",\\n          \"Section Number\": \"12.2\",\\n          \"Page\": \"37\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Basic operation\",\\n      \"Section Number\": \"13\",\\n      \"Page\": \"38\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Switching on the appliance\",\\n          \"Section Number\": \"13.1\",\\n          \"Page\": \"38\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Setting a programme\",\\n          \"Section Number\": \"13.2\",\\n          \"Page\": \"38\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Adjusting the programme settings\",\\n          \"Section Number\": \"13.3\",\\n          \"Page\": \"38\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Loading laundry\",\\n          \"Section Number\": \"13.4\",\\n          \"Page\": \"38\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Adding detergent and care product\",\\n          \"Section Number\": \"13.5\",\\n          \"Page\": \"39\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Starting the programme\",\\n          \"Section Number\": \"13.6\",\\n          \"Page\": \"39\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Soaking laundry\",\\n          \"Section Number\": \"13.7\",\\n          \"Page\": \"39\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Adding laundry\",\\n          \"Section Number\": \"13.8\",\\n          \"Page\": \"39\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cancelling the programme\",\\n          \"Section Number\": \"13.9\",\\n          \"Page\": \"39\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Resuming the programme when the programme status is Rinse Hold\",\\n          \"Section Number\": \"13.10\",\\n          \"Page\": \"40\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Unloading the laundry\",\\n          \"Section Number\": \"13.11\",\\n          \"Page\": \"40\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Switching off the appliance\",\\n          \"Section Number\": \"13.12\",\\n          \"Page\": \"40\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Childproof lock\",\\n      \"Section Number\": \"14\",\\n      \"Page\": \"40\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Activating the childproof lock\",\\n          \"Section Number\": \"14.1\",\\n          \"Page\": \"40\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Deactivating the childproof lock\",\\n          \"Section Number\": \"14.2\",\\n          \"Page\": \"41\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Intelligent dosing system\",\\n      \"Section Number\": \"15\",\\n      \"Page\": \"41\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Filling the dispenser\",\\n          \"Section Number\": \"15.1\",\\n          \"Page\": \"41\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Dispenser contents\",\\n          \"Section Number\": \"15.2\",\\n          \"Page\": \"41\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Basic dosage\",\\n          \"Section Number\": \"15.3\",\\n          \"Page\": \"42\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Home Connect\",\\n      \"Section Number\": \"16\",\\n      \"Page\": \"42\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Connecting the appliance to a WLAN home network (Wi-Fi) with WPS function\",\\n          \"Section Number\": \"16.1\",\\n          \"Page\": \"42\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance to a WLAN home network (Wi-Fi) without WPS function\",\\n          \"Section Number\": \"16.2\",\\n          \"Page\": \"43\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance to the Home Connect app\",\\n          \"Section Number\": \"16.3\",\\n          \"Page\": \"44\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance to the energy manager\",\\n          \"Section Number\": \"16.4\",\\n          \"Page\": \"44\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Activating Wi-Fi on the appliance\",\\n          \"Section Number\": \"16.5\",\\n          \"Page\": \"45\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Deactivating Wi-Fi on the appliance\",\\n          \"Section Number\": \"16.6\",\\n          \"Page\": \"45\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Software update\",\\n          \"Section Number\": \"16.7\",\\n          \"Page\": \"45\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Resetting the appliance network settings\",\\n          \"Section Number\": \"16.8\",\\n          \"Page\": \"46\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Remote diagnostics\",\\n          \"Section Number\": \"16.9\",\\n          \"Page\": \"46\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Data protection\",\\n          \"Section Number\": \"16.10\",\\n          \"Page\": \"46\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Basic settings\",\\n      \"Section Number\": \"17\",\\n      \"Page\": \"47\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Overview of basic settings\",\\n          \"Section Number\": \"17.1\",\\n          \"Page\": \"47\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Changing the basic settings\",\\n          \"Section Number\": \"17.2\",\\n          \"Page\": \"48\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Cleaning and servicing\",\\n      \"Section Number\": \"18\",\\n      \"Page\": \"48\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Tips on appliance care\",\\n          \"Section Number\": \"18.1\",\\n          \"Page\": \"48\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the drum\",\\n          \"Section Number\": \"18.2\",\\n          \"Page\": \"48\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the detergent drawer\",\\n          \"Section Number\": \"18.3\",\\n          \"Page\": \"48\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Descaling\",\\n          \"Section Number\": \"18.4\",\\n          \"Page\": \"50\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the drain pump\",\\n          \"Section Number\": \"18.5\",\\n          \"Page\": \"50\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the intake opening in the rubber gasket\",\\n          \"Section Number\": \"18.6\",\\n          \"Page\": \"53\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the water outlet hose at the siphon\",\\n          \"Section Number\": \"18.7\",\\n          \"Page\": \"53\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Clean the water inlet filters\",\\n          \"Section Number\": \"18.8\",\\n          \"Page\": \"54\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Troubleshooting\",\\n      \"Section Number\": \"19\",\\n      \"Page\": \"56\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Transportation, storage and disposal\",\\n      \"Section Number\": \"20\",\\n      \"Page\": \"67\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Removing the appliance\",\\n          \"Section Number\": \"20.1\",\\n          \"Page\": \"67\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Inserting the transit bolts\",\\n          \"Section Number\": \"20.2\",\\n          \"Page\": \"67\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Using the appliance again\",\\n          \"Section Number\": \"20.3\",\\n          \"Page\": \"68\",\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Disposing of old appliance\",\\n          \"Section Number\": \"20.4\",\\n          \"Page\": \"68\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Customer Service\",\\n      \"Section Number\": \"21\",\\n      \"Page\": \"69\",\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Product number (E-Nr.) and production number (FD)\",\\n          \"Section Number\": \"21.1\",\\n          \"Page\": \"69\",\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Consumption values\",\\n      \"Section Number\": \"22\",\\n      \"Page\": \"70\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Technical specifications\",\\n      \"Section Number\": \"23\",\\n      \"Page\": \"71\",\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Declaration of Conformity\",\\n      \"Section Number\": \"24\",\\n      \"Page\": \"71\",\\n      \"Sub Sections\": []\\n    }\\n  ]\\n}\\n```\\n\\nNote that I\\'ve ignored the text that is not part of the table of contents, as per the guidelines. I\\'ve also tried to maintain the original structure and formatting of the table of contents, while converting it into the requested JSON format.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_TOC(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = cursor.execute(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\")\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "\n",
    "    return cursor.fetch_pandas_all().iloc[0,0]\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "\n",
    "# llm_output = extract_TOC(large_chunks_df.loc[0,\"CHUNK_TEXT\"], model = 'llama3.1-70b')\n",
    "# llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_llm_output(llm_output: str) -> dict:\n",
    "    try:\n",
    "        # Confirming that a JSON block is returned\n",
    "        match = re.search(r\"```\\s*(\\{.*?\\})\\s*```\", llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        # Extracting sub string (json string)\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Clean common JSON errors (e.g., trailing commas)\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)  # remove trailing commas before ] or }\n",
    "        \n",
    "        # Parse string to json\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "        \n",
    "# parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "# print(json.dumps(parsed_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Safety</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Safe use</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Safe cleaning and maintenance</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Preventing material damage</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Environmental protection and saving energy</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Disposing of packaging</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      SECTION SECTION_NUMBER PAGE  \\\n",
       "0                                      Safety              1    4   \n",
       "1                         General information            1.1    4   \n",
       "2                                Intended use            1.2    4   \n",
       "3                   Restriction on user group            1.3    4   \n",
       "4                           Safe installation            1.4    5   \n",
       "5                                    Safe use            1.5    7   \n",
       "6               Safe cleaning and maintenance            1.6    9   \n",
       "7                  Preventing material damage              2   10   \n",
       "8  Environmental protection and saving energy              3   11   \n",
       "9                      Disposing of packaging            3.1   11   \n",
       "\n",
       "  PARENT_SECTION_NUMBER  \n",
       "0                        \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "5                     1  \n",
       "6                     1  \n",
       "7                        \n",
       "8                        \n",
       "9                     3  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traverse_sections(node, parent_section=None):\n",
    "    rows = []\n",
    "\n",
    "    # Get info from the current node\n",
    "    section = node.get(\"Section\")\n",
    "    section_number = node.get(\"Section Number\")\n",
    "    page = node.get(\"Page\")\n",
    "\n",
    "    # Add current node to the list\n",
    "    evaluator = load_evaluator(\"string_distance\")\n",
    "    levenshtein_score_toc = evaluator.evaluate_strings(\n",
    "    prediction=section,\n",
    "    reference=\"Table of Contents\",\n",
    "    metric=\"levenshtein\"\n",
    "    )[\"score\"]  # This will be a float between 0 and 1, where 0 means identical\n",
    "\n",
    "    if levenshtein_score_toc > 0.1:  # if the levenshtein distance is very small its likely to match \"Table of Contents\"\n",
    "        rows.append({\n",
    "            \"SECTION\": section,\n",
    "            \"SECTION_NUMBER\": section_number,\n",
    "            \"PAGE\": page,\n",
    "            \"PARENT_SECTION_NUMBER\": parent_section\n",
    "        })\n",
    "\n",
    "    # Recurse into each sub-section, if any\n",
    "    for subsection in node.get(\"Sub Sections\", []):\n",
    "        rows.extend(traverse_sections(subsection, parent_section=section_number))\n",
    "\n",
    "    return rows\n",
    "\n",
    "# flat_rows = traverse_sections(parsed_dict)\n",
    "# toc_df = pd.DataFrame(flat_rows)\n",
    "# toc_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:33<03:06, 93.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime in seconds: 93.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [02:38<01:17, 77.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime in seconds: 65.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:48<00:00, 76.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime in seconds: 69.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Safety</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Disposing of old appliance</td>\n",
       "      <td>18.4</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Customer Service</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Product number (E-Nr.) and production number (FD)</td>\n",
       "      <td>19.1</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Consumption values</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Technical data</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               SECTION SECTION_NUMBER PAGE  \\\n",
       "0                                               Safety              1    4   \n",
       "1                                  General information            1.1    4   \n",
       "2                                         Intended use            1.2    4   \n",
       "3                            Restriction on user group            1.3    4   \n",
       "4                                    Safe installation            1.4    5   \n",
       "..                                                 ...            ...  ...   \n",
       "216                         Disposing of old appliance           18.4   47   \n",
       "217                                   Customer Service             19   47   \n",
       "218  Product number (E-Nr.) and production number (FD)           19.1   47   \n",
       "219                                 Consumption values             20   48   \n",
       "220                                     Technical data             21   48   \n",
       "\n",
       "    PARENT_SECTION_NUMBER  DOCUMENT_ID  \n",
       "0                                    1  \n",
       "1                       1            1  \n",
       "2                       1            1  \n",
       "3                       1            1  \n",
       "4                       1            1  \n",
       "..                    ...          ...  \n",
       "216                    18            3  \n",
       "217                                  3  \n",
       "218                    19            3  \n",
       "219                                  3  \n",
       "220                                  3  \n",
       "\n",
       "[221 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\"):\n",
    "    df_list = []\n",
    "\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "        first_chunk_of_doc = large_chunks_df.loc[large_chunks_df[\"DOCUMENT_ID\"] == manual_id, \"CHUNK_TEXT\"].iloc[0]\n",
    "        # print(\"First chunk:\", first_chunk_of_doc)\n",
    "\n",
    "        llm_output = extract_TOC(first_chunk_of_doc, model = model)\n",
    "        parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "        flat_rows = traverse_sections(parsed_dict)\n",
    "        local_toc_df = pd.DataFrame(flat_rows)\n",
    "        local_toc_df[\"DOCUMENT_ID\"] = manual_id\n",
    "        df_list.append(local_toc_df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "     \n",
    "sections_df = create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\")\n",
    "sections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae98e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 221\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE SECTIONS (\n",
    "    SECTION_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    SECTION STRING NOT NULL,\n",
    "    SECTION_NUMBER STRING NOT NULL,\n",
    "    PARENT_SECTION_NUMBER STRING,\n",
    "    PAGE INT,\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=sections_df,\n",
    "    database =database,\n",
    "    table_name=\"SECTIONS\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0f10d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>EMBEDDING</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>[0.048339844, 0.0158844, -0.015388489, -0.0038...</td>\n",
       "      <td>2025-04-22 01:53:38.352000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>t. ¡ Up to an altitude of max. 4000 m above se...</td>\n",
       "      <td>[0.038604736, 0.08459473, -0.023513794, -0.014...</td>\n",
       "      <td>2025-04-22 01:53:38.352000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>on or lean against the appliance door. ▶ Do no...</td>\n",
       "      <td>[0.074279785, 0.068359375, -0.0036392212, 0.02...</td>\n",
       "      <td>2025-04-22 01:53:38.352000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>ese instructions, your creases energy and wate...</td>\n",
       "      <td>[0.008338928, 0.00027441978, 0.044067383, 0.00...</td>\n",
       "      <td>2025-04-22 01:53:38.352000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>ht. Water outlet connection types 4.6 Aligning...</td>\n",
       "      <td>[0.052764893, 0.074157715, -0.033294678, 0.011...</td>\n",
       "      <td>2025-04-22 01:53:38.352000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHUNK_ID  DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  CHUNK_ORDER  \\\n",
       "0         1            1                  0                3            0   \n",
       "1         2            1                  3                7            1   \n",
       "2         3            1                  7               10            2   \n",
       "3         4            1                 10               15            3   \n",
       "4         5            1                 15               22            4   \n",
       "\n",
       "                                          CHUNK_TEXT  \\\n",
       "0  Register your b M o ge s y n c t B e h f o r w...   \n",
       "1  t. ¡ Up to an altitude of max. 4000 m above se...   \n",
       "2  on or lean against the appliance door. ▶ Do no...   \n",
       "3  ese instructions, your creases energy and wate...   \n",
       "4  ht. Water outlet connection types 4.6 Aligning...   \n",
       "\n",
       "                                           EMBEDDING  \\\n",
       "0  [0.048339844, 0.0158844, -0.015388489, -0.0038...   \n",
       "1  [0.038604736, 0.08459473, -0.023513794, -0.014...   \n",
       "2  [0.074279785, 0.068359375, -0.0036392212, 0.02...   \n",
       "3  [0.008338928, 0.00027441978, 0.044067383, 0.00...   \n",
       "4  [0.052764893, 0.074157715, -0.033294678, 0.011...   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-22 01:53:38.352000-07:00  \n",
       "1 2025-04-22 01:53:38.352000-07:00  \n",
       "2 2025-04-22 01:53:38.352000-07:00  \n",
       "3 2025-04-22 01:53:38.352000-07:00  \n",
       "4 2025-04-22 01:53:38.352000-07:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_LARGE;\n",
    "\"\"\")\n",
    "\n",
    "large_chunks_df = cursor.fetch_pandas_all()\n",
    "large_chunks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35132e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>EMBEDDING</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>[0.032073975, 0.05307007, -0.021911621, -0.006...</td>\n",
       "      <td>2025-04-22 01:54:15.568000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>..... 29 2Preventing material damage.... 10 13...</td>\n",
       "      <td>[0.018035889, 0.035339355, -0.0013151169, 0.00...</td>\n",
       "      <td>2025-04-22 01:54:15.568000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>moving the transit bolts...... 13 13.10 Cancel...</td>\n",
       "      <td>[0.086364746, 0.014724731, 0.004234314, -0.003...</td>\n",
       "      <td>2025-04-22 01:54:15.568000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>......... 20 2 en 15 Basic settings..............</td>\n",
       "      <td>[0.029403687, -0.017074585, -0.033416748, 0.00...</td>\n",
       "      <td>2025-04-22 01:54:15.568000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e the following safety instructions. 1.1 Gener...</td>\n",
       "      <td>[0.050964355, 0.06311035, -0.032409668, 0.0217...</td>\n",
       "      <td>2025-04-22 01:54:15.568000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHUNK_ID  DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  CHUNK_ORDER  \\\n",
       "0         1            1                  0                1            0   \n",
       "1         2            1                  1                1            1   \n",
       "2         3            1                  1                2            2   \n",
       "3         4            1                  1                3            3   \n",
       "4         5            1                  3                3            4   \n",
       "\n",
       "                                          CHUNK_TEXT  \\\n",
       "0  Register your b M o ge s y n c t B e h f o r w...   \n",
       "1  ..... 29 2Preventing material damage.... 10 13...   \n",
       "2  moving the transit bolts...... 13 13.10 Cancel...   \n",
       "3  ......... 20 2 en 15 Basic settings..............   \n",
       "4  e the following safety instructions. 1.1 Gener...   \n",
       "\n",
       "                                           EMBEDDING  \\\n",
       "0  [0.032073975, 0.05307007, -0.021911621, -0.006...   \n",
       "1  [0.018035889, 0.035339355, -0.0013151169, 0.00...   \n",
       "2  [0.086364746, 0.014724731, 0.004234314, -0.003...   \n",
       "3  [0.029403687, -0.017074585, -0.033416748, 0.00...   \n",
       "4  [0.050964355, 0.06311035, -0.032409668, 0.0217...   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-22 01:54:15.568000-07:00  \n",
       "1 2025-04-22 01:54:15.568000-07:00  \n",
       "2 2025-04-22 01:54:15.568000-07:00  \n",
       "3 2025-04-22 01:54:15.568000-07:00  \n",
       "4 2025-04-22 01:54:15.568000-07:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_SMALL;\n",
    "\"\"\")\n",
    "\n",
    "small_chunks_df = cursor.fetch_pandas_all()\n",
    "small_chunks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b45f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Safety</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>Disposing of old appliance</td>\n",
       "      <td>18.4</td>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>47</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>219</td>\n",
       "      <td>3</td>\n",
       "      <td>Product number (E-Nr.) and production number (FD)</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>Consumption values</td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>48</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>221</td>\n",
       "      <td>3</td>\n",
       "      <td>Technical data</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td>48</td>\n",
       "      <td>2025-04-22 03:14:00.711000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SECTION_ID  DOCUMENT_ID  \\\n",
       "0             1            1   \n",
       "1             2            1   \n",
       "2             3            1   \n",
       "3             4            1   \n",
       "4             5            1   \n",
       "..          ...          ...   \n",
       "216         217            3   \n",
       "217         218            3   \n",
       "218         219            3   \n",
       "219         220            3   \n",
       "220         221            3   \n",
       "\n",
       "                                               SECTION SECTION_NUMBER  \\\n",
       "0                                               Safety              1   \n",
       "1                                  General information            1.1   \n",
       "2                                         Intended use            1.2   \n",
       "3                            Restriction on user group            1.3   \n",
       "4                                    Safe installation            1.4   \n",
       "..                                                 ...            ...   \n",
       "216                         Disposing of old appliance           18.4   \n",
       "217                                   Customer Service             19   \n",
       "218  Product number (E-Nr.) and production number (FD)           19.1   \n",
       "219                                 Consumption values             20   \n",
       "220                                     Technical data             21   \n",
       "\n",
       "    PARENT_SECTION_NUMBER  PAGE                       CREATED_AT  \n",
       "0                             4 2025-04-22 03:14:00.711000-07:00  \n",
       "1                       1     4 2025-04-22 03:14:00.711000-07:00  \n",
       "2                       1     4 2025-04-22 03:14:00.711000-07:00  \n",
       "3                       1     4 2025-04-22 03:14:00.711000-07:00  \n",
       "4                       1     5 2025-04-22 03:14:00.711000-07:00  \n",
       "..                    ...   ...                              ...  \n",
       "216                    18    47 2025-04-22 03:14:00.711000-07:00  \n",
       "217                          47 2025-04-22 03:14:00.711000-07:00  \n",
       "218                    19    47 2025-04-22 03:14:00.711000-07:00  \n",
       "219                          48 2025-04-22 03:14:00.711000-07:00  \n",
       "220                          48 2025-04-22 03:14:00.711000-07:00  \n",
       "\n",
       "[221 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "\"\"\")\n",
    "\n",
    "sections_df = cursor.fetch_pandas_all()\n",
    "sections_df.head()\n",
    "\n",
    "sections_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8cb1",
   "metadata": {},
   "source": [
    "# Extracting images from the manual\n",
    "\n",
    "This chosen method which appears to be more diverse across the manuals treats each page as an image. This is a good way to ensure that all images are extracted. \n",
    "The downside is that tables and other image like content will be extracted as images. Currently this is a feature not a bug. Adjusting the image extraction method is a task for the future when we have the real PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f45c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "def render_pdf_to_images(pdf_path, zoom=2.0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i, page in enumerate(doc):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img_data = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append({\n",
    "            \"page_number\": i + 1,\n",
    "            \"image\": img_data\n",
    "        })\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_pdf_page_pixel_size(pdf_image):\n",
    "    width, height = pdf_image.size\n",
    "    return width * height\n",
    "\n",
    "\n",
    "def detect_image_regions(page_image, buffer=0, min_size=70, max_size = 1000, threshold=240):\n",
    "    image = np.array(page_image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Applying blur to reduce fine lines from tables\n",
    "    _, thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > min_size and h > min_size:  # Skip tiny blocks (Maybe reconsider)\n",
    "            regions.append([x - buffer, \n",
    "                            y - buffer, \n",
    "                            x + w + buffer, \n",
    "                            y + h + buffer])\n",
    "            if w * h > max_size:\n",
    "                regions.pop(-1)  \n",
    "    return regions\n",
    "\n",
    "\n",
    "def crop_regions_from_image(page_image, regions, output_dir, page_num, manual_id):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_images = []\n",
    "\n",
    "    for i, coords in enumerate(regions):\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        cropped = page_image.crop((x1, y1, x2, y2))\n",
    "        save_path = os.path.join(output_dir, f\"doc_{manual_id}_page_{page_num}_img_{i+1}.png\")\n",
    "        cropped.save(save_path)\n",
    "        saved_images.append({\n",
    "            \"page\": page_num,\n",
    "            \"image_path\": save_path,\n",
    "            \"coords\": (x1, y1, x2, y2)\n",
    "        })\n",
    "    return saved_images\n",
    "\n",
    "\n",
    "\n",
    "def add_region_to_page(page_image, regions, output_dir, page_num, pdf_path ,color=(0, 255, 0), alpha=50, save=True, verbose=0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to RGBA to allow transparency\n",
    "    annotated = page_image.convert(\"RGBA\")\n",
    "    overlay = Image.new(\"RGBA\", annotated.size, (0, 0, 0, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    for coords in regions:\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color + (alpha,), fill=color + (alpha,))\n",
    "\n",
    "    # Combine original image with overlay\n",
    "    combined = Image.alpha_composite(annotated, overlay)\n",
    "\n",
    "    if save:\n",
    "        save_path = os.path.join(output_dir, f\"page_{page_num:03d}_with_regions_{color}.png\")\n",
    "        combined.convert(\"RGB\").save(save_path)\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved page {page_num} with highlighted regions to {save_path}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def merge_overlapping_regions(regions, buffer=0):\n",
    "    \"\"\"\n",
    "    Merges overlapping or intersecting regions.\n",
    "\n",
    "    Args:\n",
    "        regions (List[List[int]]): List of regions as [x1, y1, x2, y2].\n",
    "        buffer (int): Optional buffer added to each region before checking overlaps.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Merged list of non-overlapping regions.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    # Convert to shapely boxes with optional buffer\n",
    "    boxes = [box(x1 - buffer, y1 - buffer, x2 + buffer, y2 + buffer) for x1, y1, x2, y2 in regions]\n",
    "\n",
    "    # Merge all overlapping boxes (A fix to a previous issues of diagrams being cropped into multiple images)\n",
    "    merged = unary_union(boxes)\n",
    "\n",
    "    # Ensure output is a list of boxes\n",
    "    if merged.geom_type == 'Polygon':\n",
    "        merged_boxes = [merged]\n",
    "    else:\n",
    "        merged_boxes = list(merged.geoms)\n",
    "\n",
    "    # Convert back to [x1, y1, x2, y2] format (round to int)\n",
    "    merged_regions = []\n",
    "    for b in merged_boxes:\n",
    "        x1, y1, x2, y2 = b.bounds\n",
    "        merged_regions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    return merged_regions\n",
    "\n",
    "\n",
    "\n",
    "# This is the main function to extract images from the PDF\n",
    "def extract_images_from_pdf(pdf_path:str, manual_id:int, output_dir: str, verbose:int =0):\n",
    "    rendered_pages = render_pdf_to_images(pdf_path)\n",
    "    all_extracted = []\n",
    "\n",
    "    for page_idx,page in enumerate(rendered_pages):\n",
    "        page_num = page[\"page_number\"] \n",
    "        image = page[\"image\"]\n",
    "        if verbose > 0:\n",
    "            print(f\"Processing page {page_num}...\")\n",
    "\n",
    "        # Detecting regions\n",
    "        regions = detect_image_regions(image , buffer=2, min_size=70, \n",
    "                                        max_size=get_pdf_page_pixel_size(image) * 0.99)\n",
    "        # Creates new regions by merging overlapping regions (this is a fix for cropped images  )\n",
    "        new_regions = merge_overlapping_regions(regions, buffer=0)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Found {len(new_regions)} image regions on page {page_num}\")\n",
    "\n",
    "        if not new_regions:\n",
    "            if verbose > 0:\n",
    "                print(f\"No image regions found on page {page_num}\")\n",
    "            continue\n",
    "        \n",
    "        # Creates an image directory for each PDF file\n",
    "        image_output_dir = pdf_path.split(\"/\")[-1].replace(\".pdf\", \"\").replace(\"Washer_Manuals\", output_dir)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "        # Showing the pages with the masked regions \n",
    "        modified_image = add_region_to_page(image, new_regions, image_output_dir, page_num, pdf_path, color=(0, 0, 255), alpha=50, save = False)\n",
    "\n",
    "        # OLD code \n",
    "        extracted = crop_regions_from_image(\n",
    "            image, new_regions, output_dir=image_output_dir, page_num=page_num, manual_id=manual_id\n",
    "        )\n",
    "        all_extracted.extend(extracted)\n",
    "    return all_extracted\n",
    "\n",
    "\n",
    "for idx,row in tqdm(enumerate(documents_df.iterrows()), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "    extract_images_from_pdf(file_path, manual_id, output_dir=\"Washer_Images\", verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b32ef3",
   "metadata": {},
   "source": [
    "# Creating table for image references and metadata\n",
    "\n",
    "Currently the images are matched to the sections using the page number, which is problematic if the end of section 4.3 is one the same page as the start of section 4.4. On the top of my head i'm not quite sure how to match the images to the sections accurately, but this method yields mostly correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d7cf4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>IMG_ORDER</th>\n",
       "      <th>IMAGE_FILE</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>IMAGE_SIZE</th>\n",
       "      <th>IMAGE_WIDTH</th>\n",
       "      <th>IMAGE_HEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_13_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_1...</td>\n",
       "      <td>27070</td>\n",
       "      <td>318</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_1_page_13_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_2...</td>\n",
       "      <td>4549</td>\n",
       "      <td>113</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>doc_1_page_13_img_3.png</td>\n",
       "      <td>.\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_3...</td>\n",
       "      <td>9425</td>\n",
       "      <td>157</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_14_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WAV28KH3GB\\doc_1_page_14_img_1...</td>\n",
       "      <td>3428</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_1_page_14_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WAV28KH3GB\\doc_1_page_14_img_2...</td>\n",
       "      <td>4973</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID  SECTION_ID SECTION_NUMBER PAGE IMG_ORDER  \\\n",
       "0            1          15            4.2   13         1   \n",
       "1            1          15            4.2   13         2   \n",
       "2            1          15            4.2   13         3   \n",
       "3            1          17            4.4   14         1   \n",
       "4            1          17            4.4   14         2   \n",
       "\n",
       "                IMAGE_FILE                                         IMAGE_PATH  \\\n",
       "0  doc_1_page_13_img_1.png  .\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_1...   \n",
       "1  doc_1_page_13_img_2.png  .\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_2...   \n",
       "2  doc_1_page_13_img_3.png  .\\Washer_Images\\WAV28KH3GB\\doc_1_page_13_img_3...   \n",
       "3  doc_1_page_14_img_1.png  .\\Washer_Images\\WAV28KH3GB\\doc_1_page_14_img_1...   \n",
       "4  doc_1_page_14_img_2.png  .\\Washer_Images\\WAV28KH3GB\\doc_1_page_14_img_2...   \n",
       "\n",
       "   IMAGE_SIZE  IMAGE_WIDTH  IMAGE_HEIGHT  \n",
       "0       27070          318           452  \n",
       "1        4549          113           152  \n",
       "2        9425          157           140  \n",
       "3        3428          166           121  \n",
       "4        4973          166           120  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_page_number_from_filename(filename):\n",
    "    return filename.split(\"_\")[3] if \"_\" in filename else None\n",
    "\n",
    "def generate_image_table(documents_df, sections_df, image_dir):\n",
    "    image_records = []\n",
    "\n",
    "    # Loop over all subdirectories in image_dir\n",
    "    for subfolder in os.listdir(image_dir):\n",
    "        subfolder_path = os.path.join(image_dir, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue  # skip files\n",
    "        \n",
    "        # Match to document by DOCUMENT_NAME (strip extension if needed)\n",
    "        matching_docs = documents_df[documents_df['DOCUMENT_NAME'].str.contains(subfolder, case=False)]\n",
    "        if matching_docs.empty:\n",
    "            print(f\"No matching document for subfolder: {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        document_id = matching_docs.iloc[0]['DOCUMENT_ID']\n",
    "        document_name = matching_docs.iloc[0]['DOCUMENT_NAME']\n",
    "        \n",
    "        # List all image files in subdirectory\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            if not image_file.lower().endswith((\".png\")):\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            page_number = extract_page_number_from_filename(image_file)\n",
    "            order_number = image_file.split(\"img_\")[-1].strip(\".png\")\n",
    "\n",
    "            image_size = os.path.getsize(image_path)\n",
    "            image_width, image_height = Image.open(image_path).size\n",
    "            \n",
    "            # Try to match to a section (same document, closest PAGE <= image page)\n",
    "            section_match = None\n",
    "            if page_number is not None:\n",
    "                matching_sections = sections_df[\n",
    "                    (sections_df['DOCUMENT_ID'] == document_id) & \n",
    "                    (sections_df['PAGE'].astype(str) <= str(page_number))\n",
    "                ]\n",
    "                if not matching_sections.empty:\n",
    "                    section_match = matching_sections.sort_values(\"PAGE\", ascending=False).iloc[0]\n",
    "            \n",
    "            image_records.append({\n",
    "                \"DOCUMENT_ID\": document_id,\n",
    "                \"SECTION_ID\": section_match[\"SECTION_ID\"] if section_match is not None else None,\n",
    "                \"SECTION_NUMBER\": section_match[\"SECTION_NUMBER\"] if section_match is not None else None,\n",
    "                \"PAGE\": page_number,\n",
    "                \"IMG_ORDER\": order_number,\n",
    "                \"IMAGE_FILE\": image_file,\n",
    "                \"IMAGE_PATH\": image_path,\n",
    "                \"IMAGE_SIZE\": image_size,\n",
    "                \"IMAGE_WIDTH\": image_width,\n",
    "                \"IMAGE_HEIGHT\": image_height\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(image_records)\n",
    "\n",
    "\n",
    "image_df = generate_image_table(documents_df, sections_df, \".\\\\Washer_Images\")\n",
    "image_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4f76112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 194\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE IMAGES (\n",
    "    IMAGE_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    SECTION_ID INT NOT NULL,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    SECTION_NUMBER STRING NOT NULL,\n",
    "    PAGE INT,\n",
    "    IMG_ORDER INT,\n",
    "    IMAGE_FILE STRING,\n",
    "    IMAGE_PATH STRING,\n",
    "    IMAGE_SIZE NUMBER,\n",
    "    IMAGE_WIDTH NUMBER,\n",
    "    IMAGE_HEIGHT NUMBER,\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID),\n",
    "        \n",
    "    CONSTRAINT fk_section\n",
    "            FOREIGN KEY (SECTION_ID)\n",
    "            REFERENCES SECTIONS(SECTION_ID)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=image_df,\n",
    "    database =database,\n",
    "    table_name=\"IMAGES\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ddcf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE_ID</th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>IMG_ORDER</th>\n",
       "      <th>IMAGE_FILE</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>IMAGE_SIZE</th>\n",
       "      <th>IMAGE_WIDTH</th>\n",
       "      <th>IMAGE_HEIGHT</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>190</td>\n",
       "      <td>210</td>\n",
       "      <td>3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_3_page_36_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_2...</td>\n",
       "      <td>28987</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-22 04:25:31.443000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>210</td>\n",
       "      <td>3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>doc_3_page_36_img_3.png</td>\n",
       "      <td>.\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_3...</td>\n",
       "      <td>15373</td>\n",
       "      <td>163</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-22 04:25:31.443000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>192</td>\n",
       "      <td>210</td>\n",
       "      <td>3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>doc_3_page_36_img_4.png</td>\n",
       "      <td>.\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_4...</td>\n",
       "      <td>13176</td>\n",
       "      <td>164</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-22 04:25:31.443000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>193</td>\n",
       "      <td>216</td>\n",
       "      <td>3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_3_page_46_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WGG254Z0GB\\doc_3_page_46_img_1...</td>\n",
       "      <td>16799</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-22 04:25:31.443000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>194</td>\n",
       "      <td>216</td>\n",
       "      <td>3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_3_page_46_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WGG254Z0GB\\doc_3_page_46_img_2...</td>\n",
       "      <td>16547</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>2025-04-22 04:25:31.443000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     IMAGE_ID  SECTION_ID  DOCUMENT_ID SECTION_NUMBER  PAGE  IMG_ORDER  \\\n",
       "189       190         210            3           16.3    36          2   \n",
       "190       191         210            3           16.3    36          3   \n",
       "191       192         210            3           16.3    36          4   \n",
       "192       193         216            3           18.3    46          1   \n",
       "193       194         216            3           18.3    46          2   \n",
       "\n",
       "                  IMAGE_FILE  \\\n",
       "189  doc_3_page_36_img_2.png   \n",
       "190  doc_3_page_36_img_3.png   \n",
       "191  doc_3_page_36_img_4.png   \n",
       "192  doc_3_page_46_img_1.png   \n",
       "193  doc_3_page_46_img_2.png   \n",
       "\n",
       "                                            IMAGE_PATH  IMAGE_SIZE  \\\n",
       "189  .\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_2...       28987   \n",
       "190  .\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_3...       15373   \n",
       "191  .\\Washer_Images\\WGG254Z0GB\\doc_3_page_36_img_4...       13176   \n",
       "192  .\\Washer_Images\\WGG254Z0GB\\doc_3_page_46_img_1...       16799   \n",
       "193  .\\Washer_Images\\WGG254Z0GB\\doc_3_page_46_img_2...       16547   \n",
       "\n",
       "     IMAGE_WIDTH  IMAGE_HEIGHT                       CREATED_AT  \n",
       "189          328           237 2025-04-22 04:25:31.443000-07:00  \n",
       "190          163           237 2025-04-22 04:25:31.443000-07:00  \n",
       "191          164           237 2025-04-22 04:25:31.443000-07:00  \n",
       "192          328           237 2025-04-22 04:25:31.443000-07:00  \n",
       "193          328           237 2025-04-22 04:25:31.443000-07:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM IMAGES;\n",
    "\"\"\")\n",
    "\n",
    "images_df = cursor.fetch_pandas_all()\n",
    "images_df.head()\n",
    "\n",
    "images_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VestasVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "authorEmail": "emhaldemo1@gmail.com",
   "authorId": "3960725274243",
   "authorName": "EMHALDEMO1",
   "lastEditTime": 1744786105031,
   "notebookId": "yjjjwqle6a6h6njufd4n",
   "sessionId": "0bd143f8-a219-40e9-853f-0ddbdab7a3c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

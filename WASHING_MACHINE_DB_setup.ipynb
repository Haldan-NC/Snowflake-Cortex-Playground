{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## This Notebook will create the database for the temporary project washing machine manuals\n",
    "\n",
    "The intention of this notebook is to create a clean and best practice database structure, along with utilizing snowflakes AI functions.\n",
    "\n",
    "The intended database structure is as follows: \n",
    "\n",
    "- **documents** (Stores metadata about each manual)  \n",
    "  - `document_id` (Unique ID for each manual)  \n",
    "  - `doc_name` (Document name)\n",
    "  - `version` (Version or revision number)  \n",
    "  - `relative_path` (Original PDF file path or S3 URL) \n",
    "  - `stage_name`  (snowflake stage name (source))\n",
    "  - `size`  (size in bytes of the PDF document) \n",
    "\n",
    "<!-- - **machine_types** (Stores metadata about each manual)  \n",
    "  - `machine_type_id` (Unique ID for each machine type)\n",
    "  - `manual_id` (foreign key referencing `manuals`)  \n",
    "  - `name` (Type of machine or equipment) -->\n",
    "\n",
    "- **sections** (Defines logical sections and subsections within each manual)  \n",
    "  - `section_id` (Unique ID for the section)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `title` (Title or heading of the section)  \n",
    "  - `order_num` (Numerical order of the section in the manual)  \n",
    "  - `parent_section_id` (Optional FK for nested subsections)  \n",
    "\n",
    "- **chunks small** (1024 characters, 64 overlap)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **chunks large** (4096 characters, overlap 256)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **images** (Stores references to images extracted from the manual)  \n",
    "  - `image_id` (Unique ID for the image)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `page_number` \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `order_num` (Display order within the section)  \n",
    "  - `image_path` (S3 or web-accessible path to the image)  \n",
    "  - `description`   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6fc673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keyring\n",
    "import os \n",
    "import snowflake.connector as sf_connector # ( https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect)\n",
    "from snowflake.connector.pandas_tools import write_pandas # (https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#write_pandas)\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_docling import DoclingLoader\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Set max rows to display in pandas DataFrame 200\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ca260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Identifier:  EPTJRCA-HWB83214\n",
      "User Name:  EMHALDEMO1\n",
      "Database:  WASHING_MACHINE_MANUALS\n",
      "Schema:  PUBLIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x20c64ce3d10>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_identifier = keyring.get_password('NC_Snowflake_Trial_Account_Name', 'account_identifier')\n",
    "user_name = \"EMHALDEMO1\"\n",
    "password = keyring.get_password('NC_Snowflake_Trial_User_Password', user_name)\n",
    "database = \"WASHING_MACHINE_MANUALS\"\n",
    "schema = \"PUBLIC\"\n",
    "\n",
    "print(\"Account Identifier: \", account_identifier)\n",
    "print(\"User Name: \", user_name)\n",
    "print(\"Database: \", database)\n",
    "print(\"Schema: \", schema)\n",
    "\n",
    "try:\n",
    "    connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": database,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "except:\n",
    "        connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"schema\": \"CORTEX\"\n",
    "    }\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = sf_connector.connect(\n",
    "    user=connection_parameters['user'],\n",
    "    password=connection_parameters['password'],\n",
    "    account=connection_parameters['account_identifier'],\n",
    "    warehouse=connection_parameters['warehouse'],\n",
    "    database=connection_parameters['database'],\n",
    "    schema=connection_parameters['schema'],\n",
    "    role=connection_parameters['role']\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\" CREATE DATABASE IF NOT EXISTS {database}; \")\n",
    "cursor.execute(f\" CREATE SCHEMA IF NOT EXISTS {database}.{schema}; \")\n",
    "cursor.execute(f\" USE DATABASE {database}; \")\n",
    "cursor.execute(f\" USE SCHEMA {schema}; \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055a0e",
   "metadata": {},
   "source": [
    "## Create a stage for the PDF files with the code below\n",
    "#### DO NOT RUN - unless you don't have the documents in the stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bb5ea-b91f-4578-aa76-6c1389acd1aa",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x1943df39760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating stage to dump PDF documents into\n",
    "# cursor.execute(\" create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true ); \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530a54",
   "metadata": {},
   "source": [
    "## Creating documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c452e4c-6ecc-418e-a988-0ee231cf5ae6",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x20c64ce3d10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE DOCUMENTS (\n",
    "    DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_NAME STRING,\n",
    "    DOC_VERSION STRING,\n",
    "    RELATIVE_PATH STRING NOT NULL,\n",
    "    SIZE NUMBER,\n",
    "    STAGE_NAME STRING DEFAULT '@docs',\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO DOCUMENTS (DOCUMENT_NAME, SIZE, RELATIVE_PATH)\n",
    "    SELECT relative_path, size, file_url\n",
    "    FROM DIRECTORY(@docs);\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5524594c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_NAME</th>\n",
       "      <th>DOC_VERSION</th>\n",
       "      <th>RELATIVE_PATH</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>STAGE_NAME</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WGG254Z0GB.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bbb84589.snowflakecomputing.com/api/fi...</td>\n",
       "      <td>3291555</td>\n",
       "      <td>@docs</td>\n",
       "      <td>2025-04-16 05:35:39.026000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>technical-manual-w11663204-revb.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bbb84589.snowflakecomputing.com/api/fi...</td>\n",
       "      <td>17270389</td>\n",
       "      <td>@docs</td>\n",
       "      <td>2025-04-16 05:35:39.026000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID                        DOCUMENT_NAME DOC_VERSION  \\\n",
       "0            1                       WGG254Z0GB.pdf        None   \n",
       "1            2  technical-manual-w11663204-revb.pdf        None   \n",
       "\n",
       "                                       RELATIVE_PATH      SIZE STAGE_NAME  \\\n",
       "0  https://bbb84589.snowflakecomputing.com/api/fi...   3291555      @docs   \n",
       "1  https://bbb84589.snowflakecomputing.com/api/fi...  17270389      @docs   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-16 05:35:39.026000-07:00  \n",
       "1 2025-04-16 05:35:39.026000-07:00  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the table\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "\"\"\")\n",
    "\n",
    "documents_df = cursor.fetch_pandas_all()\n",
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1f2c9-1d83-48c5-8be6-40c1760576c8",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "# The section below focuses on creating chunks_large and chunks_small.\n",
    "\n",
    "Different size chunks are good at different things - it could be a good idea to store both size, especially during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1ccc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number: 0  :  .\\Washer_Manuals\\WGG254Z0GB.pdf\n",
      "Document number: 1  :  .\\Washer_Manuals\\technical-manual-w11663204-revb.pdf\n"
     ]
    }
   ],
   "source": [
    "# Shows local file placement\n",
    "\n",
    "pdf_files_path = \".\\\\Washer_Manuals\"\n",
    "pdf_file_list = [doc for doc in documents_df[\"DOCUMENT_NAME\"] if doc.endswith(\".pdf\")]\n",
    "\n",
    "for idx,filename in enumerate(pdf_file_list):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_files_path, filename)\n",
    "        print(f\"Document number: {idx}  : \",file_path)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524735a",
   "metadata": {},
   "source": [
    "## Creating chunks tables with vector embeddings\n",
    "\n",
    "To include page numbers, i decided to create the tables using pandas, and then uploading them to snowflake\n",
    "\n",
    "Followed by that will be a query to crete a vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e88f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>ectly, this causes a tripping hazard. ▶ Lay ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>components by type and dispose of them separ- ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>Off to switch off the appliance. → \"Basic ope...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>has been reached, it remains constant througho...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>Start / Reload. The appliance pauses. 13.12 U...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>42</td>\n",
       "      <td>because the laundry is unevenly distrib- uted....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "      <td>ault Cause and troubleshooting The spin progra...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>51</td>\n",
       "      <td>er (FD) 2012/19/EU concerning on the appliance...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>TECHNICAL MANUAL Maytag® 3.5 cu ft Commercial-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>tca puaamy. Tam epautca ce oaam up Cf fivm pae...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>ER M VW P 586 G W INTERNATIONAL SALES OR MARKE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>Coomeccaal-eaam mecamenaa lgctatCe raeame HSA...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>euobme camenficanCee. Pemee tam STl T buttCe t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>cnCee Cccue: n Ueme pemeeme STl T afme emvmeaa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>o PM. NOTE: laaCw up tC 15 emcCeae fCe eacfme ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>when energized. (cid:31) Allow enough space t...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>CeeCam tC accmee oace cCeteCa. Caoomtme, vmecf...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>peCpmeay tam oace cCeteCa, oCtCe, aea eue capa...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>cCeneucty, empaacm tam aCwme oacacem empmat et...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>a fiegme Ce tam B. J16 connector Ctame aaea tC...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>eceg tam tub eceg tC tam Cutme tub. Tamem aem ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>t pCwme. 2. moCvm CeeCam aea TCp paema cCopCem...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  \\\n",
       "0             1                  0                5   \n",
       "1             1                  5               10   \n",
       "2             1                 10               17   \n",
       "3             1                 16               24   \n",
       "4             1                 24               30   \n",
       "5             1                 30               37   \n",
       "6             1                 37               42   \n",
       "7             1                 42               46   \n",
       "8             1                 46               51   \n",
       "9             2                  0                6   \n",
       "10            2                  6               11   \n",
       "11            2                 11               17   \n",
       "12            2                 17               19   \n",
       "13            2                 19               22   \n",
       "14            2                 22               25   \n",
       "15            2                 25               31   \n",
       "16            2                 31               34   \n",
       "17            2                 34               36   \n",
       "18            2                 36               38   \n",
       "19            2                 38               46   \n",
       "20            2                 46               53   \n",
       "21            2                 53               59   \n",
       "22            2                 59               65   \n",
       "\n",
       "                                           CHUNK_TEXT  CHUNK_ORDER  \n",
       "0   Register your b M o ge s y n c t B e h f o r w...            0  \n",
       "1   ectly, this causes a tripping hazard. ▶ Lay ho...            1  \n",
       "2   components by type and dispose of them separ- ...            2  \n",
       "3    Off to switch off the appliance. → \"Basic ope...            3  \n",
       "4   has been reached, it remains constant througho...            4  \n",
       "5    Start / Reload. The appliance pauses. 13.12 U...            5  \n",
       "6   because the laundry is unevenly distrib- uted....            6  \n",
       "7   ault Cause and troubleshooting The spin progra...            7  \n",
       "8   er (FD) 2012/19/EU concerning on the appliance...            8  \n",
       "9   TECHNICAL MANUAL Maytag® 3.5 cu ft Commercial-...            0  \n",
       "10  tca puaamy. Tam epautca ce oaam up Cf fivm pae...            1  \n",
       "11  ER M VW P 586 G W INTERNATIONAL SALES OR MARKE...            2  \n",
       "12   Coomeccaal-eaam mecamenaa lgctatCe raeame HSA...            3  \n",
       "13  euobme camenficanCee. Pemee tam STl T buttCe t...            4  \n",
       "14  cnCee Cccue: n Ueme pemeeme STl T afme emvmeaa...            5  \n",
       "15  o PM. NOTE: laaCw up tC 15 emcCeae fCe eacfme ...            6  \n",
       "16   when energized. (cid:31) Allow enough space t...            7  \n",
       "17  CeeCam tC accmee oace cCeteCa. Caoomtme, vmecf...            8  \n",
       "18  peCpmeay tam oace cCeteCa, oCtCe, aea eue capa...            9  \n",
       "19  cCeneucty, empaacm tam aCwme oacacem empmat et...           10  \n",
       "20  a fiegme Ce tam B. J16 connector Ctame aaea tC...           11  \n",
       "21  eceg tam tub eceg tC tam Cutme tub. Tamem aem ...           12  \n",
       "22  t pCwme. 2. moCvm CeeCam aea TCp paema cCopCem...           13  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting section headers from the PDF files\n",
    "\n",
    "def extract_text_chunks(file_path, manual_id, chunk_size=512, chunk_overlap=128):\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Step 1: Combine all text across pages with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = []  # (char_index, page_number)\n",
    "\n",
    "    for doc_page in docs:\n",
    "        text = doc_page.page_content.strip().replace('\\n', ' ')\n",
    "        start_idx = len(all_text)\n",
    "        all_text += text + \" \"  # Add space to separate pages\n",
    "        end_idx = len(all_text)\n",
    "        page_map.append((start_idx, end_idx, doc_page.metadata['page']))\n",
    "\n",
    "    # Step 2: Create chunks with overlap, spanning across pages\n",
    "    chunks = []\n",
    "    chunk_order = []\n",
    "    page_start_list = []\n",
    "    page_end_list = []\n",
    "\n",
    "    idx = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while idx < len(all_text):\n",
    "        chunk = all_text[idx:idx + chunk_size]\n",
    "\n",
    "        # Determine pages involved in this chunk\n",
    "        chunk_start = idx\n",
    "        chunk_end = idx + len(chunk)\n",
    "\n",
    "        pages_in_chunk = [\n",
    "            page_num\n",
    "            for start, end, page_num in page_map\n",
    "            if not (end <= chunk_start or start >= chunk_end)  # overlap condition\n",
    "        ]\n",
    "\n",
    "        page_start = min(pages_in_chunk) if pages_in_chunk else None\n",
    "        page_end = max(pages_in_chunk) if pages_in_chunk else None\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        page_start_list.append(page_start)\n",
    "        page_end_list.append(page_end)\n",
    "        chunk_order.append(chunk_idx)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        idx += chunk_size - chunk_overlap\n",
    "\n",
    "    # Step 3: Create DataFrame\n",
    "    rows = [{\n",
    "        'DOCUMENT_ID': manual_id,\n",
    "        'PAGE_START_NUMBER': start,\n",
    "        'PAGE_END_NUMBER': end,\n",
    "        'CHUNK_TEXT': chunk,\n",
    "        'CHUNK_ORDER': order\n",
    "    } for chunk, start, end, order in zip(chunks, page_start_list, page_end_list, chunk_order)]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"DOCUMENT_ID\", \"PAGE_START_NUMBER\", \"PAGE_END_NUMBER\", \"CHUNK_TEXT\", \"CHUNK_ORDER\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "large_chunks_df = pd.DataFrame()\n",
    "for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    file_path = os.path.join(pdf_files_path, row[1][\"DOCUMENT_NAME\"])\n",
    "\n",
    "    tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                        manual_id = manual_id,\n",
    "                        chunk_size = 6000,#1024,\n",
    "                        chunk_overlap = 128)  # Show first 5 chunks\n",
    "    large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "large_chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df1fb5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x20c64ce3d10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    PAGE_START_NUMBER INT,\n",
    "    PAGE_END_NUMBER INT,\n",
    "    CHUNK_ORDER INT,\n",
    "    CHUNK_TEXT STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc67cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x20c64ce3d10>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=large_chunks_df,\n",
    "    database =database,\n",
    "    table_name=\"CHUNKS_LARGE\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "# Update the embeddings for the chunks in the CHUNKS_LARGE table\n",
    "cursor.execute(\"\"\"\n",
    "    UPDATE CHUNKS_LARGE\n",
    "    SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0',\n",
    "        CHUNK_TEXT\n",
    "    )\n",
    "    WHERE EMBEDDING IS NULL;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d11219e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True, Chunks: 1, Rows: 230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x20c64ce3d10>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_chunks_df = pd.DataFrame()\n",
    "for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                        manual_id = manual_id,\n",
    "                        chunk_size = 1024,\n",
    "                        chunk_overlap = 64)  # Show first 5 chunks\n",
    "    small_chunks_df = pd.concat([small_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    PAGE_START_NUMBER INT,\n",
    "    PAGE_END_NUMBER INT,\n",
    "    CHUNK_ORDER INT,\n",
    "    CHUNK_TEXT STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=small_chunks_df,\n",
    "    database =database,\n",
    "    table_name=\"CHUNKS_SMALL\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "# Update the embeddings for the small chunks\n",
    "cursor.execute(\"\"\"\n",
    "    UPDATE CHUNKS_SMALL\n",
    "    SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0',\n",
    "        CHUNK_TEXT\n",
    "    )\n",
    "    WHERE EMBEDDING IS NULL;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b8c0f",
   "metadata": {},
   "source": [
    "## Creating sections table using LLM for TOC extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOC(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = cursor.execute(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\")\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "\n",
    "    return cursor.fetch_pandas_all().iloc[0,0]\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "llm_output = extract_TOC(large_chunks_df.loc[0,\"CHUNK_TEXT\"], model = 'llama3.1-70b')\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Section\": \"Table of contents\",\n",
      "  \"Section Number\": \"\",\n",
      "  \"Page\": \"\",\n",
      "  \"Sub Sections\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_json_from_llm_output(llm_output: str) -> dict:\n",
    "    try:\n",
    "        # Confirming that a JSON block is returned\n",
    "        match = re.search(r\"```\\s*(\\{.*?\\})\\s*```\", llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        # Extracting sub string (json string)\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Clean common JSON errors (e.g., trailing commas)\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)  # remove trailing commas before ] or }\n",
    "        \n",
    "        # Parse string to json\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "        \n",
    "parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "print(json.dumps(parsed_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9c440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45c80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f6a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f9dba-c1e4-43c7-8b07-c45b73360f5e",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "-- Creates the table for storing the LARGE chunks and vector embeddings\n",
    "CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    CHUNK_INDEX INT,\n",
    "    CHUNK STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\n",
    "\n",
    "-- Creates a temp table with parsed text (1 row for each document, with a super long string of raw text of the document)\n",
    "CREATE OR REPLACE TEMP TABLE parsed_text_table AS\n",
    "SELECT \n",
    "  relative_path as document_name,\n",
    "  size,\n",
    "  file_url,\n",
    "  BUILD_SCOPED_FILE_URL(@docs, relative_path) AS scoped_file_url,\n",
    "  TO_VARCHAR(\n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'OCR'}) -- trying this instead of LAYOUT\n",
    "  ) AS full_text\n",
    "FROM DIRECTORY(@docs);\n",
    "\n",
    "\n",
    "-- Temp table for extracting the content. (Disabled as it gave worse results when finding sections)\n",
    "-- CREATE OR REPLACE TEMP TABLE parsed_text_table_extracted AS\n",
    "-- SELECT\n",
    "--   document_name,\n",
    "--   PARSE_JSON(full_text):content::STRING AS full_text\n",
    "-- FROM parsed_text_table;\n",
    "\n",
    "\n",
    "-- Using the temporary table to fill the CHUNKS tables with \n",
    "INSERT INTO CHUNKS_LARGE (DOCUMENT_ID, CHUNK_INDEX, CHUNK, EMBEDDING)\n",
    "SELECT \n",
    "    d.DOCUMENT_ID,\n",
    "    chunk_data.index AS CHUNK_INDEX,\n",
    "    chunk_data.value::STRING AS CHUNK,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_1024('snowflake-arctic-embed-l-v2.0', chunk_data.value::STRING) AS EMBEDDING\n",
    "FROM parsed_text_table p\n",
    "JOIN DOCUMENTS d ON p.document_name = d.document_name\n",
    "JOIN LATERAL FLATTEN(\n",
    "    INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "        p.full_text,\n",
    "        'none',         -- 'none' or 'markdown'\n",
    "        6000,           -- chunk size\n",
    "        256             -- overlap\n",
    "    )\n",
    ") AS chunk_data\n",
    "WHERE p.full_text IS NOT NULL;\n",
    "\n",
    "SELECT * \n",
    "FROM CHUNKS_LARGE \n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7012d-101d-457d-92dd-847c5ed99550",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "### And now, small chunks table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01253f-27ec-49e0-9886-7df765fd5097",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "-- Creates the table for storing the LARGE chunks and vector embeddings\n",
    "CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    CHUNK_INDEX INT,\n",
    "    CHUNK STRING NOT NULL,\n",
    "    EMBEDDING VECTOR(FLOAT, 1024),\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");\n",
    "\n",
    "-- Creating a temp table is not needed since we made one with the large chunks.\n",
    "\n",
    "-- Using the temporary table to fill the CHUNKS tables with \n",
    "INSERT INTO CHUNKS_SMALL (DOCUMENT_ID, CHUNK_INDEX, CHUNK, EMBEDDING)\n",
    "SELECT \n",
    "    d.DOCUMENT_ID,\n",
    "    chunk_data.index AS CHUNK_INDEX,\n",
    "    chunk_data.value::STRING AS CHUNK,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_1024('snowflake-arctic-embed-l-v2.0', chunk_data.value::STRING) AS EMBEDDING\n",
    "FROM parsed_text_table p\n",
    "JOIN DOCUMENTS d ON p.document_name = d.document_name\n",
    "JOIN LATERAL FLATTEN(\n",
    "    INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "        p.full_text,\n",
    "        'none',    -- 'none' or 'markdown'\n",
    "        512,           -- chunk size\n",
    "        64             -- overlap\n",
    "    )\n",
    ") AS chunk_data\n",
    "WHERE p.full_text IS NOT NULL;\n",
    "\n",
    "SELECT * \n",
    "FROM CHUNKS_SMALL \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a998ff6-88eb-4353-a5a7-560483a8668c",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "### This section will create the sections table, using an LLM to extract the TOC. \n",
    "\n",
    "This method assumes that the TOC is found in the first LARGE chunk for each document.\n",
    "While this is absolutely not best practice, alternative methods were shown to be time consuming and inconsistent. For the sake of this task, this will be more than sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b424a-3566-40b9-8e62-65b6b0abf338",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE SECTIONS (\n",
    "    SECTION_ID INT,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    SECTION_NAME STRING,\n",
    "    SECTION_NUMBER STRING,\n",
    "    PAGE INT,\n",
    "    PARENT_SECTION_ID INT,\n",
    "    PARENT_SECTION_NUMBER STRING,\n",
    "    CHUNK_ID INT,\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    \n",
    "    -- Constraints\n",
    "    PRIMARY KEY (SECTION_ID),\n",
    "    FOREIGN KEY (DOCUMENT_ID) REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14278f89-79e5-4d11-ae17-d828bd69d3f2",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "session = Session.builder.getOrCreate()\n",
    "df_large_chunks = session.table(\"CHUNKS_LARGE\").to_pandas()\n",
    "df_large_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ba327-da68-43e6-b436-8ff623a7568d",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "def extract_TOC(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = session.sql(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\").collect()\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'llama3.1-70b')\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce129953-600c-4e38-bd0a-8630af7b493e",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "def extract_json_from_llm_output(llm_output: str) -> dict:\n",
    "    try:\n",
    "        # Confirming that a JSON block is returned\n",
    "        match = re.search(r\"```\\s*(\\{.*?\\})\\s*```\", llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        # Extracting sub string (json string)\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Clean common JSON errors (e.g., trailing commas)\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)  # remove trailing commas before ] or }\n",
    "        \n",
    "        # Parse string to json\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "        \n",
    "parsed_dict = extract_json_from_llm_output(llm_output[0][0])\n",
    "print(json.dumps(parsed_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469e678-37ce-4999-acc3-6d8eb26eef11",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "def create_pandas_table(parsed_dict: dict, document_id: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    section_id_counter = [1]  # counter for Section_IDs\n",
    "\n",
    "    def recurse(sections, parent_id=None, parent_number=None):\n",
    "        for section in sections:\n",
    "            current_id = section_id_counter[0]\n",
    "            section_id_counter[0] += 1\n",
    "\n",
    "            current_number = section.get(\"Section Number\")\n",
    "\n",
    "            rows.append({\n",
    "                \"Section_ID\": current_id,\n",
    "                \"Document_ID\": document_id,\n",
    "                \"Section_Name\": section.get(\"Section\"),\n",
    "                \"Section_Number\": current_number,\n",
    "                \"Page\": section.get(\"Page\"),\n",
    "                \"Parent_Section_ID\": parent_id,\n",
    "                \"Parent_Section_Number\": parent_number\n",
    "            })\n",
    "\n",
    "            # Recurse into sub-sections\n",
    "            sub_sections = section.get(\"Sub Sections\", [])\n",
    "            if isinstance(sub_sections, list) and sub_sections:\n",
    "                recurse(\n",
    "                    sub_sections,\n",
    "                    parent_id=current_id,\n",
    "                    parent_number=current_number\n",
    "                )\n",
    "\n",
    "    top_level_sections = parsed_dict.get(\"Sub Sections\", []) # Some quirky issue with the format of the returned JSON object.\n",
    "    # top_level_sections = parsed_dict[\"Table of contents\"]\n",
    "    recurse(top_level_sections)\n",
    "\n",
    "    section_df = pd.DataFrame(rows)\n",
    "    section_df[\"Chunk_ID\"] = None\n",
    "\n",
    "    return section_df\n",
    "\n",
    "parsed_dict = extract_json_from_llm_output(llm_output[0][0])\n",
    "document_id = df_large_chunks.loc[0,\"DOCUMENT_ID\"]\n",
    "df_sections = create_pandas_table(parsed_dict, document_id)\n",
    "df_sections.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518657b-da24-4580-9cab-6d27f9a520d0",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "df_sections.columns = [col.upper() for col in df_sections.columns]\n",
    "\n",
    "session.write_pandas(\n",
    "    df_sections,\n",
    "    table_name=\"SECTIONS\",\n",
    "    overwrite=False,\n",
    "    auto_create_table=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad3789-b957-4bc6-bec9-ac28bce27306",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM SECTIONS \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a526f0d-979b-4540-be20-ec898dea433f",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "authorEmail": "emhaldemo1@gmail.com",
   "authorId": "3960725274243",
   "authorName": "EMHALDEMO1",
   "lastEditTime": 1744786105031,
   "notebookId": "yjjjwqle6a6h6njufd4n",
   "sessionId": "0bd143f8-a219-40e9-853f-0ddbdab7a3c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

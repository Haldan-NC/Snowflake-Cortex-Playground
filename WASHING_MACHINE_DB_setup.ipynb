{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## This Notebook will create the database for the temporary project washing machine manuals\n",
    "\n",
    "The intention of this notebook is to create a clean and best practice database structure, along with utilizing snowflakes AI functions.\n",
    "\n",
    "The intended database structure is as follows: \n",
    "\n",
    "- **documents** (Stores metadata about each manual)  \n",
    "  - `document_id` (Unique ID for each manual)  \n",
    "  - `doc_name` (Document name)\n",
    "  - `version` (Version or revision number)  \n",
    "  - `relative_path` (Original PDF file path or S3 URL) \n",
    "  - `stage_name`  (snowflake stage name (source))\n",
    "  - `size`  (size in bytes of the PDF document) \n",
    "\n",
    "- **sections** (Defines logical sections and subsections within each manual)  \n",
    "  - `section_id` (Unique ID for the section)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `title` (Title or heading of the section)  \n",
    "  - `order_num` (Numerical order of the section in the manual)  \n",
    "  - `parent_section_id` (Optional FK for nested subsections)  \n",
    "\n",
    "- **chunks small** (1024 characters, 64 overlap)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **chunks large** (4096 characters, overlap 256)\n",
    "  - `chunk_id` (Unique ID for the chunk)  \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `chunk_text` (The text content of the chunk)  \n",
    "  - `chunk_order` (Order of the chunk within the section)  \n",
    "  - `embedding` (Vector for semantic search or embeddings)  \n",
    "\n",
    "- **images** (Stores references to images extracted from the manual)  \n",
    "  - `image_id` (Unique ID for the image)  \n",
    "  - `manual_id` (Foreign key referencing `manuals`)  \n",
    "  - `page_number` \n",
    "  - `section_id` (Foreign key referencing `sections`)  \n",
    "  - `order_num` (Display order within the section)  \n",
    "  - `image_path` (S3 or web-accessible path to the image)  \n",
    "  - `image_size` (Size of the image in bytes)\n",
    "  - `image_width` \n",
    "  - `image_height`\n",
    "  - `image_coordinates` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6fc673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keyring\n",
    "import os \n",
    "import snowflake.connector as sf_connector # ( https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect)\n",
    "from snowflake.connector.pandas_tools import write_pandas # (https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#write_pandas)\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.evaluation import load_evaluator\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from io import BytesIO\n",
    "import fitz \n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from openai import OpenAI\n",
    "from RAG_app import get_openai_api_key, generate_promt_for_openai_api\n",
    "\n",
    "# Set max rows to display in pandas DataFrame 200\n",
    "pd.set_option('display.max_rows', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a90ca260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Identifier:  EPTJRCA-HWB83214\n",
      "User Name:  EMHALDEMO1\n",
      "Database:  WASHING_MACHINE_MANUALS\n",
      "Schema:  PUBLIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x2e5778715e0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "try:\n",
    "    account_identifier = keyring.get_password('NC_Snowflake_Trial_Account_Name', 'account_identifier')\n",
    "    user_name = \"EMHALDEMO1\" # Change this to your Snowflake user name\n",
    "    password = keyring.get_password('NC_Snowflake_Trial_User_Password', user_name)\n",
    "    database = \"WASHING_MACHINE_MANUALS\"\n",
    "    schema = \"PUBLIC\"\n",
    "except:\n",
    "    print(\"Please set your Snowflake account identifier and user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_Account_Name' and username 'account_identifier'\")\n",
    "    print(\"Please set your Snowflake user password in microsoft generic credential manager with the hostname 'NC_Snowflake_Trial_User_Password' and username \")\n",
    "\n",
    "\n",
    "print(\"Account Identifier: \", account_identifier)\n",
    "print(\"User Name: \", user_name)\n",
    "print(\"Database: \", database)\n",
    "print(\"Schema: \", schema)\n",
    "\n",
    "try:\n",
    "    connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": database,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "except:\n",
    "        connection_parameters = {\n",
    "        \"account_identifier\": account_identifier,\n",
    "        \"user\": user_name,\n",
    "        \"password\": password,\n",
    "        \"role\": \"ACCOUNTADMIN\",\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"SNOWFLAKE\",\n",
    "        \"schema\": \"CORTEX\"\n",
    "    }\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = sf_connector.connect(\n",
    "    user=connection_parameters['user'],\n",
    "    password=connection_parameters['password'],\n",
    "    account=connection_parameters['account_identifier'],\n",
    "    warehouse=connection_parameters['warehouse'],\n",
    "    database=connection_parameters['database'],\n",
    "    schema=connection_parameters['schema'],\n",
    "    role=connection_parameters['role']\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\" CREATE DATABASE IF NOT EXISTS {database}; \")\n",
    "cursor.execute(f\" CREATE SCHEMA IF NOT EXISTS {database}.{schema}; \")\n",
    "cursor.execute(f\" USE DATABASE {database}; \")\n",
    "cursor.execute(f\" USE SCHEMA {schema}; \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055a0e",
   "metadata": {},
   "source": [
    "## Create a stage for the PDF files with the code below\n",
    " Try Except wrapping the code should catch if you already have the database and data created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1530a54",
   "metadata": {},
   "source": [
    "## Creating documents table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dcced24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_NAME</th>\n",
       "      <th>DOC_VERSION</th>\n",
       "      <th>FILE_PATH</th>\n",
       "      <th>FILE_SIZE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WAN28258GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAN28258GB.pdf</td>\n",
       "      <td>3374759</td>\n",
       "      <td>2025-04-29 03:40:33.604000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WAN28282GC.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAN28282GC.pdf</td>\n",
       "      <td>3004904</td>\n",
       "      <td>2025-04-29 03:40:33.604000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>WAT24168IN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAT24168IN.pdf</td>\n",
       "      <td>4721819</td>\n",
       "      <td>2025-04-29 03:40:33.604000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>WAV28KH3GB.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WAV28KH3GB.pdf</td>\n",
       "      <td>5686613</td>\n",
       "      <td>2025-04-29 03:40:33.604000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>WGA1340SIN.pdf</td>\n",
       "      <td>N/A</td>\n",
       "      <td>.\\Washer_Manuals\\WGA1340SIN.pdf</td>\n",
       "      <td>3269274</td>\n",
       "      <td>2025-04-29 03:40:33.604000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_ID   DOCUMENT_NAME DOC_VERSION                        FILE_PATH  \\\n",
       "0            1  WAN28258GB.pdf         N/A  .\\Washer_Manuals\\WAN28258GB.pdf   \n",
       "1            2  WAN28282GC.pdf         N/A  .\\Washer_Manuals\\WAN28282GC.pdf   \n",
       "2            3  WAT24168IN.pdf         N/A  .\\Washer_Manuals\\WAT24168IN.pdf   \n",
       "3            4  WAV28KH3GB.pdf         N/A  .\\Washer_Manuals\\WAV28KH3GB.pdf   \n",
       "4            5  WGA1340SIN.pdf         N/A  .\\Washer_Manuals\\WGA1340SIN.pdf   \n",
       "\n",
       "   FILE_SIZE                       CREATED_AT  \n",
       "0    3374759 2025-04-29 03:40:33.604000-07:00  \n",
       "1    3004904 2025-04-29 03:40:33.604000-07:00  \n",
       "2    4721819 2025-04-29 03:40:33.604000-07:00  \n",
       "3    5686613 2025-04-29 03:40:33.604000-07:00  \n",
       "4    3269274 2025-04-29 03:40:33.604000-07:00  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "    \"\"\")\n",
    "\n",
    "    documents_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    print(\"Creating table DOCUMENTS\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE DOCUMENTS (\n",
    "        DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_NAME STRING,\n",
    "        DOC_VERSION STRING,\n",
    "        FILE_PATH STRING NOT NULL,\n",
    "        FILE_SIZE NUMBER,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table DOCUMENTS created. Fetching data from PDF files.\")\n",
    "    pdf_files_path = \".\\\\Washer_Manuals\"\n",
    "    document_rows = []\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(pdf_files_path)):\n",
    "        # Temporary filter to only process a set of PDF files\n",
    "        # if filename not in [\"WGG254Z0GB.pdf\", \"WGA1420SIN.pdf\"]: #,\"WAV28KH3GB.pdf\"]:\n",
    "        #     continue\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_files_path, filename)\n",
    "            print(f\"Document number: {idx}  : {file_path}\")\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            document_rows.append({\n",
    "                \"DOCUMENT_NAME\": filename,\n",
    "                \"FILE_PATH\": file_path,\n",
    "                \"DOC_VERSION\": \"N/A\",  # Placeholder, you can modify this logic as needed\n",
    "                \"FILE_SIZE\": file_size\n",
    "            })\n",
    "\n",
    "    documents_df = pd.DataFrame(document_rows)\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=documents_df,\n",
    "        database=database,\n",
    "        table_name=\"DOCUMENTS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Number of chunks: {nchunks}, Number of rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM DOCUMENTS;\n",
    "    \"\"\")\n",
    "\n",
    "    documents_df = cursor.fetch_pandas_all()\n",
    "\n",
    "documents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524735a",
   "metadata": {},
   "source": [
    "# The section below focuses on creating chunks_large and chunks_small.\n",
    "\n",
    "Different size chunks are good at different things - it could be a good idea to store both size, especially during testing\n",
    "\n",
    "To include page numbers, i decided to create the tables using pandas, and then uploading them to snowflake\n",
    "\n",
    "Followed by that will be a query to crete a vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34e88f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>EMBEDDING</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>[0.028259277, 0.038513184, 0.008262634, 0.0149...</td>\n",
       "      <td>2025-04-29 03:42:34.872000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>appliance may result in injury when lifted. ▶...</td>\n",
       "      <td>[0.051940918, 0.04940796, -0.03387451, 0.04159...</td>\n",
       "      <td>2025-04-29 03:42:34.872000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>plied bolts which have not been removed with t...</td>\n",
       "      <td>[0.072387695, 0.07904053, 0.0060920715, 0.0329...</td>\n",
       "      <td>2025-04-29 03:42:34.872000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>the water inlet hose is connected in- correct...</td>\n",
       "      <td>[0.06744385, 0.04437256, -0.02709961, -0.02598...</td>\n",
       "      <td>2025-04-29 03:42:34.872000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>ry. Note: Add the detergent for the pre- wash ...</td>\n",
       "      <td>[0.05621338, -0.016036987, -0.034729004, 0.008...</td>\n",
       "      <td>2025-04-29 03:42:34.872000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHUNK_ID  DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  CHUNK_ORDER  \\\n",
       "0         1            1                  0                5            0   \n",
       "1         2            1                  5                9            1   \n",
       "2         3            1                  9               13            2   \n",
       "3         4            1                 13               20            3   \n",
       "4         5            1                 20               25            4   \n",
       "\n",
       "                                          CHUNK_TEXT  \\\n",
       "0  Register your b M o ge s y n c t B e h f o r w...   \n",
       "1   appliance may result in injury when lifted. ▶...   \n",
       "2  plied bolts which have not been removed with t...   \n",
       "3   the water inlet hose is connected in- correct...   \n",
       "4  ry. Note: Add the detergent for the pre- wash ...   \n",
       "\n",
       "                                           EMBEDDING  \\\n",
       "0  [0.028259277, 0.038513184, 0.008262634, 0.0149...   \n",
       "1  [0.051940918, 0.04940796, -0.03387451, 0.04159...   \n",
       "2  [0.072387695, 0.07904053, 0.0060920715, 0.0329...   \n",
       "3  [0.06744385, 0.04437256, -0.02709961, -0.02598...   \n",
       "4  [0.05621338, -0.016036987, -0.034729004, 0.008...   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-29 03:42:34.872000-07:00  \n",
       "1 2025-04-29 03:42:34.872000-07:00  \n",
       "2 2025-04-29 03:42:34.872000-07:00  \n",
       "3 2025-04-29 03:42:34.872000-07:00  \n",
       "4 2025-04-29 03:42:34.872000-07:00  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting section headers from the PDF files\n",
    "\n",
    "def extract_text_chunks(file_path, manual_id, chunk_size=512, chunk_overlap=128):\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Step 1: Combine all text across pages with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = []  # (char_index, page_number)\n",
    "\n",
    "    for doc_page in docs:\n",
    "        text = doc_page.page_content.strip().replace('\\n', ' ')\n",
    "        start_idx = len(all_text)\n",
    "        all_text += text + \" \"  # Add space to separate pages\n",
    "        end_idx = len(all_text)\n",
    "        page_map.append((start_idx, end_idx, doc_page.metadata['page']))\n",
    "\n",
    "    # Step 2: Create chunks with overlap, spanning across pages\n",
    "    chunks = []\n",
    "    chunk_order = []\n",
    "    page_start_list = []\n",
    "    page_end_list = []\n",
    "\n",
    "    idx = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while idx < len(all_text):\n",
    "        chunk = all_text[idx:idx + chunk_size]\n",
    "\n",
    "        # Determine pages involved in this chunk\n",
    "        chunk_start = idx\n",
    "        chunk_end = idx + len(chunk)\n",
    "\n",
    "        pages_in_chunk = [\n",
    "            page_num\n",
    "            for start, end, page_num in page_map\n",
    "            if not (end <= chunk_start or start >= chunk_end)  # overlap condition\n",
    "        ]\n",
    "\n",
    "        page_start = min(pages_in_chunk) if pages_in_chunk else None\n",
    "        page_end = max(pages_in_chunk) if pages_in_chunk else None\n",
    "\n",
    "        chunks.append(chunk)\n",
    "        page_start_list.append(page_start)\n",
    "        page_end_list.append(page_end)\n",
    "        chunk_order.append(chunk_idx)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        idx += chunk_size - chunk_overlap\n",
    "\n",
    "    # Step 3: Create DataFrame\n",
    "    rows = [{\n",
    "        'DOCUMENT_ID': manual_id,\n",
    "        'PAGE_START_NUMBER': start,\n",
    "        'PAGE_END_NUMBER': end,\n",
    "        'CHUNK_TEXT': chunk,\n",
    "        'CHUNK_ORDER': order\n",
    "    } for chunk, start, end, order in zip(chunks, page_start_list, page_end_list, chunk_order)]\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"DOCUMENT_ID\", \"PAGE_START_NUMBER\", \"PAGE_END_NUMBER\", \"CHUNK_TEXT\", \"CHUNK_ORDER\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "\n",
    "    large_chunks_df = cursor.fetch_pandas_all()\n",
    "\n",
    "except:\n",
    "    create_dataframe = True\n",
    "\n",
    "    print(\"Creating table CHUNKS_LARGE\")\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_LARGE (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "\n",
    "    large_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = row[1][\"FILE_PATH\"]\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 7000,#1024,\n",
    "                            chunk_overlap = 128)  # Show first 5 chunks\n",
    "        large_chunks_df = pd.concat([large_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    print(\"Writing the large chunks DataFrame to Snowflake\")\n",
    "    # Write the DataFrame to Snowflake\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=large_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_LARGE\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Update the embeddings for the chunks in the CHUNKS_LARGE table\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_LARGE\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_LARGE;\n",
    "    \"\"\")\n",
    "    large_chunks_df = cursor.fetch_pandas_all()\n",
    "\n",
    "large_chunks_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d11219e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHUNK_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>PAGE_START_NUMBER</th>\n",
       "      <th>PAGE_END_NUMBER</th>\n",
       "      <th>CHUNK_ORDER</th>\n",
       "      <th>CHUNK_TEXT</th>\n",
       "      <th>EMBEDDING</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Register your b M o ge s y n c t B e h f o r w...</td>\n",
       "      <td>[0.021438599, 0.046142578, -0.011352539, 0.027...</td>\n",
       "      <td>2025-04-29 03:43:49.567000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>.. 27 2Preventing material damage.... 10 12 De...</td>\n",
       "      <td>[0.00843811, 0.0473938, 0.006866455, 0.0100173...</td>\n",
       "      <td>2025-04-29 03:43:49.567000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>transit bolts...... 13 13.9 Soaking laundry......</td>\n",
       "      <td>[0.08099365, 0.018310547, 0.003227234, 0.01860...</td>\n",
       "      <td>2025-04-29 03:43:49.567000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>........... 30 14.2 Deactivating the childproo...</td>\n",
       "      <td>[0.020263672, 0.0001231432, -0.0038833618, -0....</td>\n",
       "      <td>2025-04-29 03:43:49.567000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-Nr.) and production number (FD)........ 45 20...</td>\n",
       "      <td>[0.024765015, 0.04498291, -0.03314209, 0.02304...</td>\n",
       "      <td>2025-04-29 03:43:49.567000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CHUNK_ID  DOCUMENT_ID  PAGE_START_NUMBER  PAGE_END_NUMBER  CHUNK_ORDER  \\\n",
       "0         1            1                  0                1            0   \n",
       "1         2            1                  1                1            1   \n",
       "2         3            1                  1                2            2   \n",
       "3         4            1                  2                2            3   \n",
       "4         5            1                  2                3            4   \n",
       "\n",
       "                                          CHUNK_TEXT  \\\n",
       "0  Register your b M o ge s y n c t B e h f o r w...   \n",
       "1  .. 27 2Preventing material damage.... 10 12 De...   \n",
       "2  transit bolts...... 13 13.9 Soaking laundry......   \n",
       "3  ........... 30 14.2 Deactivating the childproo...   \n",
       "4  -Nr.) and production number (FD)........ 45 20...   \n",
       "\n",
       "                                           EMBEDDING  \\\n",
       "0  [0.021438599, 0.046142578, -0.011352539, 0.027...   \n",
       "1  [0.00843811, 0.0473938, 0.006866455, 0.0100173...   \n",
       "2  [0.08099365, 0.018310547, 0.003227234, 0.01860...   \n",
       "3  [0.020263672, 0.0001231432, -0.0038833618, -0....   \n",
       "4  [0.024765015, 0.04498291, -0.03314209, 0.02304...   \n",
       "\n",
       "                        CREATED_AT  \n",
       "0 2025-04-29 03:43:49.567000-07:00  \n",
       "1 2025-04-29 03:43:49.567000-07:00  \n",
       "2 2025-04-29 03:43:49.567000-07:00  \n",
       "3 2025-04-29 03:43:49.567000-07:00  \n",
       "4 2025-04-29 03:43:49.567000-07:00  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "    \n",
    "except:\n",
    "    print(\"Creating table CHUNKS_SMALL\")\n",
    "\n",
    "    small_chunks_df = pd.DataFrame()\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = row[1][\"FILE_PATH\"]\n",
    "        tmp_chunked_df = extract_text_chunks(file_path = file_path, \n",
    "                            manual_id = manual_id,\n",
    "                            chunk_size = 1024,\n",
    "                            chunk_overlap = 64)  # Show first 5 chunks\n",
    "        small_chunks_df = pd.concat([small_chunks_df, tmp_chunked_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE OR REPLACE TABLE CHUNKS_SMALL (\n",
    "        CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        PAGE_START_NUMBER INT,\n",
    "        PAGE_END_NUMBER INT,\n",
    "        CHUNK_ORDER INT,\n",
    "        CHUNK_TEXT STRING NOT NULL,\n",
    "        EMBEDDING VECTOR(FLOAT, 1024),\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=small_chunks_df,\n",
    "        database =database,\n",
    "        table_name=\"CHUNKS_SMALL\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    # Update the embeddings for the small chunks\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE CHUNKS_SMALL\n",
    "        SET EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "            'snowflake-arctic-embed-l-v2.0',\n",
    "            CHUNK_TEXT\n",
    "        )\n",
    "        WHERE EMBEDDING IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    time.sleep(2)\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * \n",
    "        FROM CHUNKS_SMALL;\n",
    "    \"\"\")\n",
    "    small_chunks_df = cursor.fetch_pandas_all()\n",
    "\n",
    "small_chunks_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b8c0f",
   "metadata": {},
   "source": [
    "## Creating sections table using LLM for TOC extraction\n",
    "\n",
    "The function `extract_TOC` takes quite a while due to the chunk size and the model. This can be tampered with, but i found most consistent results with said model. I also think that larger chunks are better for this task, as the model can see context of the first few pages, and it also ensures that the table of contents is included in the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95ea5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_TOC_cortex(text: str, model : str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    result = cursor.execute(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n",
    "    \"\"\")\n",
    "    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n",
    "\n",
    "    return cursor.fetch_pandas_all().iloc[0,0]\n",
    "\n",
    "\n",
    "# This example prints out section 4 of the first document of the database. mistral-large2 mistral-7b\n",
    "# llm_output = extract_TOC(df_large_chunks.loc[0,\"CHUNK\"], model = 'mistral-7b')\n",
    "\n",
    "# llm_output = extract_TOC(large_chunks_df.loc[0,\"CHUNK_TEXT\"], model = 'llama3.1-70b')\n",
    "# llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64895cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"Section\": \"Table of contents\",\\n  \"Sub Sections\": [\\n    {\\n      \"Section\": \"Safety\",\\n      \"Section Number\": 1.0,\\n      \"Page\": 4,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"General information\",\\n          \"Section Number\": 1.1,\\n          \"Page\": 4,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Intended use\",\\n          \"Section Number\": 1.2,\\n          \"Page\": 4,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Restriction on user group\",\\n          \"Section Number\": 1.3,\\n          \"Page\": 4,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe installation\",\\n          \"Section Number\": 1.4,\\n          \"Page\": 5,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe use\",\\n          \"Section Number\": 1.5,\\n          \"Page\": 7,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Safe cleaning and maintenance\",\\n          \"Section Number\": 1.6,\\n          \"Page\": 9,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Preventing material damage\",\\n      \"Section Number\": 2.0,\\n      \"Page\": 10,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Environmental protection and saving energy\",\\n      \"Section Number\": 3.0,\\n      \"Page\": 11,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Disposing of packaging\",\\n          \"Section Number\": 3.1,\\n          \"Page\": 11,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Save energy and conserve resources\",\\n          \"Section Number\": 3.2,\\n          \"Page\": 11,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Energy saving mode\",\\n          \"Section Number\": 3.3,\\n          \"Page\": 11,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Installation and connection\",\\n      \"Section Number\": 4.0,\\n      \"Page\": 11,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Unpacking the appliance\",\\n          \"Section Number\": 4.1,\\n          \"Page\": 11,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Contents of package\",\\n          \"Section Number\": 4.2,\\n          \"Page\": 11,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Requirements for the installation location\",\\n          \"Section Number\": 4.3,\\n          \"Page\": 12,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Removing the transit bolts\",\\n          \"Section Number\": 4.4,\\n          \"Page\": 13,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Connecting the appliance\",\\n          \"Section Number\": 4.5,\\n          \"Page\": 14,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Aligning the appliance\",\\n          \"Section Number\": 4.6,\\n          \"Page\": 15,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Before using for the first time\",\\n      \"Section Number\": 5.0,\\n      \"Page\": 16,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Starting an empty washing cycle\",\\n          \"Section Number\": 5.1,\\n          \"Page\": 16,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Familiarising yourself with your appliance\",\\n      \"Section Number\": 6.0,\\n      \"Page\": 17,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Appliance\",\\n          \"Section Number\": 6.1,\\n          \"Page\": 17,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Detergent drawer\",\\n          \"Section Number\": 6.2,\\n          \"Page\": 17,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Control panel\",\\n          \"Section Number\": 6.3,\\n          \"Page\": 18,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Display\",\\n      \"Section Number\": 7.0,\\n      \"Page\": 19,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Buttons\",\\n      \"Section Number\": 8.0,\\n      \"Page\": 21,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Programmes\",\\n      \"Section Number\": 9.0,\\n      \"Page\": 22,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Accessories\",\\n      \"Section Number\": 10.0,\\n      \"Page\": 26,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Laundry\",\\n      \"Section Number\": 11.0,\\n      \"Page\": 26,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Preparing the laundry\",\\n          \"Section Number\": 11.1,\\n          \"Page\": 26,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Care symbols on the care labels\",\\n          \"Section Number\": 11.2,\\n          \"Page\": 27,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Detergents and care products\",\\n      \"Section Number\": 12.0,\\n      \"Page\": 27,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Basic operation\",\\n      \"Section Number\": 13.0,\\n      \"Page\": 27,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Switching on the appliance\",\\n          \"Section Number\": 13.1,\\n          \"Page\": 27,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Setting a programme\",\\n          \"Section Number\": 13.2,\\n          \"Page\": 27,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Opening the door\",\\n          \"Section Number\": 13.3,\\n          \"Page\": 28,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Loading laundry\",\\n          \"Section Number\": 13.4,\\n          \"Page\": 28,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Inserting the measuring aid\",\\n          \"Section Number\": 13.5,\\n          \"Page\": 28,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Using the measuring aid\",\\n          \"Section Number\": 13.6,\\n          \"Page\": 28,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Adding detergent and care product\",\\n          \"Section Number\": 13.7,\\n          \"Page\": 29,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Starting the programme\",\\n          \"Section Number\": 13.8,\\n          \"Page\": 29,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Soaking laundry\",\\n          \"Section Number\": 13.9,\\n          \"Page\": 29,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Adding laundry\",\\n          \"Section Number\": 13.10,\\n          \"Page\": 29,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cancelling the programme\",\\n          \"Section Number\": 13.11,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Resuming the programme when the programme status is Rinse Hold\",\\n          \"Section Number\": 13.12,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Unloading the laundry\",\\n          \"Section Number\": 13.13,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Switching off the appliance\",\\n          \"Section Number\": 13.14,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Childproof lock\",\\n      \"Section Number\": 14.0,\\n      \"Page\": 30,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Activating the childproof lock\",\\n          \"Section Number\": 14.1,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Deactivating the childproof lock\",\\n          \"Section Number\": 14.2,\\n          \"Page\": 30,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Basic settings\",\\n      \"Section Number\": 15.0,\\n      \"Page\": 31,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Overview of the basic settings\",\\n          \"Section Number\": 15.1,\\n          \"Page\": 31,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Changing the basic settings\",\\n          \"Section Number\": 15.2,\\n          \"Page\": 31,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Cleaning and servicing\",\\n      \"Section Number\": 16.0,\\n      \"Page\": 31,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Cleaning the drum\",\\n          \"Section Number\": 16.1,\\n          \"Page\": 31,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the detergent drawer\",\\n          \"Section Number\": 16.2,\\n          \"Page\": 32,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the drain pump\",\\n          \"Section Number\": 16.3,\\n          \"Page\": 33,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Cleaning the rubber gasket\",\\n          \"Section Number\": 16.4,\\n          \"Page\": 36,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Troubleshooting\",\\n      \"Section Number\": 17.0,\\n      \"Page\": 37,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Emergency release\",\\n          \"Section Number\": 17.1,\\n          \"Page\": 43,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Reset electronic card\",\\n          \"Section Number\": 17.2,\\n          \"Page\": 43,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Transportation, storage and disposal\",\\n      \"Section Number\": 18.0,\\n      \"Page\": 43,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Removing the appliance\",\\n          \"Section Number\": 18.1,\\n          \"Page\": 43,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Inserting the transit bolts\",\\n          \"Section Number\": 18.2,\\n          \"Page\": 43,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Using the appliance again\",\\n          \"Section Number\": 18.3,\\n          \"Page\": 44,\\n          \"Sub Sections\": []\\n        },\\n        {\\n          \"Section\": \"Disposing of old appliance\",\\n          \"Section Number\": 18.4,\\n          \"Page\": 44,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Customer Service\",\\n      \"Section Number\": 19.0,\\n      \"Page\": 44,\\n      \"Sub Sections\": [\\n        {\\n          \"Section\": \"Product number (E-Nr.) and production number (FD)\",\\n          \"Section Number\": 19.1,\\n          \"Page\": 45,\\n          \"Sub Sections\": []\\n        }\\n      ]\\n    },\\n    {\\n      \"Section\": \"Consumption values\",\\n      \"Section Number\": 20.0,\\n      \"Page\": 46,\\n      \"Sub Sections\": []\\n    },\\n    {\\n      \"Section\": \"Technical data\",\\n      \"Section Number\": 21.0,\\n      \"Page\": 46,\\n      \"Sub Sections\": []\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_TOC_OpenAI(text: str) -> str:\n",
    "    prompt = (\n",
    "    \"\"\"\n",
    "    I will provide a long string of text that most likely contains a table of contents, \n",
    "    although it may also include additional body text from a document. Your task is to carefully \n",
    "    extract only the table of contents and structure it as a JSON object in the following \n",
    "    format:\n",
    "    {\n",
    "      \"Section\": \"<section name>\",\n",
    "      \"Section Number\": \"<section name>\",\n",
    "      \"Page\": <page number>,\n",
    "      \"Sub Sections\" : [{\n",
    "        \"Section\": \"<section name>\",\n",
    "        \"Section Number\": \"<section name>\",\n",
    "        \"Page\": <page number>,\n",
    "        \"Sub Sections\" : []}\n",
    "      ],\n",
    "    }    \n",
    "\n",
    "    Guideines:\n",
    "    - All keys in the json object must be either \"Section\", \"Section Number\", \"Page\", \"Sub Sections\".\n",
    "    - \"Section Number\" must be represented as a float - E.G: 1, 2, 5.3, 1,4, etc.\n",
    "    - \"Section Number\" is usually before the section name, but not always, infer it it you must.\n",
    "    - Ignore any text that is not part of the table of contents.\n",
    "    - Ensure that sub-sections are nested appropriately under their parent section.\n",
    "    - Page numbers should be extracted as integers, if possible.\n",
    "    - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n",
    "    - Do not include duplicate or repeated sections.\n",
    "    - You should only consider items which are part of the table of contents, nothing before, nothing after.\n",
    "    - \"Section\" must consist of words\n",
    "    - You must include a top level key value pair called \"Section\":\"Table of contents\".\n",
    "\n",
    "    \"\"\"\n",
    "    f\"Text:\\n{text}\"\n",
    "    )\n",
    "    open_ai_api_key = get_openai_api_key()\n",
    "    client = OpenAI(api_key = open_ai_api_key)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = generate_promt_for_openai_api(instructions = prompt, \n",
    "                                          input_text = text, \n",
    "                                          open_ai_client = client)\n",
    "\n",
    "    return result.output_text\n",
    "\n",
    "llm_output = extract_TOC_OpenAI(large_chunks_df.loc[0,\"CHUNK_TEXT\"])\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0a143e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_llm_output(llm_output_text: str) -> dict:\n",
    "    try:\n",
    "        # Look for a code block marked with ```json ... ```\n",
    "        match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", llm_output_text, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON code block found in the text.\")\n",
    "\n",
    "        raw_json = match.group(1)\n",
    "\n",
    "        # Optional: Remove trailing commas which are invalid in JSON\n",
    "        cleaned_json = re.sub(r\",\\s*([\\]}])\", r\"\\1\", raw_json)\n",
    "\n",
    "        parsed = json.loads(cleaned_json)\n",
    "        return parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "# parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "# parsed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aef7f0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Safety</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Safe use</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Safe cleaning and maintenance</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Preventing material damage</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Environmental protection and saving energy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Disposing of packaging</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      SECTION  SECTION_NUMBER  PAGE  \\\n",
       "0                                      Safety             1.0     4   \n",
       "1                         General information             1.1     4   \n",
       "2                                Intended use             1.2     4   \n",
       "3                   Restriction on user group             1.3     4   \n",
       "4                           Safe installation             1.4     5   \n",
       "5                                    Safe use             1.5     7   \n",
       "6               Safe cleaning and maintenance             1.6     9   \n",
       "7                  Preventing material damage             2.0    10   \n",
       "8  Environmental protection and saving energy             3.0    11   \n",
       "9                      Disposing of packaging             3.1    11   \n",
       "\n",
       "   PARENT_SECTION_NUMBER  \n",
       "0                    NaN  \n",
       "1                    1.0  \n",
       "2                    1.0  \n",
       "3                    1.0  \n",
       "4                    1.0  \n",
       "5                    1.0  \n",
       "6                    1.0  \n",
       "7                    NaN  \n",
       "8                    NaN  \n",
       "9                    3.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traverse_sections(node, parent_section=None):\n",
    "    rows = []\n",
    "\n",
    "    # Get info from the current node\n",
    "    section = node.get(\"Section\")\n",
    "    section_number = node.get(\"Section Number\")\n",
    "    page = node.get(\"Page\")\n",
    "\n",
    "    # The levenshtein distance is used to ensure the the section called \"Table of Contents\" is not added to the dataframe\n",
    "    evaluator = load_evaluator(\"string_distance\")\n",
    "    levenshtein_score_toc = evaluator.evaluate_strings(\n",
    "    prediction=section,\n",
    "    reference=\"Table of Contents\",\n",
    "    metric=\"levenshtein\"\n",
    "    )[\"score\"]  # This will be a float between 0 and 1, where 0 means identical\n",
    "\n",
    "    if levenshtein_score_toc > 0.1:  # if the levenshtein distance is very small its likely to match \"Table of Contents\"\n",
    "        rows.append({\n",
    "            \"SECTION\": section,\n",
    "            \"SECTION_NUMBER\": section_number,\n",
    "            \"PAGE\": page,\n",
    "            \"PARENT_SECTION_NUMBER\": parent_section\n",
    "        })\n",
    "\n",
    "    # Recurse into each sub-section, if any\n",
    "    for subsection in node.get(\"Sub Sections\", []):\n",
    "        rows.extend(traverse_sections(subsection, parent_section=section_number))\n",
    "\n",
    "    return rows\n",
    "\n",
    "parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "flat_rows = traverse_sections(parsed_dict)\n",
    "toc_df = pd.DataFrame(flat_rows)\n",
    "toc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62b9c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TOC_table(documents_df, large_chunks_df, model =\"llama3.1-70b\"):\n",
    "    \"\"\"\n",
    "    documents_df: DataFrame containing the documents metadata, fetched from DOCUMENTS table.\n",
    "    large_chunks_df: DataFrame containing the large chunks, fetched from CHUNKS_LARGE table.\n",
    "    model: The model to use for extracting the table of contents. Options are \"OpenAI\", \"llama3.1-70b\", or any other snowflake cortex complete model.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for row in tqdm(documents_df.iterrows(), total = len(documents_df)):\n",
    "        manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "        file_path = row[1][\"FILE_PATH\"]\n",
    "        first_chunk_of_doc = large_chunks_df.loc[large_chunks_df[\"DOCUMENT_ID\"] == manual_id, \"CHUNK_TEXT\"].iloc[0]\n",
    "        \n",
    "        if model == \"OpenAI\":\n",
    "            llm_output = extract_TOC_OpenAI(first_chunk_of_doc)\n",
    "        else:\n",
    "            llm_output = extract_TOC_cortex(first_chunk_of_doc, model) # Can we hot swapped to Cortex\n",
    "\n",
    "        parsed_dict = extract_json_from_llm_output(llm_output)\n",
    "        flat_rows = traverse_sections(parsed_dict)\n",
    "        local_toc_df = pd.DataFrame(flat_rows)\n",
    "        local_toc_df[\"DOCUMENT_ID\"] = manual_id\n",
    "        df_list.append(local_toc_df)\n",
    "\n",
    "    toc_df = pd.concat(df_list, ignore_index=True)\n",
    "    toc_df[\"SECTION_NUMBER\"] = toc_df[\"SECTION_NUMBER\"].astype(str)\n",
    "    toc_df[\"PARENT_SECTION_NUMBER\"] = toc_df[\"PARENT_SECTION_NUMBER\"].astype(str)\n",
    "    return toc_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73257e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PARENT_SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Safety</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>General information</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Intended use</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Restriction on user group</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe installation</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe use</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Safe cleaning and maintenance</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Preventing material damage</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Environmental protection and saving energy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Disposing of packaging</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-04-29 03:55:39.582000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SECTION_ID  DOCUMENT_ID                                     SECTION  \\\n",
       "0           1            1                                      Safety   \n",
       "1           2            1                         General information   \n",
       "2           3            1                                Intended use   \n",
       "3           4            1                   Restriction on user group   \n",
       "4           5            1                           Safe installation   \n",
       "5           6            1                                    Safe use   \n",
       "6           7            1               Safe cleaning and maintenance   \n",
       "7           8            1                  Preventing material damage   \n",
       "8           9            1  Environmental protection and saving energy   \n",
       "9          10            1                      Disposing of packaging   \n",
       "\n",
       "  SECTION_NUMBER PARENT_SECTION_NUMBER  PAGE                       CREATED_AT  \n",
       "0            1.0                   nan     4 2025-04-29 03:55:39.582000-07:00  \n",
       "1            1.1                   1.0     4 2025-04-29 03:55:39.582000-07:00  \n",
       "2            1.2                   1.0     4 2025-04-29 03:55:39.582000-07:00  \n",
       "3            1.3                   1.0     4 2025-04-29 03:55:39.582000-07:00  \n",
       "4            1.4                   1.0     5 2025-04-29 03:55:39.582000-07:00  \n",
       "5            1.5                   1.0     7 2025-04-29 03:55:39.582000-07:00  \n",
       "6            1.6                   1.0     9 2025-04-29 03:55:39.582000-07:00  \n",
       "7            2.0                   nan    10 2025-04-29 03:55:39.582000-07:00  \n",
       "8            3.0                   nan    11 2025-04-29 03:55:39.582000-07:00  \n",
       "9            3.1                   3.0    11 2025-04-29 03:55:39.582000-07:00  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    create_dataframe = False\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "except:\n",
    "    create_dataframe = True\n",
    "    print(\"Creating table SECTIONS\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SECTIONS (\n",
    "        SECTION_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "        DOCUMENT_ID INT NOT NULL,\n",
    "        SECTION STRING NOT NULL,\n",
    "        SECTION_NUMBER STRING NOT NULL,\n",
    "        PARENT_SECTION_NUMBER STRING,\n",
    "        PAGE INT,\n",
    "        CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        CONSTRAINT fk_document\n",
    "            FOREIGN KEY (DOCUMENT_ID)\n",
    "            REFERENCES DOCUMENTS(DOCUMENT_ID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = create_TOC_table(documents_df, large_chunks_df, model =\"OpenAI\")\n",
    "\n",
    "    success, nchunks, nrows, output = write_pandas(\n",
    "        conn=conn,\n",
    "        df=sections_df,\n",
    "        database =database,\n",
    "        table_name=\"SECTIONS\",\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "    time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM SECTIONS;\n",
    "    \"\"\")\n",
    "\n",
    "    sections_df = cursor.fetch_pandas_all()\n",
    "\n",
    "sections_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8cb1",
   "metadata": {},
   "source": [
    "# Extracting images from the manual\n",
    "\n",
    "This chosen method which appears to be more diverse across the manuals treats each page as an image. This is a good way to ensure that all images are extracted. \n",
    "The downside is that tables and other image like content will be extracted as images. Currently this is a feature not a bug. Adjusting the image extraction method is a task for the future when we have the real PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f45c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pdf_to_images(pdf_path, zoom=2.0):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for i, page in enumerate(doc):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img_data = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append({\n",
    "            \"page_number\": i + 1,\n",
    "            \"image\": img_data\n",
    "        })\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_pdf_page_pixel_size(pdf_image):\n",
    "    width, height = pdf_image.size\n",
    "    return width * height\n",
    "\n",
    "\n",
    "def detect_image_regions(page_image, buffer=0, min_size=70, max_size = 1000, threshold=240):\n",
    "    image = np.array(page_image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Applying blur to reduce fine lines from tables\n",
    "    _, thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > min_size and h > min_size:  # Skip tiny blocks (Maybe reconsider)\n",
    "            regions.append([x - buffer, \n",
    "                            y - buffer, \n",
    "                            x + w + buffer, \n",
    "                            y + h + buffer])\n",
    "            if w * h > max_size:\n",
    "                regions.pop(-1)  \n",
    "    return regions\n",
    "\n",
    "\n",
    "def crop_regions_from_image(page_image, regions, output_dir, page_num, manual_id, manual_images_metadata):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, coords in enumerate(regions):\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        cropped = page_image.crop((x1, y1, x2, y2))\n",
    "        save_path = os.path.join(output_dir, f\"doc_{manual_id}_page_{page_num}_img_{i}.png\")\n",
    "        cropped.save(save_path)\n",
    "        manual_images_metadata[page_num][i] ={\n",
    "            \"page\": page_num,\n",
    "            \"image_path\": save_path,\n",
    "            \"coords\": (x1, y1, x2, y2)\n",
    "        }\n",
    "\n",
    "    return manual_images_metadata\n",
    "\n",
    "\n",
    "\n",
    "def add_region_to_page(page_image, regions, output_dir, page_num, pdf_path ,color=(0, 255, 0), alpha=50, save=True, verbose=0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to RGBA to allow transparency\n",
    "    annotated = page_image.convert(\"RGBA\")\n",
    "    overlay = Image.new(\"RGBA\", annotated.size, (0, 0, 0, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    for coords in regions:\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color + (alpha,), fill=color + (alpha,))\n",
    "\n",
    "    # Combine original image with overlay\n",
    "    combined = Image.alpha_composite(annotated, overlay)\n",
    "\n",
    "    if save:\n",
    "        save_path = os.path.join(output_dir, f\"page_{page_num:03d}_with_regions_{color}.png\")\n",
    "        combined.convert(\"RGB\").save(save_path)\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved page {page_num} with highlighted regions to {save_path}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def merge_overlapping_regions(regions, buffer=0):\n",
    "    \"\"\"\n",
    "    Merges overlapping or intersecting regions.\n",
    "\n",
    "    Args:\n",
    "        regions (List[List[int]]): List of regions as [x1, y1, x2, y2].\n",
    "        buffer (int): Optional buffer added to each region before checking overlaps.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Merged list of non-overlapping regions.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    # Convert to shapely boxes with optional buffer\n",
    "    boxes = [box(x1 - buffer, y1 - buffer, x2 + buffer, y2 + buffer) for x1, y1, x2, y2 in regions]\n",
    "\n",
    "    # Merge all overlapping boxes (A fix to a previous issues of diagrams being cropped into multiple images)\n",
    "    merged = unary_union(boxes)\n",
    "\n",
    "    # Ensure output is a list of boxes\n",
    "    if merged.geom_type == 'Polygon':\n",
    "        merged_boxes = [merged]\n",
    "    else:\n",
    "        merged_boxes = list(merged.geoms)\n",
    "\n",
    "    # Convert back to [x1, y1, x2, y2] format (round to int)\n",
    "    merged_regions = []\n",
    "    for b in merged_boxes:\n",
    "        x1, y1, x2, y2 = b.bounds\n",
    "        merged_regions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    return merged_regions\n",
    "\n",
    "\n",
    "def extract_images_from_pdf(pdf_path:str, manual_id:int, output_dir: str, verbose:int =0):\n",
    "    rendered_pages = render_pdf_to_images(pdf_path)\n",
    "    all_extracted = []\n",
    "\n",
    "    manual_images_metadata = {}\n",
    "\n",
    "    for page_idx,page in enumerate(rendered_pages):\n",
    "        page_num = page[\"page_number\"] \n",
    "        manual_images_metadata[page_num] = {}\n",
    "        image = page[\"image\"]\n",
    "        if verbose > 0:\n",
    "            print(f\"Processing page {page_num}...\")\n",
    "\n",
    "        # Detecting regions\n",
    "        regions = detect_image_regions(image , buffer=2, min_size=70, \n",
    "                                        max_size=get_pdf_page_pixel_size(image) * 0.99)\n",
    "        # Creates new regions by merging overlapping regions (this is a fix for cropped images  )\n",
    "        new_regions = merge_overlapping_regions(regions, buffer=0)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Found {len(new_regions)} image regions on page {page_num}\")\n",
    "\n",
    "        if not new_regions:\n",
    "            if verbose > 0:\n",
    "                print(f\"No image regions found on page {page_num}\")\n",
    "            continue\n",
    "        \n",
    "        # Creates an image directory for each PDF file\n",
    "        image_output_dir = pdf_path.split(\"/\")[-1].replace(\".pdf\", \"\").replace(\"Washer_Manuals\", output_dir)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "        # Showing the pages with the masked regions \n",
    "        modified_image = add_region_to_page(image, new_regions, image_output_dir, page_num, pdf_path, color=(0, 0, 255), alpha=50, save = False)\n",
    "\n",
    "        # OLD code \n",
    "        manual_images_metadata = crop_regions_from_image(\n",
    "            page_image=image, regions=new_regions, \n",
    "            output_dir=image_output_dir, \n",
    "            page_num=page_num, \n",
    "            manual_id=manual_id,\n",
    "            manual_images_metadata=manual_images_metadata)\n",
    "\n",
    "    return manual_images_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b32ef3",
   "metadata": {},
   "source": [
    "# Creating table for image references and metadata\n",
    "\n",
    "Currently the images are matched to the sections using the page number, which is problematic if the end of section 4.3 is one the same page as the start of section 4.4. On the top of my head i'm not quite sure how to match the images to the sections accurately, but this method yields mostly correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d7cf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_number_from_filename(filename):\n",
    "    return filename.split(\"_\")[3] if \"_\" in filename else None\n",
    "\n",
    "def generate_image_table(documents_df, sections_df, image_dir : str, all_manuals_metadata : dict):\n",
    "    image_records = []\n",
    "\n",
    "    # Loop over all subdirectories in image_dir\n",
    "    for subfolder in os.listdir(image_dir):\n",
    "        subfolder_path = os.path.join(image_dir, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue  # skip files\n",
    "        \n",
    "        # Match to document by DOCUMENT_NAME (strip extension if needed)\n",
    "        matching_docs = documents_df[documents_df['DOCUMENT_NAME'].str.contains(subfolder, case=False)]\n",
    "        if matching_docs.empty:\n",
    "            print(f\"No matching document for subfolder: {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        document_id = matching_docs.iloc[0]['DOCUMENT_ID']\n",
    "        document_name = matching_docs.iloc[0]['DOCUMENT_NAME']\n",
    "        \n",
    "        # List all image files in subdirectory\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            if not image_file.lower().endswith((\".png\")):\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            page_number = extract_page_number_from_filename(image_file)\n",
    "            order_number = image_file.split(\"img_\")[-1].strip(\".png\")\n",
    "\n",
    "            image_size = os.path.getsize(image_path)\n",
    "            image_width, image_height = Image.open(image_path).size\n",
    "            \n",
    "            # Try to match to a section (same document, closest PAGE <= image page)\n",
    "            section_match = None\n",
    "            if page_number is not None:\n",
    "                matching_sections = sections_df[\n",
    "                    (sections_df['DOCUMENT_ID'] == document_id) & \n",
    "                    (sections_df['PAGE'].astype(str) <= str(page_number))\n",
    "                ]\n",
    "                if not matching_sections.empty:\n",
    "                    section_match = matching_sections.sort_values(\"PAGE\", ascending=False).iloc[0]\n",
    "            \n",
    "            image_records.append({\n",
    "                \"DOCUMENT_ID\": document_id,\n",
    "                \"SECTION_ID\": section_match[\"SECTION_ID\"] if section_match is not None else None,\n",
    "                \"SECTION_NUMBER\": section_match[\"SECTION_NUMBER\"] if section_match is not None else None,\n",
    "                \"PAGE\": page_number,\n",
    "                \"IMG_ORDER\": order_number,\n",
    "                \"IMAGE_FILE\": image_file,\n",
    "                \"IMAGE_PATH\": image_path,\n",
    "                \"IMAGE_SIZE\": image_size,\n",
    "                \"IMAGE_WIDTH\": image_width,\n",
    "                \"IMAGE_HEIGHT\": image_height,\n",
    "                \"IMAGE_X1\": all_manuals_metadata[document_id][int(page_number)][int(order_number)][\"coords\"][0],\n",
    "                \"IMAGE_Y1\": all_manuals_metadata[document_id][int(page_number)][int(order_number)][\"coords\"][1],\n",
    "                \"IMAGE_X2\": all_manuals_metadata[document_id][int(page_number)][int(order_number)][\"coords\"][2],\n",
    "                \"IMAGE_Y2\": all_manuals_metadata[document_id][int(page_number)][int(order_number)][\"coords\"][3],\n",
    "                \"DESCRIPTION\": \"\"  # Placeholder for image description\n",
    "            })\n",
    "\n",
    "    image_df = pd.DataFrame(image_records)\n",
    "    image_df.dropna(inplace=True)\n",
    "    image_df.reset_index(drop=True, inplace=True)\n",
    "    return image_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4f76112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Images from PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table IMAGES\n",
      "Success: True, Chunks: 1, Rows: 430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE_ID</th>\n",
       "      <th>SECTION_ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>SECTION_NUMBER</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>IMG_ORDER</th>\n",
       "      <th>IMAGE_FILE</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>IMAGE_SIZE</th>\n",
       "      <th>IMAGE_WIDTH</th>\n",
       "      <th>IMAGE_HEIGHT</th>\n",
       "      <th>IMAGE_X1</th>\n",
       "      <th>IMAGE_Y1</th>\n",
       "      <th>IMAGE_X2</th>\n",
       "      <th>IMAGE_Y2</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CREATED_AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>doc_1_page_12_img_0.png</td>\n",
       "      <td>.\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_0...</td>\n",
       "      <td>4963</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "      <td>446</td>\n",
       "      <td>475</td>\n",
       "      <td>612</td>\n",
       "      <td>595</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-30 01:49:23.845000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_12_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_1...</td>\n",
       "      <td>4961</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "      <td>446</td>\n",
       "      <td>648</td>\n",
       "      <td>612</td>\n",
       "      <td>768</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-30 01:49:23.845000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>doc_1_page_12_img_2.png</td>\n",
       "      <td>.\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_2...</td>\n",
       "      <td>5040</td>\n",
       "      <td>166</td>\n",
       "      <td>120</td>\n",
       "      <td>446</td>\n",
       "      <td>802</td>\n",
       "      <td>612</td>\n",
       "      <td>922</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-30 01:49:23.845000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>doc_1_page_13_img_0.png</td>\n",
       "      <td>.\\Washer_Images\\WAN28258GB\\doc_1_page_13_img_0...</td>\n",
       "      <td>7466</td>\n",
       "      <td>166</td>\n",
       "      <td>121</td>\n",
       "      <td>60</td>\n",
       "      <td>164</td>\n",
       "      <td>226</td>\n",
       "      <td>285</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-30 01:49:23.845000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>doc_1_page_13_img_1.png</td>\n",
       "      <td>.\\Washer_Images\\WAN28258GB\\doc_1_page_13_img_1...</td>\n",
       "      <td>27925</td>\n",
       "      <td>328</td>\n",
       "      <td>237</td>\n",
       "      <td>462</td>\n",
       "      <td>154</td>\n",
       "      <td>790</td>\n",
       "      <td>391</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-30 01:49:23.845000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMAGE_ID  SECTION_ID  DOCUMENT_ID SECTION_NUMBER  PAGE  IMG_ORDER  \\\n",
       "0         1          16            1            4.3    12          0   \n",
       "1         2          16            1            4.3    12          1   \n",
       "2         3          16            1            4.3    12          2   \n",
       "3         4          17            1            4.4    13          0   \n",
       "4         5          17            1            4.4    13          1   \n",
       "\n",
       "                IMAGE_FILE                                         IMAGE_PATH  \\\n",
       "0  doc_1_page_12_img_0.png  .\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_0...   \n",
       "1  doc_1_page_12_img_1.png  .\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_1...   \n",
       "2  doc_1_page_12_img_2.png  .\\Washer_Images\\WAN28258GB\\doc_1_page_12_img_2...   \n",
       "3  doc_1_page_13_img_0.png  .\\Washer_Images\\WAN28258GB\\doc_1_page_13_img_0...   \n",
       "4  doc_1_page_13_img_1.png  .\\Washer_Images\\WAN28258GB\\doc_1_page_13_img_1...   \n",
       "\n",
       "   IMAGE_SIZE  IMAGE_WIDTH  IMAGE_HEIGHT  IMAGE_X1  IMAGE_Y1  IMAGE_X2  \\\n",
       "0        4963          166           120       446       475       612   \n",
       "1        4961          166           120       446       648       612   \n",
       "2        5040          166           120       446       802       612   \n",
       "3        7466          166           121        60       164       226   \n",
       "4       27925          328           237       462       154       790   \n",
       "\n",
       "   IMAGE_Y2 DESCRIPTION                       CREATED_AT  \n",
       "0       595             2025-04-30 01:49:23.845000-07:00  \n",
       "1       768             2025-04-30 01:49:23.845000-07:00  \n",
       "2       922             2025-04-30 01:49:23.845000-07:00  \n",
       "3       285             2025-04-30 01:49:23.845000-07:00  \n",
       "4       391             2025-04-30 01:49:23.845000-07:00  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try:\n",
    "#     cursor.execute(\"\"\"\n",
    "#         SELECT * \n",
    "#         FROM IMAGES;\n",
    "#     \"\"\")\n",
    "\n",
    "#     images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "# except:\n",
    "print(\"Extracting Images from PDF files\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE IMAGES (\n",
    "    IMAGE_ID INT AUTOINCREMENT PRIMARY KEY,\n",
    "    SECTION_ID INT NOT NULL,\n",
    "    DOCUMENT_ID INT NOT NULL,\n",
    "    SECTION_NUMBER STRING NOT NULL,\n",
    "    PAGE INT,\n",
    "    IMG_ORDER INT,\n",
    "    IMAGE_FILE STRING,\n",
    "    IMAGE_PATH STRING,\n",
    "    IMAGE_SIZE NUMBER,\n",
    "    IMAGE_WIDTH NUMBER,\n",
    "    IMAGE_HEIGHT NUMBER,\n",
    "    IMAGE_X1 NUMBER,\n",
    "    IMAGE_Y1 NUMBER,\n",
    "    IMAGE_X2 NUMBER,\n",
    "    IMAGE_Y2 NUMBER,\n",
    "    DESCRIPTION STRING,\n",
    "    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "\n",
    "    CONSTRAINT fk_document\n",
    "        FOREIGN KEY (DOCUMENT_ID)\n",
    "        REFERENCES DOCUMENTS(DOCUMENT_ID),\n",
    "        \n",
    "    CONSTRAINT fk_section\n",
    "            FOREIGN KEY (SECTION_ID)\n",
    "            REFERENCES SECTIONS(SECTION_ID)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "all_manuals_metadata = {}\n",
    "for idx,row in tqdm(enumerate(documents_df.iterrows()), total = len(documents_df)):\n",
    "    manual_id = row[1][\"DOCUMENT_ID\"]\n",
    "    file_path = row[1][\"FILE_PATH\"]\n",
    "    all_manuals_metadata[manual_id] = extract_images_from_pdf(file_path, manual_id, output_dir=\"Washer_Images\", verbose = 0)\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Creating table IMAGES\")\n",
    "images_df = generate_image_table(documents_df, sections_df, \".\\\\Washer_Images\", all_manuals_metadata)\n",
    "\n",
    "images_df.head(5)\n",
    "\n",
    "success, nchunks, nrows, output = write_pandas(\n",
    "    conn=conn,\n",
    "    df=images_df,\n",
    "    database =database,\n",
    "    table_name=\"IMAGES\",\n",
    "    schema=schema,\n",
    "    auto_create_table=False,\n",
    "    overwrite=False\n",
    ")\n",
    "print(f\"Success: {success}, Chunks: {nchunks}, Rows: {nrows}\")\n",
    "\n",
    "time.sleep(3) # Sleep for 3 seconds to ensure the table is ready in snowflake. We need to query the table to get the SECTION_ID\n",
    "cursor.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM IMAGES;\n",
    "\"\"\")\n",
    "images_df = cursor.fetch_pandas_all()\n",
    "\n",
    "images_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VestasVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "lastEditStatus": {
   "authorEmail": "emhaldemo1@gmail.com",
   "authorId": "3960725274243",
   "authorName": "EMHALDEMO1",
   "lastEditTime": 1744786105031,
   "notebookId": "yjjjwqle6a6h6njufd4n",
   "sessionId": "0bd143f8-a219-40e9-853f-0ddbdab7a3c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

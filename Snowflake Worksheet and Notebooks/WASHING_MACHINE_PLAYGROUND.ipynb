{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "jswhbkeshpcjr42ftino",
   "authorId": "4483907350653",
   "authorName": "EMILHALDAN5468402",
   "authorEmail": "emilhaldan@live.dk",
   "sessionId": "18ff4f35-9d9a-4d24-9bc1-7067ce646042",
   "lastEditTime": 1744366813011
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "## This section of the notebook creates the foundation of the database, with text chunks and vector embeddings"
  },
  {
   "cell_type": "code",
   "id": "f7762667-321c-4269-b1d9-b1ddbbf9d839",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "CREATE DATABASE IF NOT EXISTS WASHING_MACHINE_MANUALS;\nCREATE SCHEMA IF NOT EXISTS WASHING_MACHINE_MANUALS.PUBLIC;\nUSE DATABASE WASHING_MACHINE_MANUALS;\nUSE SCHEMA PUBLIC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a7bb5ea-b91f-4578-aa76-6c1389acd1aa",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Creating stage to dump PDF documents into\ncreate or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2811deb0-316e-4c33-941b-616c02c7ab44",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Uploading the documents to the @docs stage directly (DO THIS MANUALLY)\n-- Check that the files were uploaded\nLS @docs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c452e4c-6ecc-418e-a988-0ee231cf5ae6",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DOCUMENTS (\n    DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n    RELATIVE_PATH STRING NOT NULL,\n    FILE_URL STRING,\n    SIZE NUMBER,\n    STAGE_NAME STRING DEFAULT '@docs',\n    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\nINSERT INTO DOCUMENTS (RELATIVE_PATH, FILE_URL, SIZE)\nSELECT \n    RELATIVE_PATH,\n    FILE_URL,\n    SIZE\nFROM DIRECTORY(@docs);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ef3816c-24f0-4697-bd83-8cc2e99719c7",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "SELECT * \nFROM DOCUMENTS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22798072-1506-43aa-92d6-2f21ebf716a8",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "-- Scale up!\n-- ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = '4X-Large'; -- Didn't seem to have any effect on the run time. Probably have to ask about this.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84c800a6-cf3e-4769-9d6c-27f3b35c156d",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\n-- Creates the table for storing the chunks and vector embeddings\nCREATE OR REPLACE TABLE CHUNKS (\n    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n    DOCUMENT_ID INT NOT NULL,\n    CHUNK_INDEX INT,\n    CHUNK STRING NOT NULL,\n    EMBEDDING VECTOR(FLOAT, 768),\n    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n    CONSTRAINT fk_document\n        FOREIGN KEY (DOCUMENT_ID)\n        REFERENCES DOCUMENTS(DOCUMENT_ID)\n);\n\n\n-- Creates a temp table with parsed text (1 row for each document, with a super long string of raw text of the document)\nCREATE OR REPLACE TEMP TABLE parsed_text_table AS\nSELECT \n  relative_path,\n  size,\n  file_url,\n  BUILD_SCOPED_FILE_URL(@docs, relative_path) AS scoped_file_url,\n  TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})) AS full_text\nFROM DIRECTORY(@docs);\n\n\n-- Using the temporary table to fill the CHUNKS tables with \nINSERT INTO CHUNKS (DOCUMENT_ID, CHUNK_INDEX, CHUNK, EMBEDDING)\nSELECT \n    d.DOCUMENT_ID,\n    chunk_data.index AS CHUNK_INDEX,\n    chunk_data.value::STRING AS CHUNK,\n    SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', chunk_data.value::STRING) AS EMBEDDING\nFROM parsed_text_table p\nJOIN DOCUMENTS d ON p.RELATIVE_PATH = d.RELATIVE_PATH\nJOIN LATERAL FLATTEN(\n    INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        p.full_text,\n        'none',     -- or 'markdown'\n        256,           -- chunk size\n        32             -- overlap\n    )\n) AS chunk_data\nWHERE p.full_text IS NOT NULL;\n\nSELECT * \nFROM CHUNKS \nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6d110b5-ef6f-4179-923a-7cb015c43425",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a998ff6-88eb-4353-a5a7-560483a8668c",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### This section will focus on classifying the sections of the document, using a sequence of LLM functions and logic"
  },
  {
   "cell_type": "code",
   "id": "14278f89-79e5-4d11-ae17-d828bd69d3f2",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\n\nsession = Session.builder.getOrCreate()\n\ndf_chunks = session.table(\"CHUNKS\").to_pandas()\ndf_chunks.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b5ba327-da68-43e6-b436-8ff623a7568d",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "def classify_toc_chunk(text: str) -> str:\n    prompt = (\n    \"Determine whether the following text is part of a document's Table of Contents (TOC). \"\n    \"Use the following rules to guide your decision:\\n\\n\"\n    \"Consider the text to be part of the TOC **only if** it satisfies most of the following conditions:\\n\"\n    \"- It contains multiple lines or entries of newline characters.\\n\"\n    \"- The chunk contains a sequence of numbers (e.g., 1, 1.2, 2.3.4), possibly indicating section or subsection numbers.\\n\"\n    \"- This is followed by short, non-sentence fragments (i.e., not full grammatical sentences), typically a title or heading.\\n\"\n    \"- Ends with a page number, often preceded by dots or whitespace for alignment (e.g., '..... 12').\\n\"\n    \"- The tone is formal and lacks narrative or explanatory text.\\n\\n\"\n    \"- Sequences of numbers apper in the chunk of text, such as 2.1, 2.2, 2.3, or 5.3, 5.4, 5.5. These sequences are usually seperated by some natural language\"\n    \"Do **not** classify the text as TOC if:\\n\"\n    \"- It mostly contains complete paragraphs or full sentences.\\n\"\n    \"- It does not contain numbered sections or page numbers.\\n\"\n    \"- It appears to be body content, such as an introduction, abstract, or explanation.\\n\\n\"\n    \"Respond strictly with 'Yes' or 'No'.\\n\\n\"\n    f\"Text:\\n{text}\"\n)\n    \n    result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COMPLETE('snowflake-arctic', $$ {prompt} $$)\n    \"\"\").collect()\n    \n    return result[0][0].strip()\n\ndf_chunks_sample = df_chunks.head(50)  # or filter by a specific DOCUMENT_ID\ntoc_labels = df_chunks_sample[\"CHUNK\"].apply(classify_toc_chunk)\ndf_chunks_sample[\"TOC_LABEL\"] = toc_labels\ndf_chunks_sample",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63148ddf-d8ec-4f6d-95a7-cbb22e8caf2d",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "\ndf_toc_chunks.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56d99d4f-6d88-4261-bd18-8fee6c800ad0",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "def extract_section_names(text: str) -> list[str]:\n    prompt = (\n        \"\"\"\n        The following text is either completely or partially part of a Table of Contents. \n        Extract each section and subsection into a clean list of tuples with section number and section titles mentioned in the text. \n        For an example. the input text 'Further information and explanations are available online:\\n  \\nTable of contents\\n|Safety \\\n        ..|4|Buttons|22|\\n| :---: | :---: | :---: | :---: |\\n|1.1 General information..|4|||\\n|1.2 \\\n        Intended use....|4|Programmes...|24|\\n|1.3 Restriction on user'\n        Should return a list in the structure of: \n        [(1.), ()]\n        Return the list as specified.\\n\\n\n        \"\"\"\n    )\n    \n    result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.EXTRACT_ANSWER('{text.strip()}', $$ {prompt} $$)\n    \"\"\").collect()\n\n    return result\n    # try:\n    #     sections = json.loads(result[0][0])\n    #     return sections if isinstance(sections, list) else []\n    # except Exception as e:\n    #     print(\"Error parsing output:\", e)\n    #     return []\n\nextract_section_names(df_toc_chunks.loc[0,\"CHUNK\"])\n    \n# section_lists = df_toc_chunks[\"CHUNK\"].apply(extract_section_names)\n# section_lists\n\n",
   "execution_count": null
  }
 ]
}
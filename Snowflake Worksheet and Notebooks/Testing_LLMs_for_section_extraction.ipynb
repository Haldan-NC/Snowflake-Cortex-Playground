{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2pkmejpke2uf7ti2mqgy",
   "authorId": "4483907350653",
   "authorName": "EMILHALDAN5468402",
   "authorEmail": "emilhaldan@live.dk",
   "sessionId": "c0196120-80e3-44a6-a60c-c31ce33aad70",
   "lastEditTime": 1744372096669
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a5c8e6-c606-4c78-b292-6463095ad347",
   "metadata": {
    "name": "Abstract",
    "collapsed": false
   },
   "source": "## This Notebook tests the possibility of using LLMs in order to extract information of the PDFs\n\nThis method has been tested as using classical string manipulation and data engineering appeared to be more challenging than anticipated. This could be due to several reasons such as:\n- The specific PDF document used for testing \n- Lack of competence on the task (Emil Haldan)\n\nThe findings of the tests made on a single document show that the larger models yield much better results. Currently the best ouput came from using llama3.1-70b, which appears to be the \"best\" model offered by snowflake which is available in our current region \"Azure West Europe - (Netherlands)\".\n\nThe function `extract_TOC(text: str, model : str)` currently takes 1 text string of 8192 characters, and returns a string which includes a JSON structure of the table of contents (cell 13).\n\nProcessing time pr. document: 61.44 seconds\nWarehouse specs: Small, 2 clusters."
  },
  {
   "cell_type": "code",
   "id": "f7762667-321c-4269-b1d9-b1ddbbf9d839",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "CREATE DATABASE IF NOT EXISTS WASHING_MACHINE_MANUALS;\nCREATE SCHEMA IF NOT EXISTS WASHING_MACHINE_MANUALS.PUBLIC;\nUSE DATABASE WASHING_MACHINE_MANUALS;\nUSE SCHEMA PUBLIC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a7bb5ea-b91f-4578-aa76-6c1389acd1aa",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Creating stage to dump PDF documents into\ncreate or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2811deb0-316e-4c33-941b-616c02c7ab44",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Uploading the documents to the @docs stage directly (DO THIS MANUALLY)\n-- Check that the files were uploaded\nLS @docs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c452e4c-6ecc-418e-a988-0ee231cf5ae6",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DOCUMENTS (\n    DOCUMENT_ID INT AUTOINCREMENT PRIMARY KEY,\n    RELATIVE_PATH STRING NOT NULL,\n    FILE_URL STRING,\n    SIZE NUMBER,\n    STAGE_NAME STRING DEFAULT '@docs',\n    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\nINSERT INTO DOCUMENTS (RELATIVE_PATH, FILE_URL, SIZE)\nSELECT \n    RELATIVE_PATH,\n    FILE_URL,\n    SIZE\nFROM DIRECTORY(@docs);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ef3816c-24f0-4697-bd83-8cc2e99719c7",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "SELECT * \nFROM DOCUMENTS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22798072-1506-43aa-92d6-2f21ebf716a8",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "-- Scale up!\n-- ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = '4X-Large'; -- Didn't seem to have any effect on the run time. Probably have to ask about this.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84c800a6-cf3e-4769-9d6c-27f3b35c156d",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\n-- Creates the table for storing the chunks and vector embeddings\nCREATE OR REPLACE TABLE CHUNKS (\n    CHUNK_ID INT AUTOINCREMENT PRIMARY KEY,\n    DOCUMENT_ID INT NOT NULL,\n    CHUNK_INDEX INT,\n    CHUNK STRING NOT NULL,\n    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n    CONSTRAINT fk_document\n        FOREIGN KEY (DOCUMENT_ID)\n        REFERENCES DOCUMENTS(DOCUMENT_ID)\n);\n\n\n-- Creates a temp table with parsed text (1 row for each document, with a super long string of raw text of the document)\nCREATE OR REPLACE TEMP TABLE parsed_text_table AS\nSELECT \n  relative_path,\n  size,\n  file_url,\n  BUILD_SCOPED_FILE_URL(@docs, relative_path) AS scoped_file_url,\n  TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})) AS full_text\nFROM DIRECTORY(@docs);\n\n\n-- Using the temporary table to fill the CHUNKS tables with \nINSERT INTO CHUNKS (DOCUMENT_ID, CHUNK_INDEX, CHUNK)\nSELECT \n    d.DOCUMENT_ID,\n    chunk_data.index AS CHUNK_INDEX,\n    chunk_data.value::STRING AS CHUNK,\nFROM parsed_text_table p\nJOIN DOCUMENTS d ON p.RELATIVE_PATH = d.RELATIVE_PATH\nJOIN LATERAL FLATTEN(\n    INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        p.full_text,\n        'none',     -- or 'markdown'\n        8192,       -- chunk size\n        256             -- overlap\n    )\n) AS chunk_data\nWHERE p.full_text IS NOT NULL;\n\nSELECT * \nFROM CHUNKS \nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6d110b5-ef6f-4179-923a-7cb015c43425",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a998ff6-88eb-4353-a5a7-560483a8668c",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### This section will focus on classifying the sections of the document, using a sequence of LLM functions and logic"
  },
  {
   "cell_type": "code",
   "id": "14278f89-79e5-4d11-ae17-d828bd69d3f2",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nimport time\n\nsession = Session.builder.getOrCreate()\n\ndf_chunks = session.table(\"CHUNKS\").to_pandas()\ndf_chunks.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56d99d4f-6d88-4261-bd18-8fee6c800ad0",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "def extract_TOC(text: str, model : str) -> str:\n    prompt = (\n    \"\"\"\n    I will provide a long string of text that most likely contains a table of contents, \n    although it may also include additional body text from a document. Your task is to carefully \n    extract only the table of contents and structure it as a JSON object in the following \n    format:\n    {\n      \"Section\": \"<section name>\",\n      \"Section Number\": \"<section name>\",\n      \"Page\": <page number>\n    }\n\n    Guidelines:\n        - Ignore any text that is not part of the table of contents.\n        - Ensure that sub-sections are nested appropriately under their parent section.\n        - If a section has no sub-sections, return \"Sub sections\": [].\n        - Page numbers should be extracted as integers, if possible.\n        - Be tolerant of inconsistencies in formatting, spacing, or punctuation (e.g. dashes, colons, ellipses).\n        - Do not include duplicate or repeated sections.\n        - You should only consider items which are part of the table of contents, nothing before, nothing after.\n        - \"Section\" must consist of words\n        - \"Section Number\" must be represented as an integer or float - E.G: 1, 2, 5.3, 1,4, etc.\n        - \"Page\" must be an integer.\n            \n    \"\"\"\n    f\"Text:\\n{text}\"\n    )\n    start_time = time.time()\n    result = session.sql(f\"\"\"\n        SELECT SNOWFLAKE.CORTEX.COMPLETE('{model}', $$ {prompt} $$)\n    \"\"\").collect()\n    print(f\"Runtime in seconds: {time.time() - start_time:.4f}\")\n    \n    return result\n\nllm_output = extract_TOC(df_chunks.loc[0,\"CHUNK\"], model = 'snowflake-arctic')\nllm_output[0][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f4c1ab0-852d-4b9b-823e-869252730ec8",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "llm_output = extract_TOC(df_chunks.loc[0,\"CHUNK\"], model = 'llama3.1-8b')\nllm_output[0][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae3da5ac-91bf-4acb-9a99-f951e111b14a",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "llm_output = extract_TOC(df_chunks.loc[0,\"CHUNK\"], model = 'llama3.1-70b')\nllm_output[0][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7518aef-fb4a-43df-883b-c89b3c55dc76",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "llm_output = extract_TOC(df_chunks.loc[0,\"CHUNK\"], model = 'mistral-large2')\nllm_output[0][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2152fe06-01c3-4e09-8417-6886765b0cbe",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "llm_output = extract_TOC(df_chunks.loc[0,\"CHUNK\"], model = 'mixtral-8x7b')\nllm_output[0][0]\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41b22f55-09b2-4912-93ec-52b05427c112",
   "metadata": {
    "name": "Conclusion_of_tests",
    "collapsed": false
   },
   "source": "### It appears that the best results are constructed using llama3.1-70b.\n\nThings to improve could potentially be:\n- The chunk size which is currently 8192 (whic was estmated based on a 2 page TOC)\n- text prompt: could potentially include more instructions or be cleaned up."
  },
  {
   "cell_type": "code",
   "id": "53895a98-b5e8-4425-8b56-bc6288a54e38",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# Testing it on another document\n\nllm_output = extract_TOC(df_chunks.loc[10,\"CHUNK\"], model = 'llama3.1-70b')\nllm_output[0][0]",
   "execution_count": null
  }
 ]
}